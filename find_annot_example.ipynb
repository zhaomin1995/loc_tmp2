{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from IPython.display import display, Image\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instance_ids = []\n",
    "json_filename = 'full_gold.json'\n",
    "with open(json_filename, 'r') as jsonfile:\n",
    "    lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "    for line in lines:\n",
    "        instance = json.loads(line)\n",
    "        instance_id = instance['anchor_id'].split(\"_\")[0] + \"_\" + instance['anchor_id'].split(\"_\")[1]\n",
    "        instance_ids.append(instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "onlytweets_dir = '/home/zhaomin/Documents/saved_tweets/'\n",
    "instances = []\n",
    "num_context = 3\n",
    "url_prefix = \"http://www.cse.unt.edu/~blanco/screenshot/\"\n",
    "  \n",
    "for event in os.listdir(onlytweets_dir):\n",
    "    event_name = event.split(\"_\")[0] + event.split(\"_\")[1].split(\"-\")[0]\n",
    "    event_path = os.path.join(onlytweets_dir, event)\n",
    "    days = os.listdir(event_path)\n",
    "    for day in days:\n",
    "        day_path = os.path.join(event_path, day)\n",
    "        day_instances = os.listdir(day_path)\n",
    "        for day_instance in day_instances:\n",
    "            day_instance_path = os.path.join(day_path, day_instance)\n",
    "\n",
    "            # load the anchor tweet\n",
    "            anchor_filename = f\"anchor_{day_instance}.json\"\n",
    "            anchor_filepath = os.path.join(day_instance_path, anchor_filename)\n",
    "            with open(anchor_filepath, 'r') as anchor_jsonfile:\n",
    "                anchor_tweet = json.load(anchor_jsonfile)\n",
    "\n",
    "            location_name = \" \".join([token.capitalize() for token in anchor_tweet['location'].split(\" \")])\n",
    "\n",
    "            # only use the gold instance in phase 1\n",
    "            instance_id = f\"{event_name}_{day_instance}\"\n",
    "            if instance_id not in instance_ids:\n",
    "                continue\n",
    "\n",
    "            ele = {\n",
    "                'instance_id': instance_id,\n",
    "                'event': f\"{event_name}\",\n",
    "                'anchor_location': location_name,\n",
    "                'anchor_timestamp': anchor_tweet['created_at'],\n",
    "                'anchor_url': f\"{url_prefix}{event_name}_{day_instance}_anchor_{day_instance}.png\"\n",
    "            }\n",
    "\n",
    "            # load the 20 context tweets\n",
    "            tweets = []\n",
    "            filenames = os.listdir(day_instance_path)\n",
    "            json_filenames = [x for x in filenames if x.endswith(\".json\") and not x.startswith(\"anchor\")]\n",
    "            for json_filename in json_filenames:\n",
    "                json_filepath = os.path.join(day_instance_path, json_filename)\n",
    "                with open(json_filepath, 'r') as jsonfile:\n",
    "                    tweet = json.load(jsonfile)\n",
    "                    tweets.append(tweet)\n",
    "            context_tweets = sorted(tweets, key = lambda x: datetime.strptime(x['created_at'], '%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "            # only keep part of the context tweets\n",
    "            context_tweets = context_tweets[10-num_context:10+num_context]\n",
    "\n",
    "            for index, context_tweet in enumerate(context_tweets):\n",
    "                context_id = context_tweet['id_str']\n",
    "                ele[f\"context{index+(10-num_context+1)}_url\"] = f\"{url_prefix}{event_name}_{day_instance}_{context_id}.png\"\n",
    "                context_timestamp = context_tweet['created_at']\n",
    "                ele[f\"context{index+(10-num_context+1)}_timestamp\"] = context_timestamp\n",
    "\n",
    "            instances.append(ele)\n",
    "#             break\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "instances = []\n",
    "csv_filepath = 'batch_6540.csv'\n",
    "with open(csv_filepath, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        instances.append(row)\n",
    "        \n",
    "num_instances = 500\n",
    "num_batches = math.ceil(len(instances)/500)\n",
    "batch_folder = 'batches'\n",
    "for i in range(num_batches):\n",
    "    batch_filename = f\"{i+1}.csv\"\n",
    "    batch_filepath = os.path.join(batch_folder, batch_filename)\n",
    "    if i != num_batches:\n",
    "        small_instances = instances[num_instances*i:num_instances*(i+1)]\n",
    "    else:\n",
    "        small_instances = instances[num_instances*i:]\n",
    "    with open(batch_filepath, 'w') as csvfile:\n",
    "        column_names = list(small_instances[0].keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=column_names)\n",
    "        writer.writeheader()\n",
    "        for instance in small_instances:\n",
    "            writer.writerow(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_filename = 'batch_3context.csv'\n",
    "        \n",
    "# generate the html file\n",
    "html_filename = \"display_context_anchor.html\"\n",
    "with open(html_filename, 'w') as htmlfile:\n",
    "\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # write the start\n",
    "        htmlfile.write(\"<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Tweet Example</title>\\n</head>\\n<body>\\n\")\n",
    "\n",
    "        # use a big table to display the tweet example\n",
    "        htmlfile.write(\"\"\"<table style=\\\"margin-left: auto; margin-right: auto; border: 1px solid black; line-height: 1.0em; width: 1200\\\">\\n\"\"\")\n",
    "        image_css = \"\"\"style=\\\"display: block; margin-left: auto; margin-right: auto; max-width: 450px; max-height: 650px;\\\"\"\"\"\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            # 3 context tweets before\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # context8_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context8_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context9_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context9_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context10_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context10_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "            # anchor tweet\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # anchor_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['anchor_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "            # 3 context tweets after\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # context11_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context11_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context12_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context12_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context13_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['context13_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "        htmlfile.write(\"</table>\\n\")\n",
    "\n",
    "        # write the end\n",
    "        htmlfile.write(\"</body>\\n</html>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the confusion matrix between the tweets with and without context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweets without context (only anchor tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "annot_nocontext_folder = '/home/zhaomin/Documents/github/loc_tmp/saved_anchors/annotation'\n",
    "annot_nocontext_filename = 'full_gold.json'\n",
    "annot_nocontext_filepath = os.path.join(annot_nocontext_folder, annot_nocontext_filename)\n",
    "\n",
    "annot_nocontext = {}\n",
    "with open(annot_nocontext_filepath, 'r') as jsonfile:\n",
    "    lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "    for line in lines:\n",
    "        instance = json.loads(line)\n",
    "        # add the instance_id so that two versions of annotation can be aligned\n",
    "        instance_id = '_'.join(instance['anchor_id'].split(\"_\")[:-1])\n",
    "        instance['instance_id'] = instance_id\n",
    "        annot_nocontext[instance_id] = instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tweets with context (second batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the yes with no if confidence level is less than or equal to 4\n",
    "import csv\n",
    "\n",
    "annotation_csv = f'batches/2_finished.csv'\n",
    "new_annot_csv = f'batches/new_2_finished.csv'\n",
    "conf_threshold = 3.0\n",
    "instances = []\n",
    "with open(annotation_csv, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if int(row['Answer.Q2']) <= conf_threshold:\n",
    "            row['Answer.Q1'] = 'No'\n",
    "        instances.append(row)\n",
    "        \n",
    "with open(new_annot_csv, 'w') as csvfile:\n",
    "    column_names = list(instances[0].keys())\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=column_names)\n",
    "    writer.writeheader()\n",
    "    for instance in instances:\n",
    "        writer.writerow(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the annotation list for using the MACE...\n",
      "Done.\n",
      "------------------------------------------------------------\n",
      "The Krippendorf's Alpha threshold is 0.6\n",
      "The overall k_alpah is 0.65\n",
      "You have removed 59 annotators\n"
     ]
    }
   ],
   "source": [
    "from annotation_helper import *\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "loc_tmp_dir = '/home/zhaomin/Documents/github/loc_tmp2'\n",
    "annotation_csv = f'batches/new_2_finished.csv'\n",
    "\n",
    "workers_removed = []\n",
    "k_threshold = 0.6\n",
    "\n",
    "# generate the file to use MACE, the generated file is 'annotation_list.csv'\n",
    "annotation_list_csvfilepath = 'annotation_list.csv'\n",
    "annotation_list, annotators, hitids = get_annotation_list(annotation_csv, workers_removed)\n",
    "save_annotation_list(annotation_list, annotation_list_csvfilepath)\n",
    "\n",
    "# run MACE\n",
    "mace_prefix = f\"check\"\n",
    "!java -jar MACE.jar --prefix $mace_prefix annotation_list.csv >/dev/null 2>&1\n",
    "\n",
    "# load the competence file and prediction file\n",
    "competence_file = f'{loc_tmp_dir}/{mace_prefix}.competence'\n",
    "competence_annotators = read_competence(competence_file, annotators)\n",
    "mace_prediction_file = f'{loc_tmp_dir}/{mace_prefix}.prediction'\n",
    "predictions = read_prediction(mace_prediction_file)\n",
    "\n",
    "raw_preds = {}\n",
    "for index, hitid in enumerate(hitids):\n",
    "    pred = predictions[index]\n",
    "    raw_preds[hitid] = pred\n",
    "\n",
    "max_num_instance = len([x[0] for x in annotation_list])\n",
    "\n",
    "# get the workers_removed\n",
    "print(\"-\" * 60)\n",
    "print(\"The Krippendorf's Alpha threshold is {}\".format(k_threshold))\n",
    "workers_removed = []\n",
    "for index, score_annotator in enumerate(competence_annotators):\n",
    "    rank = index + 1\n",
    "    score, annotator = score_annotator[0], score_annotator[1]\n",
    "\n",
    "    # calculate Krippendorf's Alpha\n",
    "    annotation_list, annotators, hitids = get_annotation_list(annotation_csv, workers_removed)\n",
    "    k_alpha = calculate_krippendorff(annotation_list)\n",
    "\n",
    "    # if calculated alpha is lower than the threshold, then continue to remove the annotators\n",
    "    if k_alpha <= k_threshold:\n",
    "        workers_removed.append(annotator)\n",
    "    else:\n",
    "        print(\"The overall k_alpah is {:.2f}\".format(k_alpha))\n",
    "        break\n",
    "\n",
    "print(\"You have removed {} annotators\".format(len(workers_removed)))\n",
    "\n",
    "# calculate the statistics\n",
    "filtered_annotations_of_instance = read_annotation_of_instance(annotation_csv, workers_removed)\n",
    "for index, hitid in enumerate(hitids):\n",
    "    mace_prediction = predictions[index]\n",
    "    if hitid in filtered_annotations_of_instance.keys():\n",
    "        annots = filtered_annotations_of_instance[hitid]\n",
    "        annots['adjudicated_label'] = mace_prediction\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "annot_context = {}\n",
    "\n",
    "for _, annot in filtered_annotations_of_instance.items():\n",
    "    instance_id = annot['Input.instance_id']\n",
    "    annot_context[instance_id] = annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_filename = 'batches/2_finished.csv'\n",
    "        \n",
    "# generate the html file\n",
    "i = 0\n",
    "html_filename = \"check_new_instances.html\"\n",
    "with open(html_filename, 'w') as htmlfile:\n",
    "\n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # write the start\n",
    "        htmlfile.write(\"<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Tweet Example</title>\\n</head>\\n<body>\\n\")\n",
    "\n",
    "        # use a big table to display the tweet example\n",
    "        htmlfile.write(\"\"\"<table style=\\\"margin-left: auto; margin-right: auto; border: 1px solid black; line-height: 1.0em; width: 1200\\\">\\n\"\"\")\n",
    "        image_css = \"\"\"style=\\\"display: block; margin-left: auto; margin-right: auto; max-width: 450px; max-height: 650px;\\\"\"\"\"\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            if int(row['Answer.Q2']) > 3.0 or row['Answer.Q1'] == 'No':\n",
    "                continue\n",
    "            \n",
    "            i += 1\n",
    "            if i == 100:\n",
    "                break\n",
    "            \n",
    "            # 3 context tweets before\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # context8_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context8_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context9_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context9_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context10_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context10_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "            # anchor tweet\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # anchor_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.anchor_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "            # 3 context tweets after\n",
    "            htmlfile.write(\"<tr>\\n\")\n",
    "\n",
    "            # context11_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context11_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context12_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context12_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            # context13_url\n",
    "            htmlfile.write(\"<td style=\\\"width:400; border-bottom: 1px solid black;\\\">\\n\")\n",
    "            htmlfile.write(f\"<img src=\\\"{row['Input.context13_url']}\\\" {image_css}>\\n\")\n",
    "            htmlfile.write(\"</td>\\n\")\n",
    "\n",
    "            htmlfile.write(\"</tr>\\n\")\n",
    "            \n",
    "        htmlfile.write(\"</table>\\n\")\n",
    "\n",
    "        # write the end\n",
    "        htmlfile.write(\"</body>\\n</html>\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
