{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from utils import preprocess\n",
    "from utils import learning_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/annotations/new_annot.json'\n",
    "mode = 'anchor_text_image'\n",
    "\n",
    "instances = preprocess.load_data(data_dir, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for BERT: 0m 41s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "instances = preprocess.add_bert_output(instances, anchor_only=True)\n",
    "end_time = time.time()\n",
    "elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "print(f\"Time spent for BERT: {elapsed_mins}m {elapsed_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaomin/envs/torch/lib/python3.8/site-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for VGG: 1m 17s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "instances = preprocess.add_vgg_output(instances, anchor_only=True)\n",
    "end_time = time.time()\n",
    "elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "print(f\"Time spent for VGG: {elapsed_mins}m {elapsed_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances, dev_instances, test_instances = preprocess.split_instances(instances)\n",
    "train_loader, dev_loader, test_loader = preprocess.get_data_loader(train_instances, dev_instances, test_instances, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from utils import evaluator\n",
    "from sklearn.metrics import classification_report\n",
    "from model.anchor_text_image import AnchorTextImageModel\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# define the label mapping\n",
    "label_to_idx = {'Yes': 1, 'No': 0}\n",
    "idx_to_label = {1: 'Yes', 0: 'No'}\n",
    "\n",
    "# parameter setting\n",
    "bert_feat_dim = 768\n",
    "vgg_feat_dim = 1000\n",
    "output_dim = 2\n",
    "hidden_dim = 4096\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 1e-03\n",
    "    \n",
    "# get the model based on mode and move model to GPU is GPU is available\n",
    "classifier = AnchorTextImageModel()\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# define the optimizer, loos function, and some parameters\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch: 1 || Epoch Time: 0m 3s\n",
      "Epoch: 1 || Train loss: 0.10, Train Acc: 0.62\n",
      "Epoch: 1 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "------------------------------------------------------------\n",
      "Epoch: 2 || Epoch Time: 0m 3s\n",
      "Epoch: 2 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 2 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 3 || Epoch Time: 0m 3s\n",
      "Epoch: 3 || Train loss: 0.04, Train Acc: 0.66\n",
      "Epoch: 3 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 4 || Epoch Time: 0m 3s\n",
      "Epoch: 4 || Train loss: 0.04, Train Acc: 0.68\n",
      "Epoch: 4 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 5 || Epoch Time: 0m 3s\n",
      "Epoch: 5 || Train loss: 0.04, Train Acc: 0.68\n",
      "Epoch: 5 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 6 || Epoch Time: 0m 3s\n",
      "Epoch: 6 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 6 || Dev loss: 0.05, Dev Acc: 0.35\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 7 || Epoch Time: 0m 3s\n",
      "Epoch: 7 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 7 || Dev loss: 0.05, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 8 || Epoch Time: 0m 3s\n",
      "Epoch: 8 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 8 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 9 || Epoch Time: 0m 3s\n",
      "Epoch: 9 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 9 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 10 || Epoch Time: 0m 3s\n",
      "Epoch: 10 || Train loss: 0.04, Train Acc: 0.68\n",
      "Epoch: 10 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 11 || Epoch Time: 0m 3s\n",
      "Epoch: 11 || Train loss: 0.04, Train Acc: 0.68\n",
      "Epoch: 11 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 12 || Epoch Time: 0m 3s\n",
      "Epoch: 12 || Train loss: 0.04, Train Acc: 0.68\n",
      "Epoch: 12 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "The loss on development set does not decrease, stop training!\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.00      0.00      0.00       226\n",
      "         Yes       0.68      1.00      0.81       473\n",
      "\n",
      "    accuracy                           0.68       699\n",
      "   macro avg       0.34      0.50      0.40       699\n",
      "weighted avg       0.46      0.68      0.55       699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "patience = 10\n",
    "best_valid_loss = float('inf')\n",
    "check_stopping = 0\n",
    "model_name = f'retrained_{mode}_classifier.pkl'\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = learning_helper.train(classifier, train_loader, optimizer, criterion, device, label_to_idx)\n",
    "    dev_loss, dev_acc = learning_helper.evaluate(classifier, dev_loader, criterion, device, label_to_idx)\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Epoch: {i+1} || Epoch Time: {elapsed_mins}m {elapsed_secs}s\")\n",
    "    print(f\"Epoch: {i+1} || Train loss: {train_loss:.02f}, Train Acc: {train_acc:.02f}\")\n",
    "    print(f\"Epoch: {i+1} || Dev loss: {dev_loss:.02f}, Dev Acc: {dev_acc:.02f}\")\n",
    "\n",
    "    # check if we need to save the model\n",
    "    if dev_loss < best_valid_loss:\n",
    "        check_stopping = 0\n",
    "        best_valid_loss = dev_loss\n",
    "        torch.save(classifier, model_name)\n",
    "    else:\n",
    "        check_stopping += 1\n",
    "        print(f\"The loss on development set does not decrease\")\n",
    "        if check_stopping == patience:\n",
    "            print(\"The loss on development set does not decrease, stop training!\")\n",
    "            break\n",
    "            \n",
    "classifier.eval()\n",
    "pred_labels = evaluator.test_model(classifier, test_loader, idx_to_label, device)\n",
    "gold_labels = [x['adjudicated_label'] for x in test_instances]\n",
    "print('-' * 60)\n",
    "print(classification_report(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complicated_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from utils import preprocess\n",
    "from utils import learning_helper\n",
    "\n",
    "data_dir = 'data/annotations/new_annot.json'\n",
    "mode = 'all_bert_lstm'\n",
    "\n",
    "instances = preprocess.load_data(data_dir, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3494/3494 [04:26<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for BERT: 4m 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "instances = preprocess.add_bert_output(instances, anchor_only=False)\n",
    "end_time = time.time()\n",
    "elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "print(f\"Time spent for BERT: {elapsed_mins}m {elapsed_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting additional features using SpaCy ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19933996399342cd82287513413dd96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent for SpaCy preprocessing: 2m 23s\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'anchor_addfeattensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eac0795a48c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0melapsed_mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_secs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time spent for SpaCy preprocessing: {elapsed_mins}m {elapsed_secs}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"additional feature dimension: {instances[0]['anchor_addfeattensor'].shape[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'anchor_addfeattensor'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# load MPQA lexicon\n",
    "mpqa_path = os.path.join('data', 'reference', 'MPQA_Lexicon')\n",
    "mpqa_lexicon = preprocess.load_mpqa(mpqa_path)\n",
    "\n",
    "def add_additional_features(instances, mpqa_lexicon):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe('emoji', first=True)\n",
    "    pbar = tqdm(total=len(instances))\n",
    "    for instance in instances:\n",
    "\n",
    "        location = instance['anchor_location']\n",
    "        keys = [key for key in instance.keys() if key.endswith(\"tweettext\")]\n",
    "        for key in keys:\n",
    "            \n",
    "            tweet = nlp(instance[key])\n",
    "            featkey = key.split(\"_\")[0] + \"_addfeat\"\n",
    "            addfeat = {}\n",
    "\n",
    "            # entirely uppercase words\n",
    "            num_entireuppercasewords = len(['x' for token in tweet if token.text.isupper()])\n",
    "            addfeat['num_entireuppercasewords'] = num_entireuppercasewords\n",
    "\n",
    "            # the number of URLs\n",
    "#             num_urls = len(['x' for token in tweet if token.text.startswith(\"http\")])\n",
    "            num_urls = len(['x' for token in tweet if token.like_url])\n",
    "            addfeat['num_urls'] = num_urls\n",
    "\n",
    "            # the number of exclamation marks\n",
    "            num_exclamationmarks = len(['x' for token in tweet if token.text == '!'])\n",
    "            addfeat['num_exclamationmarks'] = num_exclamationmarks\n",
    "\n",
    "            # the number of strongly subjective words in MPQA lexicon\n",
    "            num_strongsubj = len([token for token in tweet if token.text in mpqa_lexicon['strongsubj']])\n",
    "            addfeat['num_strongsubj'] = num_strongsubj\n",
    "\n",
    "            # the number of weakly subjective words in MPQA lexicon\n",
    "            num_weaksubj = len([token for token in tweet if token.text in mpqa_lexicon['weaksubj']])\n",
    "            addfeat['num_weaksubj'] = num_weaksubj\n",
    "\n",
    "            # the number of emoji\n",
    "            num_emoji = len(tweet._.emoji)\n",
    "            addfeat['num_emoji'] = num_emoji\n",
    "\n",
    "            # the three most common emoji (in the form of description)\n",
    "            emoji_desc_lists = [token._.emoji_desc for token in tweet if token._.is_emoji]\n",
    "            emoji_count = Counter(emoji_desc_lists).most_common(3)\n",
    "            for index, x in enumerate(emoji_count):\n",
    "                addfeat[f\"no.{index + 1}_emoji\"] = x[0]\n",
    "\n",
    "            # the number of tokens\n",
    "            num_tokens = len(tweet)\n",
    "            addfeat['num_tokens'] = num_tokens\n",
    "            \n",
    "            # the number of elongated words\n",
    "            elong_pattern = re.compile(\"([a-zA-Z])\\\\1{2,}\")\n",
    "            num_elong = len(['x' for token in tweet if bool(elong_pattern.search(token.text))])\n",
    "            addfeat['num_elong'] = num_elong\n",
    "            \n",
    "            # the number of hashtags\n",
    "            num_hashtags = len(['x' for token in tweet if token.text.startswith(\"#\")])\n",
    "            addfeat['num_hashtags'] = num_hashtags\n",
    "            \n",
    "            # the number of first letter uppercased words\n",
    "            num_uppercasewords = len(['x' for token in tweet if token.text[0].isupper()])\n",
    "            addfeat['num_uppercasewords'] = num_uppercasewords\n",
    "            \n",
    "            # the surround words/lemma/pos/hashtag/reply\n",
    "            contain_location = False\n",
    "            for token in tweet:\n",
    "                if location in token.text:\n",
    "                    contain_location = True\n",
    "                    \n",
    "                    # check if the location is included in a hashtag\n",
    "                    addfeat['loc_hashtag'] = '1' if token.text.startswith(\"#\") else '0'\n",
    "                    \n",
    "                    # check if the location is included in a mention\n",
    "                    addfeat['loc_mention'] = '1' if token.text.startswith(\"@\") else '0'\n",
    "                    \n",
    "#                     # previous word\n",
    "#                     previous_word = tweet[token.i-1].text if token.i > 0 else 'nan'\n",
    "#                     addfeat['previous_word'] = previous_word\n",
    "                    \n",
    "                    break\n",
    "            if not contain_location:\n",
    "                addfeat['loc_hashtag'] = '0'\n",
    "                addfeat['loc_mention'] = '0'\n",
    "\n",
    "            instance[featkey] = addfeat\n",
    "#             break\n",
    "        pbar.update(1)\n",
    "#         break\n",
    "    pbar.close()\n",
    "\n",
    "#     # modify the feature value\n",
    "#     instances = feat_polished(instances) \n",
    "                \n",
    "#     # convert the dict to tensor to learn the model\n",
    "#     feat_dicts = []\n",
    "#     for instance in instances:\n",
    "#         # ensure the order is the same as in the later part\n",
    "#         keys = sorted([key for key in instance.keys() if key.endswith(\"tweettext\")])\n",
    "#         for key in keys:\n",
    "#             featkey = key.split(\"_\")[0] + \"_addfeat\"\n",
    "#             feat_dicts.append(instance[featkey])\n",
    "#     dv = DictVectorizer(sparse=False)\n",
    "#     feat_vectorized = dv.fit_transform(feat_dicts)\n",
    "#     for index_outside, instance in enumerate(instances):\n",
    "#         small_feats = feat_vectorized[index_outside * 7:(index_outside + 1) * 7]\n",
    "#         # ensure the order is the same as the previous part\n",
    "#         keys = sorted([key for key in instance.keys() if key.endswith(\"tweettext\")])\n",
    "#         for index_inside, key in enumerate(keys):\n",
    "#             newfeatkey = key.split(\"_\")[0] + \"_addfeattensor\"\n",
    "#             feattensor = torch.FloatTensor(small_feats[index_inside]).unsqueeze(0).to('cpu')\n",
    "#             instance[newfeatkey] = feattensor\n",
    "\n",
    "            \n",
    "    return instances\n",
    "\n",
    "# extract additional features\n",
    "print(\"Extracting additional features using SpaCy ...\")\n",
    "start_time = time.time()\n",
    "instances = add_additional_features(instances, mpqa_lexicon)\n",
    "end_time = time.time()\n",
    "elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "print(f\"Time spent for SpaCy preprocessing: {elapsed_mins}m {elapsed_secs}s\")\n",
    "# print(f\"additional feature dimension: {instances[0]['anchor_addfeattensor'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def feat_transform(instances):\n",
    "    \n",
    "    numerical_features = [key for key, value in instances[0]['anchor_addfeat'].items() if isinstance(value, int)]\n",
    "    categorical_features = [key for key, value in instances[0]['anchor_addfeat'].items() if isinstance(value, str)]\n",
    "    \n",
    "    numerical_feat_stats = defaultdict(list)\n",
    "    categorical_feat_stats = []\n",
    "    \n",
    "    # combine all features to do the polish things\n",
    "    for instance in instances:\n",
    "        \n",
    "        # sort the key so that the procedure in the next step could be aligned\n",
    "        sorted_key = sorted([key for key in instance.keys() if key.endswith(\"addfeat\")])\n",
    "        all_instance_feats = [instance[key] for key in sorted_key]\n",
    "        for instance_feat in all_instance_feats:\n",
    "            \n",
    "            # deal with numerical features\n",
    "            for numer_feat in numerical_features:\n",
    "                feat = instance_feat[numer_feat]\n",
    "                numerical_feat_stats[numer_feat].append(feat)\n",
    "\n",
    "            # deal with categorical features\n",
    "            small_cate_feat = {}\n",
    "            for cate_feat in categorical_features:\n",
    "                small_cate_feat.update({cate_feat: instance_feat[cate_feat]})\n",
    "            categorical_feat_stats.append(small_cate_feat)\n",
    "    \n",
    "    # normalize the numerical features\n",
    "    normalized_numerical_features = defaultdict(list)\n",
    "    for numer_feat, value_list in numerical_feat_stats.items():\n",
    "        # add 1e-05 to avoid zero division\n",
    "        value_sum = sum(value_list) if sum(value_list) != 0 else sum(value_list)+1e-05\n",
    "        normalized_values = [float(i)/value_sum for i in value_list]\n",
    "        normalized_numerical_features[numer_feat] = normalized_values\n",
    "    \n",
    "    # attach normalized numerical features to instances dictionary\n",
    "    for numer_feat, value_list in normalized_numerical_features.items():\n",
    "        for index, instance in enumerate(instances):\n",
    "            # sort the key so that the procedure in the previous step could be aligned\n",
    "            sorted_key = sorted([key for key in instance.keys() if key.endswith(\"addfeat\")])\n",
    "            small_value_list = value_list[index*7: (index+1)*7]\n",
    "            for index2, key in enumerate(sorted_key):\n",
    "                feat_dict = instance[key]\n",
    "                feat_dict[numer_feat] = small_value_list[index2]\n",
    "                instance[key] = feat_dict\n",
    "                \n",
    "    # convert the categorical features to the arrays\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    feat_vectorized = dv.fit_transform(categorical_feat_stats)\n",
    "    \n",
    "    # concatenate the categorical features and numerical features of each instance\n",
    "    for index_outside, instance in enumerate(instances):\n",
    "        small_feats = feat_vectorized[index_outside * 7:(index_outside + 1) * 7]\n",
    "        # ensure the order is the same as the previous step\n",
    "        sorted_key = sorted([key for key in instance.keys() if key.endswith(\"addfeat\")])\n",
    "        for index_inside, key in enumerate(sorted_key):\n",
    "            newfeatkey = key.split(\"_\")[0] + \"_addfeattensor\"\n",
    "            categorical_part = torch.FloatTensor(small_feats[index_inside])\n",
    "            numerical_part = torch.FloatTensor([instance[key][feat] for feat in numerical_features])\n",
    "            combined_feat = torch.cat((categorical_part, numerical_part), dim=0)\n",
    "            feattensor = combined_feat.unsqueeze(0).to('cpu')\n",
    "            instance[newfeatkey] = feattensor\n",
    "#         break\n",
    "\n",
    "    return instances\n",
    "\n",
    "temp_instances = copy.deepcopy(instances)\n",
    "temp_instances = feat_transform(temp_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor_location': 'Atlanta',\n",
       " 'instance_id': 'thanksgiving2019_1199840018380001281',\n",
       " 'event': 'thanksgiving2019',\n",
       " 'Answer.Q1_A2ZJS73XSSMRTD': 'Yes',\n",
       " 'Answer.Q2_A2ZJS73XSSMRTD': '4',\n",
       " 'adjudicated_label': 'Yes',\n",
       " 'anchor_timestamp': 'Wed Nov 27 23:58:46 +0000 2019',\n",
       " 'anchor_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/anchor_1199840018380001281.json',\n",
       " 'anchor_tweettext': 'Just a sampling of our Thanksgiving Eve feast. Special shout-out to @FOX5ATLCallaway and @Elepo for organizing it!!!!!! @FOX5Atlanta https://t.co/w6k2p0c46u',\n",
       " 'anchor_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_anchor_1199840018380001281.png',\n",
       " 'anchor_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/anchor_1199840018380001281.jpg',\n",
       " 'context8_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199503490923536389.json',\n",
       " 'context8_tweettext': '@aungeliquefox5 Yes! And so generous !',\n",
       " 'context8_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199503490923536389.png',\n",
       " 'context8_timestamp': 'Wed Nov 27 01:41:32 +0000 2019',\n",
       " 'context9_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199732704411082752.json',\n",
       " 'context9_tweettext': '@AlexaLiackoFOX5 YOU NAME IT!!!!',\n",
       " 'context9_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199732704411082752.png',\n",
       " 'context9_timestamp': 'Wed Nov 27 16:52:21 +0000 2019',\n",
       " 'context10_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199744669372030978.json',\n",
       " 'context10_tweettext': \"YES! I am SO happy for Celine. She has one of the greatest voices in music. And she's been through so much. No one deserves this more! https://t.co/3v6CjjiSDX\",\n",
       " 'context10_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199744669372030978.png',\n",
       " 'context10_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1199744669372030978.jpg',\n",
       " 'context10_timestamp': 'Wed Nov 27 17:39:54 +0000 2019',\n",
       " 'context11_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199840318230794241.json',\n",
       " 'context11_tweettext': 'Mood when I saw @FOX5ATLCallaway slicing our @FOX5Atlanta Thanksgiving Eve Turkey!!! YAAASSSSSS!!! https://t.co/gBosgWALqr',\n",
       " 'context11_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199840318230794241.png',\n",
       " 'context11_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1199840318230794241.jpg',\n",
       " 'context11_timestamp': 'Wed Nov 27 23:59:58 +0000 2019',\n",
       " 'context12_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199877306862522373.json',\n",
       " 'context12_tweettext': '@CityStonecrest @FOX5ATLCallaway @Elepo @FOX5Atlanta GONE!!! LOL',\n",
       " 'context12_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199877306862522373.png',\n",
       " 'context12_timestamp': 'Thu Nov 28 02:26:57 +0000 2019',\n",
       " 'context13_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1200164708428525572.json',\n",
       " 'context13_tweettext': 'I have SO much to be thankful for! Take time to spend this holiday with the people you love. @FOX5Atlanta https://t.co/dGVNcaOIo3',\n",
       " 'context13_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1200164708428525572.png',\n",
       " 'context13_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1200164708428525572.jpg',\n",
       " 'context13_timestamp': 'Thu Nov 28 21:28:59 +0000 2019',\n",
       " 'tasktype': 'all_bert_lstm',\n",
       " 'anchor_bertoutput': tensor([[-1.0482e-03, -2.5729e-01,  1.1928e-01, -1.2261e-01, -3.1548e-01,\n",
       "          -6.6459e-01,  2.2888e-01,  7.8913e-01, -8.7957e-03, -4.2319e-01,\n",
       "          -1.9068e-01, -3.2553e-01,  1.3188e-01,  2.3773e-01,  2.3905e-01,\n",
       "           1.1760e-01, -1.1538e-01,  7.6703e-01,  3.0015e-01,  1.8020e-02,\n",
       "          -2.9057e-01, -1.1390e+00,  2.5464e-01,  1.2548e-02,  1.0971e-01,\n",
       "          -1.1311e-02,  6.7778e-02, -1.0386e-01,  1.0003e-01, -1.7049e-01,\n",
       "          -4.6367e-01,  3.4323e-01,  1.2479e-01, -3.0393e-01,  5.8957e-01,\n",
       "          -2.3230e-01,  7.8672e-02, -5.2385e-02,  4.2112e-01,  5.9248e-01,\n",
       "           4.7237e-02, -1.8949e-01,  2.8657e-01,  4.6666e-02, -1.5309e-02,\n",
       "          -2.5098e-01, -3.2472e+00,  4.1862e-01, -3.1534e-01, -3.4627e-01,\n",
       "           3.9896e-01, -1.8431e-01,  3.3800e-01, -1.6887e-02,  1.6463e-01,\n",
       "           5.0306e-01, -8.5692e-01,  2.9515e-01,  1.3279e-01,  2.8370e-01,\n",
       "          -1.5535e-01,  8.2629e-02, -1.7905e-01,  1.2290e-02, -2.4404e-01,\n",
       "          -4.6582e-03,  2.8956e-02,  5.4471e-01, -1.3896e-01,  7.4682e-01,\n",
       "          -1.7067e-01, -1.4863e-01,  1.4635e-01,  1.4046e-02,  1.5780e-01,\n",
       "          -3.4148e-02, -1.7736e-01,  3.4897e-01, -2.3438e-01,  1.9909e-01,\n",
       "           6.0286e-02,  3.2497e-01,  3.4102e-01,  1.1904e-01,  2.7982e-01,\n",
       "           6.8262e-01, -4.9862e-01, -6.4719e-01,  2.9266e-01,  3.1317e-01,\n",
       "          -1.0199e-01,  1.8651e-01,  3.5646e-02,  2.7780e-01,  7.4773e-01,\n",
       "          -3.2420e-01,  2.0844e-02, -1.3916e-01,  1.0248e-01,  3.3874e-01,\n",
       "          -7.4268e-02,  2.7164e-01,  1.2590e-01, -6.2787e-01, -1.2296e-01,\n",
       "           1.0701e-01,  1.1634e-01, -4.2285e-01,  1.5367e-01, -2.5409e+00,\n",
       "           5.5335e-01,  7.1096e-01, -2.4441e-01, -4.9853e-01,  1.7554e-01,\n",
       "           1.1972e-01,  6.1297e-01, -2.8369e-01,  1.0124e-01, -1.1605e-01,\n",
       "          -1.7227e-01,  8.0959e-01,  2.1943e-01, -4.6777e-01,  1.8172e-01,\n",
       "           3.7042e-01,  2.9763e-01, -4.1704e-02,  2.4668e-01,  1.4464e-01,\n",
       "           1.7735e-01,  3.4941e-01, -2.5613e-01, -3.7278e-01, -1.1967e-01,\n",
       "          -3.4330e-02,  1.5091e-01, -3.2924e-01, -8.0266e-02, -3.3325e-02,\n",
       "          -5.7399e-01, -3.3602e-01, -3.1637e+00,  2.2387e-01,  5.7452e-01,\n",
       "           3.2972e-01, -3.7101e-01,  9.6824e-02, -3.3748e-01, -1.5760e-01,\n",
       "           2.2497e-02,  1.5217e-01, -4.6963e-01,  1.0252e-01, -4.7419e-01,\n",
       "           3.7641e-01, -8.5609e-02, -9.8637e-02, -4.5077e-02,  6.9241e-01,\n",
       "           4.8177e-01,  2.6165e-01,  3.6990e-02, -5.8185e-02, -1.2566e-01,\n",
       "           3.6285e-01,  2.5654e-01,  2.1085e-01, -2.6994e-02, -5.1835e-01,\n",
       "          -1.5803e-01,  6.5462e-02,  2.8819e-01,  4.8700e-02,  5.4976e-02,\n",
       "           5.6994e-02,  1.5312e-01,  1.4373e-01,  1.8096e-01, -4.9712e-02,\n",
       "          -3.9713e-01, -2.2884e-01, -3.6905e-02,  2.9435e-02,  2.0948e-01,\n",
       "          -2.3954e-01,  4.1436e-01,  4.1196e-02, -1.4886e-01,  4.0108e-01,\n",
       "          -3.8985e-01, -7.4286e-02,  3.6580e-01,  1.7958e-01,  5.0061e-01,\n",
       "           2.0287e-01,  1.0547e-01, -1.4305e-01,  3.7555e-01,  1.5402e-01,\n",
       "          -4.5919e-02, -1.3530e-01,  1.6957e-01,  1.4095e-01, -3.7242e-01,\n",
       "           3.9451e+00, -6.7182e-02, -2.3182e-01, -4.0348e-03, -8.0239e-02,\n",
       "          -5.9293e-02,  1.4558e-01, -8.0523e-02,  1.3922e-01, -7.5189e-02,\n",
       "           1.7329e-02,  4.5110e-01,  7.2615e-03, -1.7468e-01,  1.3680e-01,\n",
       "           2.0867e-01,  4.2675e-01, -1.5172e-01,  4.8965e-02, -2.9435e-01,\n",
       "           4.5027e-02,  1.4387e-01,  3.1474e-01,  3.2538e-01, -1.3017e+00,\n",
       "          -3.2791e-02, -1.7358e-01, -5.5696e-01,  4.0405e-01,  2.6508e-01,\n",
       "           1.8945e-02, -8.6965e-02, -3.5820e-01, -2.8465e-03,  1.8994e-01,\n",
       "           1.8139e-01,  3.1696e-01, -4.8503e-02,  1.0138e-01, -3.8723e-01,\n",
       "           3.2056e-01,  4.9152e-01, -1.3614e-01,  4.1194e-01, -2.5032e-01,\n",
       "           5.8023e-01, -3.1255e-02,  3.7645e-01, -2.6553e-01,  3.2994e-01,\n",
       "           1.5167e-01,  6.3366e-02, -2.8122e-01, -7.3601e-01,  7.2692e-02,\n",
       "          -3.2170e-01, -7.4379e-02,  7.7105e-02, -9.5179e-02, -2.4945e-01,\n",
       "          -6.0434e-01,  7.3807e-02, -5.0302e-01,  2.4058e-01, -7.2893e-02,\n",
       "           1.5707e-01, -2.6105e-01, -4.3791e-01, -3.3962e+00,  3.8929e-01,\n",
       "           1.6738e-01,  1.2386e-01,  3.0879e-01,  1.3002e-01, -1.0251e-01,\n",
       "          -9.4501e-02, -8.2729e-02, -1.8439e-01,  7.0507e-01,  2.6435e-01,\n",
       "          -4.8793e-01,  1.9306e-01, -2.4146e-01,  1.0205e-01,  5.5074e-01,\n",
       "          -6.6955e-02, -3.8071e-01, -6.2997e-02, -1.2964e-01,  3.8681e-01,\n",
       "          -1.1686e-01,  4.4354e-01, -1.1702e-01, -2.8210e-01, -1.4885e-01,\n",
       "          -5.0415e-01,  1.4392e-01, -3.4418e-01,  2.4321e-04, -2.5630e-01,\n",
       "           2.6870e-01, -1.1339e-01, -4.6154e-01, -3.1843e+00,  3.0560e-02,\n",
       "          -2.9670e-01, -2.4515e-01, -1.2027e-01, -3.8288e-03,  5.2688e-01,\n",
       "           5.4784e-02,  2.1706e-01, -1.9247e-01,  1.6590e-01,  3.1182e-03,\n",
       "           2.7120e-01, -6.3410e-02,  2.6728e-01,  3.1071e-01,  2.1849e-01,\n",
       "           2.0671e-01,  4.3960e-01, -9.0247e-03, -3.1787e-01,  8.3083e-02,\n",
       "           1.2372e-01,  3.3194e-02,  3.0579e-01, -8.9054e-02, -3.7660e-01,\n",
       "          -1.7207e-01, -5.7285e-02,  3.1370e-01,  5.8456e-01,  5.1620e-02,\n",
       "          -3.6413e-02, -1.3180e-01, -6.0172e-01, -2.0363e-01,  3.1592e-01,\n",
       "           1.2545e-01,  2.3003e-01, -5.1845e-02,  2.1207e-01,  4.0727e-01,\n",
       "          -1.9998e-01,  1.6721e-01,  8.0143e-01,  8.9030e-03,  1.4495e-01,\n",
       "           1.0361e-01,  2.1196e-01,  2.8250e-01, -2.0435e-01,  1.4986e-01,\n",
       "           1.2590e+00, -7.2105e-02,  9.3788e-02, -2.7144e-01,  2.9420e-01,\n",
       "           3.2462e-01,  1.4345e-01,  2.9198e-01,  7.0772e-01, -1.4188e-01,\n",
       "           4.2995e-01, -1.6661e-02, -5.5398e-02, -3.7198e-01,  2.7800e-01,\n",
       "          -5.1739e-01, -3.9509e-01,  1.8977e-02, -1.4574e-01,  2.6899e-01,\n",
       "          -1.8042e-01, -1.0217e+00,  7.1954e-02,  5.9463e-02, -4.3718e-01,\n",
       "           1.5299e-01,  2.2748e-02,  3.6690e-01,  1.5625e-02, -2.7084e-01,\n",
       "          -2.4796e-01,  5.8655e-01, -5.2423e-01, -3.1262e-03, -2.8532e-02,\n",
       "           8.9963e-03, -7.3095e-01,  4.8528e-02, -3.9787e-01,  4.7035e-01,\n",
       "           7.8678e-02,  4.0333e-01, -3.1197e-01,  2.8157e-01,  5.7213e-01,\n",
       "          -8.2634e-01,  3.4116e-01,  1.0663e-01,  4.0631e-01,  2.1564e-01,\n",
       "          -2.3285e-02, -7.9841e-02, -4.1606e-01, -2.4874e-02, -9.8640e-02,\n",
       "           1.3882e-01,  1.0505e-01,  1.0125e-01, -1.6578e-01,  2.8497e-02,\n",
       "          -4.8532e-01, -1.8736e-01,  6.3282e-01, -8.0467e-02,  3.5670e-02,\n",
       "           2.1266e-01,  2.8402e-01,  1.3710e-02,  2.2953e-01, -4.5354e-03,\n",
       "           1.1693e-02,  4.0288e-01, -2.5893e-01,  2.8823e-02,  1.8884e-01,\n",
       "          -2.7691e-01, -1.5450e-01, -2.2008e-01, -8.8823e-02, -2.5042e-01,\n",
       "          -4.1457e-01, -2.2604e-01,  1.0482e-01, -2.7354e-01,  9.9801e-02,\n",
       "          -2.4342e-02,  3.0290e-01, -3.2399e-01,  4.3769e-01, -1.3309e-01,\n",
       "          -3.1790e-01,  6.2723e-01, -4.2893e-01,  8.1298e-01,  9.2826e-02,\n",
       "           1.9580e-01, -7.1877e-01,  4.8621e-01, -2.7269e-01, -2.5067e-02,\n",
       "          -1.4993e-01, -3.5220e-01,  1.9343e-01,  1.3697e-01, -3.1610e-02,\n",
       "           8.7836e-03,  3.1655e-02, -1.1569e-01,  7.1475e-02, -5.3249e-02,\n",
       "          -1.5349e+00,  5.3972e-01,  1.9679e-01,  1.7636e-01,  8.3878e-02,\n",
       "          -2.6501e-01, -2.4788e-01,  7.1265e-02, -3.5299e-01,  6.1364e-01,\n",
       "          -3.2362e-01,  1.3719e-01, -1.3289e-02, -8.1975e-02,  2.8623e-01,\n",
       "          -1.2333e-01,  1.1358e-01, -1.0940e-02, -2.6679e-01, -9.9395e-02,\n",
       "          -5.3853e-02,  5.7224e-01,  2.3877e-01, -2.4549e-02,  2.4334e-01,\n",
       "           6.4703e-02, -6.6936e-02,  5.0919e-01, -3.1020e-01,  1.2311e-01,\n",
       "          -1.1847e-01, -6.3275e-01, -5.1668e-01, -1.6953e-01,  3.6252e-01,\n",
       "           2.0201e-01,  9.7977e-02, -2.1422e-01,  2.9732e-01,  6.2656e-01,\n",
       "          -7.3868e-01,  4.9580e-01,  2.8740e-01,  4.5619e-01,  6.4225e-01,\n",
       "           3.2299e-01, -1.9856e-01,  5.5574e-02,  1.5609e-01, -3.7247e-01,\n",
       "           1.7634e-01, -4.5426e-02, -1.1679e-01, -2.3143e-01, -2.9611e-01,\n",
       "           3.2146e-02,  2.6294e-01, -2.1743e-02, -3.6431e-01,  1.7164e-01,\n",
       "           4.2842e-01, -3.7874e-01, -3.4615e-01,  3.9182e-01, -3.6419e-01,\n",
       "          -6.7634e-01, -2.4695e-02, -2.5942e-01, -2.0296e-03,  1.2898e-01,\n",
       "           2.3258e-01,  2.4517e-01, -5.8571e-01,  5.2072e-02, -2.8634e-01,\n",
       "           1.4688e-01,  3.4738e-02,  2.2187e-01, -1.0636e-01, -4.3186e-01,\n",
       "           3.1564e-01, -7.8181e-01, -6.9003e-01,  1.8072e-01, -4.5540e-01,\n",
       "           3.0232e-01, -1.2964e-01, -2.2538e-01, -6.6599e-02, -7.3139e-02,\n",
       "          -2.4897e-01, -2.8941e-01,  2.2148e-01,  7.1984e-02,  4.6136e-02,\n",
       "           2.2793e-01, -1.2678e-01, -6.2039e-02, -1.4155e-01,  1.3933e-01,\n",
       "          -2.0433e-01,  6.1944e-01,  5.1771e-01,  4.8736e-01,  3.0090e-01,\n",
       "          -1.0516e-01,  2.9312e-01,  7.3497e-02, -2.5057e-01,  2.6956e-01,\n",
       "          -1.4358e-01, -3.5738e-02, -2.1264e-01, -6.6933e-02,  6.8603e-02,\n",
       "          -2.2649e-01,  2.0529e-01, -1.2368e-01,  2.0347e+00, -1.1355e-01,\n",
       "           1.8034e-01, -4.9095e-02,  4.3243e-01, -1.3320e-01, -5.9744e-02,\n",
       "           4.4345e-02, -3.6436e-01,  6.3610e-01, -5.3333e-01,  4.6333e-01,\n",
       "          -4.1282e-01,  9.8024e-02,  4.3406e-01,  1.9066e-01,  2.5514e-01,\n",
       "          -4.6091e-01, -2.2978e-01,  2.0393e-01, -3.5717e-01,  1.1079e-01,\n",
       "           5.2232e-01, -1.3881e-01, -5.5189e-02,  2.6908e-01, -3.5076e-01,\n",
       "          -2.2983e-01, -5.1115e-02,  3.3836e-01, -7.2604e-02, -9.1474e-02,\n",
       "           7.5378e-02,  6.3283e-01, -2.7863e-01,  2.0160e-01, -2.1314e-01,\n",
       "          -4.9293e-01, -3.7057e-01,  1.1913e-02, -1.5901e-01, -3.7352e-01,\n",
       "           4.3060e-01,  7.9225e-03, -2.5952e-01,  4.5463e-01, -1.9992e-01,\n",
       "          -2.9355e-01,  4.5305e-01, -5.1876e-02, -2.5070e-02, -3.3592e-02,\n",
       "          -3.4616e-01,  2.5589e-01,  5.2820e-02, -3.1760e-01, -6.3965e-02,\n",
       "          -1.3154e-01, -4.0654e-01,  6.7996e-01, -8.1721e-02,  3.7000e-02,\n",
       "           8.3064e-02, -3.5185e-01,  3.7160e-01, -1.7806e-01, -3.1538e-01,\n",
       "           3.8143e-01,  1.6058e-01, -3.6552e-01, -2.3022e-01,  4.5374e-01,\n",
       "           4.2553e-01, -6.9647e-02,  2.2114e-01,  1.6913e-01,  3.4824e-01,\n",
       "          -2.7192e-01, -6.3996e-02, -2.8271e+00,  5.0649e-01,  3.1246e-01,\n",
       "          -8.4628e-02, -3.6598e-03,  1.2312e-01, -2.5420e-01, -9.3925e-02,\n",
       "           3.4446e-01,  9.5365e-02,  4.0609e-02,  3.6741e-01,  2.6561e-01,\n",
       "          -1.7026e-01, -7.7918e-02, -2.1831e-02,  4.9750e-01,  2.0602e-01,\n",
       "          -2.9465e-01,  2.7746e-01,  1.3281e-01,  1.7599e-01, -2.7731e-01,\n",
       "          -2.7560e-01, -6.1330e-02,  9.4316e-02,  3.6012e-02, -4.1412e-01,\n",
       "           2.6865e-01,  1.6561e-01, -2.6231e-01,  3.3875e-01, -1.1595e-01,\n",
       "           2.6824e-02,  1.3895e-01,  2.2289e-03, -3.3380e-01, -2.0498e-02,\n",
       "           8.7858e-02,  1.3711e-01,  1.0694e-01,  3.6067e-01, -1.9847e-01,\n",
       "           1.6899e-01, -7.8794e-02,  2.1130e-01,  3.8939e-01,  6.2351e-02,\n",
       "           2.0681e-01, -3.1245e-01,  2.1559e-01, -1.2220e-01,  1.7295e-01,\n",
       "          -3.1832e-01,  2.2826e-01, -6.3986e-02,  7.3632e-02,  2.1416e-01,\n",
       "          -5.4866e-02, -6.3566e-01, -3.4950e-01,  1.1377e-01, -6.9246e-02,\n",
       "           1.1493e-01,  2.1244e-01, -1.8061e-01, -1.7882e-01, -1.7225e-01,\n",
       "          -2.1278e-02, -3.5505e-01, -2.5188e-01,  1.6567e-03,  4.9576e-02,\n",
       "           2.7105e-01, -5.8853e-02, -1.8832e-01,  7.1872e-01,  1.9114e-01,\n",
       "           6.9103e-01,  2.7297e-01,  6.8250e-02, -4.1881e-01, -1.2049e-01,\n",
       "           2.2634e-01,  1.0026e-02, -7.4655e+00, -6.1580e-02, -1.0637e-01,\n",
       "          -5.2997e-01, -6.0576e-01, -6.6485e-01, -1.2656e-02, -1.3275e-01,\n",
       "           4.8254e-02, -1.1795e-01,  2.3792e-01, -3.3106e-02, -3.4893e-01,\n",
       "          -4.4167e-02,  1.7845e-01,  4.5434e-01]]),\n",
       " 'context8_bertoutput': tensor([[ 1.8129e-01, -6.6487e-02,  3.7492e-01, -2.1685e-01, -4.1043e-01,\n",
       "          -4.8909e-02,  4.9164e-01,  4.3803e-01,  6.5572e-02, -3.5612e-01,\n",
       "           3.0142e-02, -2.2067e-02,  3.8208e-01,  5.7076e-01,  2.1715e-01,\n",
       "          -1.6895e-01, -3.1602e-01,  6.5017e-01,  3.8200e-01,  1.1598e-02,\n",
       "          -1.4895e-01, -3.5974e-01,  3.8509e-01, -2.0933e-02,  3.7444e-01,\n",
       "          -1.5990e-01,  1.3275e-01, -2.7617e-01, -1.1901e-02,  3.5013e-02,\n",
       "          -2.1265e-01,  3.5229e-01, -1.9967e-01, -1.5123e-01,  1.5203e-01,\n",
       "          -3.5447e-01,  7.8018e-02, -7.0498e-03, -4.8682e-02,  1.0903e-01,\n",
       "          -1.5862e-01, -3.4066e-01,  4.0399e-01,  2.1132e-01, -8.7849e-02,\n",
       "          -2.0656e-01, -3.1618e+00, -5.3806e-02, -6.7830e-02, -6.2952e-02,\n",
       "           5.5787e-01, -3.2918e-02,  2.8456e-02, -1.7223e-02,  2.6465e-02,\n",
       "           4.1792e-01, -8.4313e-01,  6.8511e-01, -2.7530e-02, -9.5853e-03,\n",
       "          -9.1602e-03,  2.3552e-01,  1.3376e-02,  3.3128e-01,  3.1564e-02,\n",
       "           4.2682e-02, -2.8194e-02,  5.1180e-01, -1.5409e-01,  5.3258e-01,\n",
       "          -6.0869e-02, -2.9008e-01,  4.4113e-01, -3.1027e-02, -1.3711e-01,\n",
       "          -3.5618e-01, -3.3306e-01,  3.9691e-01, -3.6170e-01,  3.9123e-01,\n",
       "          -1.6916e-01,  2.3394e-01,  1.2811e-01,  4.3068e-01,  2.0952e-01,\n",
       "           5.2067e-01,  9.1910e-02, -3.0584e-01,  2.7298e-01,  3.6920e-01,\n",
       "          -1.3791e-01, -6.3223e-02,  3.1238e-02,  7.3586e-02,  6.9094e-01,\n",
       "          -3.9593e-02,  2.1062e-01, -9.7339e-02,  1.6114e-01,  1.5849e-01,\n",
       "          -2.6152e-02,  4.6492e-02, -3.5832e-03, -1.1022e+00,  7.2000e-02,\n",
       "           2.9694e-01, -8.4004e-02, -2.7671e-01,  3.5122e-01, -2.6954e+00,\n",
       "           4.6861e-01,  4.6993e-01, -3.8600e-01, -3.2457e-01, -1.5952e-01,\n",
       "           4.6853e-01,  5.6098e-01, -6.4170e-02,  1.0056e-02, -2.1695e-01,\n",
       "           2.6931e-01,  2.8046e-01,  1.6356e-01, -2.2988e-01,  1.5835e-01,\n",
       "           5.6294e-01,  3.4552e-01,  1.6842e-01,  3.4810e-01,  1.7279e-01,\n",
       "           2.4504e-01,  4.7417e-01, -1.8844e-01, -5.2706e-01, -3.7639e-03,\n",
       "          -8.8076e-02,  1.2223e-01, -5.0020e-01, -3.1363e-01, -3.2248e-01,\n",
       "          -5.6366e-01, -3.9724e-01, -2.8044e+00,  5.2918e-01,  6.3970e-01,\n",
       "           2.6033e-01, -2.9261e-01,  9.9170e-02, -1.7490e-01,  3.3766e-02,\n",
       "          -1.5762e-02,  3.3615e-02, -4.1433e-01,  1.6187e-01, -1.7221e-01,\n",
       "           4.9630e-02,  3.5693e-01, -4.4990e-01,  4.2231e-02,  8.2378e-01,\n",
       "           1.8118e-01,  7.6345e-02,  1.8464e-01, -3.6336e-01, -6.4437e-01,\n",
       "          -9.5390e-02,  1.8404e-01,  3.8786e-01, -3.9483e-02, -1.9045e-01,\n",
       "           3.6965e-03, -1.2087e-03,  1.9736e-01,  1.1386e-01,  1.5878e-01,\n",
       "          -1.7743e-01,  3.6485e-01,  7.1096e-03,  2.7191e-01, -9.5773e-02,\n",
       "          -3.1150e-01,  3.4872e-01,  2.5486e-01,  3.9375e-02,  4.5455e-01,\n",
       "          -2.1316e-02,  4.0598e-01,  9.9982e-02, -2.3021e-01,  3.2695e-01,\n",
       "          -1.6677e-01, -4.9183e-01,  1.5022e-01,  3.5771e-02,  5.6405e-01,\n",
       "           2.2569e-01,  5.1836e-01, -2.7818e-01,  3.3748e-01,  1.8887e-01,\n",
       "           9.0683e-02,  1.5336e-01,  8.4752e-03, -1.3893e-02, -3.0953e-01,\n",
       "           3.9089e+00,  9.5045e-02, -1.2947e-01, -7.4832e-02, -4.7023e-02,\n",
       "          -3.7762e-01,  1.1006e-01,  1.6835e-01, -1.8942e-01, -4.3553e-01,\n",
       "           4.6133e-02,  5.5390e-01,  9.2564e-02, -1.0508e-01,  1.8799e-01,\n",
       "           2.4563e-01,  2.3883e-01, -1.0075e-01,  1.8989e-02,  1.0632e-01,\n",
       "          -1.8059e-02,  4.3706e-03, -2.0298e-01, -1.6083e-04, -1.1766e+00,\n",
       "          -3.6804e-01, -2.3676e-01, -2.5472e-01,  2.9528e-01,  4.3664e-02,\n",
       "           7.8891e-02,  2.0951e-01, -1.4517e-02, -3.6290e-02,  1.3363e-01,\n",
       "           2.4941e-01,  3.5924e-01, -1.9985e-01, -5.4915e-02, -4.8925e-01,\n",
       "           2.7269e-01,  2.4336e-01, -1.5414e-01,  3.4345e-01, -1.3030e-01,\n",
       "           4.2222e-01, -6.8656e-02, -2.3053e-01, -3.0622e-01,  7.9261e-02,\n",
       "          -2.2280e-01,  3.4006e-01,  1.7788e-01, -7.6957e-01,  1.4497e-02,\n",
       "          -3.1425e-01,  3.1725e-02, -1.1773e-01,  4.9579e-01, -2.1629e-01,\n",
       "           2.4266e-02,  1.9894e-02, -4.8669e-01,  1.6035e-01, -2.0814e-01,\n",
       "          -1.8842e-01, -1.9010e-01, -3.0957e-01, -3.4380e+00,  2.8371e-02,\n",
       "          -1.2617e-01,  4.3237e-01,  6.1335e-01, -6.6989e-02, -2.4996e-01,\n",
       "           9.2537e-02,  2.6849e-01, -3.8375e-01,  2.8105e-01,  3.6061e-01,\n",
       "          -3.2135e-01,  3.0309e-01, -1.7222e-01, -6.7592e-02,  9.6644e-02,\n",
       "          -2.0703e-01, -5.1872e-01, -1.2780e-01, -6.6960e-02,  5.7216e-01,\n",
       "          -3.7951e-01,  5.6461e-01,  3.8728e-01, -3.6762e-01,  1.1271e-01,\n",
       "          -2.1147e-01, -9.0132e-02,  9.6932e-02, -9.0813e-02, -7.5107e-02,\n",
       "           3.7596e-02,  6.9509e-02, -4.5172e-01, -2.9287e+00, -1.6118e-01,\n",
       "          -1.4840e-01, -2.1079e-01,  4.8931e-02,  1.4814e-01,  6.9574e-01,\n",
       "           1.9801e-02, -9.9222e-02,  1.7955e-01,  1.2781e-01,  4.8760e-02,\n",
       "           1.3833e-01,  1.9747e-01,  6.8632e-02,  2.8715e-01,  3.2552e-01,\n",
       "          -3.9671e-02,  8.2760e-01, -1.1508e-01, -3.3759e-01, -4.4749e-02,\n",
       "           3.0303e-02,  3.2486e-01,  2.4054e-01,  2.0124e-02, -2.4277e-01,\n",
       "          -5.0662e-01, -2.2190e-01,  2.8068e-01,  1.9561e-01, -2.8465e-02,\n",
       "           6.5637e-02, -4.2212e-01, -6.4528e-01, -3.9414e-01,  3.9034e-01,\n",
       "          -1.6046e-01,  1.7717e-02, -1.8584e-01,  4.1646e-01,  3.5873e-01,\n",
       "          -3.2450e-04,  3.8475e-01,  2.8870e-01, -2.4772e-01, -4.2589e-01,\n",
       "          -3.9004e-02,  3.6784e-01,  5.7449e-02, -4.4623e-01, -4.0270e-02,\n",
       "           1.2010e+00, -4.2115e-01,  1.3076e-01, -2.2062e-01,  1.4944e-01,\n",
       "           2.3388e-01,  2.0710e-01,  3.0420e-01,  9.6386e-01,  4.7251e-02,\n",
       "           3.2630e-01, -2.3680e-01, -2.5885e-01, -1.8418e-01, -1.6909e-01,\n",
       "          -5.0288e-01, -3.8158e-01,  3.7927e-02, -2.8189e-02,  3.3216e-01,\n",
       "          -7.1635e-02, -1.0037e+00,  1.0898e-01,  4.0595e-02, -1.6994e-01,\n",
       "           1.3234e-01,  1.9207e-02,  1.4985e-01, -4.3528e-01, -2.6969e-01,\n",
       "          -1.2398e-01,  4.7911e-01, -5.7829e-01, -1.1770e-01, -1.9084e-01,\n",
       "          -1.0363e-01, -6.4575e-01, -1.7392e-01, -2.1104e-01,  2.0545e-02,\n",
       "          -2.1859e-01,  4.9722e-01,  4.0172e-03,  4.1859e-01,  2.3977e-01,\n",
       "          -6.0526e-01,  1.0775e-01, -2.9949e-01,  3.2327e-01, -1.2946e-02,\n",
       "          -4.7811e-02,  1.6193e-01, -2.5956e-01, -9.6951e-03, -3.8270e-01,\n",
       "           1.3872e-01,  7.1865e-02,  2.7885e-01, -4.7364e-02, -3.2564e-02,\n",
       "          -4.5917e-01,  3.5659e-01,  7.5084e-01,  2.1499e-01,  6.4351e-03,\n",
       "           3.1885e-01,  3.9327e-01, -1.8110e-02,  3.4025e-01,  9.2996e-02,\n",
       "          -1.1426e-01,  3.9965e-01, -2.8004e-01,  2.2800e-02,  2.8878e-01,\n",
       "          -5.8822e-03, -3.9693e-01, -5.6283e-01,  3.1874e-01, -2.5963e-01,\n",
       "          -4.8973e-01, -2.7065e-01, -1.9551e-02, -4.3027e-01, -6.7831e-02,\n",
       "           3.1938e-02,  3.3690e-01, -4.2270e-02,  1.9846e-01,  4.6624e-02,\n",
       "          -8.5206e-01,  2.8355e-01, -4.2386e-02,  5.7611e-01, -3.7909e-01,\n",
       "           3.2371e-01, -4.5111e-01,  7.6445e-01, -1.8075e-02, -3.8842e-01,\n",
       "          -1.7115e-01, -7.5861e-01,  2.0385e-01,  2.1454e-01,  1.8287e-01,\n",
       "           3.2032e-02,  7.7305e-02,  1.2224e-01,  1.2639e-01, -1.0774e-01,\n",
       "          -1.9251e+00,  3.6972e-01,  5.1451e-01,  1.6943e-02, -1.1084e-01,\n",
       "           1.5524e-02, -4.3637e-01,  6.1495e-01, -2.0508e-01,  5.7422e-01,\n",
       "          -2.4362e-01,  1.9351e-01,  1.3762e-01,  8.9583e-02, -5.4214e-02,\n",
       "           2.2548e-02,  1.7486e-01, -1.4758e-01, -5.1964e-01,  7.4880e-02,\n",
       "           3.7396e-02,  3.8384e-01,  1.8244e-01, -6.5358e-02,  6.8362e-02,\n",
       "          -4.8447e-02, -1.0514e-01,  7.0336e-01, -2.4712e-01,  3.4358e-01,\n",
       "          -4.3530e-01, -2.1480e-01, -3.7755e-01, -1.8898e-01,  1.9804e-01,\n",
       "          -1.1886e-01, -5.4434e-02, -1.2123e-01,  4.8337e-01,  4.0345e-01,\n",
       "          -3.3264e-01,  2.9636e-01,  2.4286e-01, -7.1551e-03,  5.3138e-01,\n",
       "           4.3971e-01, -8.3398e-02,  4.4512e-01,  6.3249e-02, -4.4813e-01,\n",
       "          -4.1422e-02, -3.7950e-02, -1.9148e-01, -1.5756e-01, -3.3433e-02,\n",
       "          -6.7850e-02, -1.5475e-01, -6.3829e-02, -5.3739e-01, -2.2750e-01,\n",
       "           4.4022e-02, -2.9204e-01, -3.4494e-01,  2.0691e-01, -4.3661e-01,\n",
       "          -5.1721e-01, -8.2491e-02, -3.4470e-01, -8.3877e-02,  1.8564e-01,\n",
       "           4.2271e-01,  3.7679e-01, -4.3166e-01,  2.0264e-01, -3.6560e-01,\n",
       "           3.5523e-01,  2.3013e-02,  5.7993e-02, -1.4741e-03, -4.3804e-01,\n",
       "           9.1366e-02, -5.1618e-01, -4.5870e-01,  5.2478e-01, -4.2456e-01,\n",
       "           2.0442e-01,  2.0942e-01, -9.2936e-02, -1.2973e-01,  2.6467e-02,\n",
       "          -5.7153e-01, -2.6575e-01, -1.5815e-01, -1.5037e-01,  3.3251e-01,\n",
       "          -1.1913e-01, -3.3692e-02, -9.5248e-02, -2.4415e-01,  2.2896e-01,\n",
       "          -9.7613e-02,  4.1345e-01,  6.8928e-01,  1.9179e-01,  4.7291e-01,\n",
       "           5.7796e-02,  2.9336e-01,  1.4300e-01, -3.5193e-01,  2.1277e-01,\n",
       "          -4.5507e-01,  1.3593e-01, -1.2185e-01, -9.1028e-02, -7.3571e-02,\n",
       "          -2.1684e-01,  5.0198e-01, -3.5231e-01,  2.1035e+00,  5.3660e-01,\n",
       "           3.3324e-01, -1.3433e-01,  4.3450e-01, -7.7040e-02, -1.9579e-01,\n",
       "           9.2608e-02, -5.4634e-03,  8.7001e-01, -4.8165e-01,  2.9553e-01,\n",
       "          -8.8631e-02,  8.1634e-02,  6.9624e-01,  1.8348e-01,  1.1713e-01,\n",
       "          -8.6145e-01, -8.0442e-01, -1.7958e-01, -2.7464e-02, -3.3327e-02,\n",
       "           4.4393e-01, -2.3527e-01, -1.0703e-01,  2.0893e-01,  1.0927e-01,\n",
       "          -1.4098e-02, -6.3907e-03,  3.4296e-01, -4.3973e-02, -2.0110e-01,\n",
       "          -1.9348e-01,  6.1745e-01, -3.3875e-01,  3.1922e-01,  1.3730e-02,\n",
       "          -4.5604e-01, -2.4072e-01,  4.5010e-02,  1.8476e-01, -1.8203e-01,\n",
       "           2.6835e-01,  4.5314e-01, -3.1322e-02,  3.0092e-01, -1.8436e-01,\n",
       "          -5.5350e-01,  8.3359e-01,  2.4834e-01, -2.9764e-01,  1.5377e-01,\n",
       "          -5.5774e-01,  3.9010e-02,  2.1843e-01, -5.8644e-01, -1.0101e-01,\n",
       "          -1.1174e-01, -4.3148e-01,  3.3450e-01,  1.0440e-01,  6.3501e-02,\n",
       "           2.2223e-01, -4.1258e-01,  6.3361e-02,  3.3783e-01, -3.0003e-01,\n",
       "           2.3161e-01,  1.3859e-01, -5.5894e-01, -3.5289e-01,  5.8883e-01,\n",
       "           4.8399e-01, -3.4423e-01,  4.5570e-01,  1.5030e-02,  5.4868e-02,\n",
       "          -3.9204e-01, -8.4393e-02, -2.8202e+00,  1.3967e-01,  2.1003e-01,\n",
       "           6.2074e-02,  1.5052e-01,  1.4926e-01, -1.6652e-01,  4.8628e-02,\n",
       "           3.6849e-01, -1.4784e-01,  1.2234e-01,  5.4733e-01,  4.0144e-01,\n",
       "           1.9052e-02,  1.0067e-01,  1.5482e-02,  1.3047e-01, -5.1847e-01,\n",
       "          -1.7855e-01,  3.3948e-02,  1.4744e-01,  2.6824e-02, -1.5655e-01,\n",
       "          -2.3910e-01,  1.1690e-01,  2.5480e-01, -2.1048e-02, -5.0605e-01,\n",
       "          -1.6571e-01,  4.8467e-01, -2.5468e-01,  5.1348e-01,  5.4647e-02,\n",
       "          -1.0607e-01,  3.0099e-01, -1.7541e-01, -2.5159e-01,  2.1051e-01,\n",
       "           2.1612e-01, -3.8636e-03,  1.3808e-01,  5.7552e-01,  1.3594e-01,\n",
       "           4.9155e-01, -1.6573e-01,  1.9141e-01,  4.7400e-01, -2.2016e-01,\n",
       "           3.1544e-01, -1.5574e-01, -2.5913e-02,  5.6878e-02,  3.6586e-01,\n",
       "          -2.6231e-01,  3.7528e-01,  3.0737e-01,  3.4559e-02,  2.7925e-01,\n",
       "           2.5173e-01, -3.6413e-01,  6.1639e-02,  8.3257e-03, -1.0394e-01,\n",
       "           6.6823e-02,  4.5541e-01, -5.1885e-03, -3.4304e-02, -1.5324e-03,\n",
       "          -8.8500e-02, -2.9668e-01, -1.9077e-01,  5.1059e-02,  4.5666e-01,\n",
       "           4.9668e-01,  1.1878e-01,  1.6195e-01,  7.4004e-01,  1.0221e-01,\n",
       "          -5.1846e-02,  8.0320e-02, -3.0524e-01, -3.2994e-01, -2.0342e-01,\n",
       "           1.7607e-01,  3.7917e-02, -7.8527e+00, -6.4682e-02, -1.9594e-01,\n",
       "          -4.0747e-01, -3.6957e-01, -5.5515e-01,  7.3057e-02, -3.1247e-01,\n",
       "          -2.2758e-02, -1.2245e-01,  3.1400e-02, -2.7889e-02, -3.4714e-01,\n",
       "          -4.3050e-01, -7.0721e-02,  4.4375e-01]]),\n",
       " 'context9_bertoutput': tensor([[ 1.8272e-01,  3.7653e-02,  1.3489e-01, -2.6965e-01, -3.5733e-01,\n",
       "          -3.5457e-01,  4.5952e-01,  5.2956e-01, -2.6890e-01, -2.0015e-01,\n",
       "          -1.2668e-01, -1.1807e-02,  1.5484e-01,  6.4052e-01,  3.5908e-01,\n",
       "          -1.5885e-01, -4.6188e-01,  7.2137e-01,  1.5333e-01,  1.4003e-02,\n",
       "          -5.5490e-02, -6.1355e-01,  1.6124e-01, -3.3629e-01,  2.1111e-01,\n",
       "           1.9152e-01,  8.8572e-02, -1.1088e-01, -3.7333e-02,  7.8973e-02,\n",
       "          -3.6194e-01,  3.8197e-02, -1.9020e-01,  1.3539e-01, -9.0518e-02,\n",
       "          -1.7004e-02, -1.2969e-01,  2.6484e-01,  3.4917e-01,  1.0082e-01,\n",
       "          -1.6330e-01, -2.4914e-01,  3.0727e-01,  3.6888e-01,  6.0104e-02,\n",
       "          -1.6263e-01, -2.9519e+00,  1.5395e-01, -2.2999e-01, -3.9329e-01,\n",
       "           1.6391e-01, -1.4561e-02,  3.1531e-01, -3.9067e-02,  6.0095e-02,\n",
       "           6.6816e-02, -4.1547e-01,  4.0519e-01, -9.8761e-02,  2.5682e-01,\n",
       "           5.1759e-02,  1.9494e-01, -4.1757e-02,  2.0847e-01, -6.6815e-02,\n",
       "           1.2269e-01, -1.3890e-01,  5.1798e-01, -5.4942e-01,  7.0032e-01,\n",
       "           1.5799e-02, -1.0965e-01,  5.0936e-02, -7.6461e-02,  3.6435e-02,\n",
       "          -4.4458e-03, -8.7299e-02,  2.6120e-01,  4.8814e-02,  1.3020e-01,\n",
       "           1.7116e-01,  8.2618e-02,  3.2290e-01,  1.0458e-01,  3.0083e-01,\n",
       "           2.4325e-01, -1.1291e-01, -3.4705e-01,  3.2408e-01,  3.9777e-01,\n",
       "          -4.1708e-02, -1.6033e-01,  1.6170e-01,  1.1296e-01,  7.8243e-01,\n",
       "           2.0765e-01,  9.4629e-02, -1.4990e-01,  4.8517e-02,  2.0894e-01,\n",
       "          -1.3344e-02,  2.4280e-02, -2.4219e-01, -6.0233e-01,  8.9315e-02,\n",
       "           2.3881e-01, -1.3683e-01, -9.9580e-02,  1.1607e-02, -2.7230e+00,\n",
       "           3.6963e-01,  2.7065e-01, -4.0733e-01, -3.4740e-01,  1.8416e-01,\n",
       "           3.3844e-01,  5.5488e-01,  2.3714e-01,  3.8267e-01,  3.4855e-02,\n",
       "          -8.7276e-02,  5.9307e-01,  8.9080e-02, -8.6914e-02,  1.9367e-01,\n",
       "           7.1467e-01,  3.4616e-01,  8.7674e-02,  4.2865e-01,  1.7859e-01,\n",
       "           6.4336e-02,  4.9786e-01, -1.0724e-01, -4.0686e-01, -5.2711e-02,\n",
       "          -7.5876e-02,  1.3726e-01, -1.9050e-01, -1.9292e-01, -3.1517e-01,\n",
       "          -3.8235e-01, -3.5705e-01, -3.3337e+00,  4.5655e-01,  4.4976e-01,\n",
       "           2.4350e-01, -4.3194e-01, -2.9683e-01, -1.2808e-01,  1.1313e-02,\n",
       "          -4.6992e-01,  5.2200e-02, -3.5170e-01,  6.5524e-02, -2.8679e-01,\n",
       "           1.9173e-01, -1.7421e-02, -6.2791e-01,  3.2726e-02,  6.7267e-01,\n",
       "           2.9996e-01,  2.9818e-03, -1.6312e-03, -7.1469e-02, -3.5344e-01,\n",
       "          -1.1871e-01,  1.0016e-01,  3.0717e-01, -4.1791e-02, -3.2596e-01,\n",
       "          -1.1675e-01, -6.7549e-02,  3.4238e-01,  1.0842e-01,  2.6275e-01,\n",
       "          -6.8333e-02,  3.0401e-01, -7.9261e-02,  1.9579e-01, -1.1339e-02,\n",
       "          -2.6875e-02,  1.1095e-01,  2.7574e-01,  8.9133e-02,  2.7861e-01,\n",
       "          -1.5699e-03,  4.4574e-01, -5.7850e-02, -6.0476e-02,  1.8636e-01,\n",
       "          -2.0760e-01,  7.2609e-02,  5.1027e-01,  2.1968e-01,  4.9735e-01,\n",
       "          -1.2238e-01, -6.6803e-02, -7.3546e-03,  4.6401e-01,  2.8695e-01,\n",
       "           1.6734e-01,  3.7997e-02, -6.7960e-02, -1.3900e-02, -5.1764e-01,\n",
       "           3.7414e+00,  7.4264e-02, -1.6862e-01,  5.7557e-02,  6.0519e-02,\n",
       "          -2.8729e-01,  2.7542e-01,  1.6266e-01, -7.4143e-03, -1.2499e-01,\n",
       "           8.0600e-02,  5.9501e-02,  1.3345e-01, -1.6507e-01,  8.3513e-02,\n",
       "           2.6576e-02,  1.5511e-01, -3.4391e-01, -2.3941e-01, -8.4698e-03,\n",
       "          -1.6225e-01,  1.7649e-01,  6.4335e-02,  2.3825e-01, -1.3694e+00,\n",
       "          -5.8681e-02,  2.0192e-01, -2.1581e-01,  3.3787e-01, -4.0298e-02,\n",
       "           6.6804e-02, -3.1136e-03, -8.0934e-02,  7.5507e-02, -1.0864e-02,\n",
       "           2.2542e-01,  2.1702e-01, -2.2270e-01,  1.3949e-01, -3.7524e-01,\n",
       "           5.0186e-01,  3.4032e-01, -4.5108e-01,  2.1358e-01, -7.0760e-02,\n",
       "           6.2339e-01, -1.5618e-01, -1.8611e-01, -3.5183e-01,  3.1099e-01,\n",
       "          -2.3356e-01,  1.8853e-01, -4.3471e-02, -9.5264e-01, -1.3945e-01,\n",
       "          -1.8174e-01, -1.5909e-01, -1.7450e-01,  2.4469e-01, -2.8537e-01,\n",
       "          -4.4194e-01,  2.4609e-01, -3.7114e-01,  3.3974e-01, -1.5786e-01,\n",
       "          -1.5561e-01, -3.2571e-01, -1.6873e-01, -3.7786e+00,  2.2292e-01,\n",
       "           6.9894e-03, -7.2082e-03,  5.3416e-01,  1.7344e-01, -2.9073e-01,\n",
       "          -2.4712e-01,  7.7419e-02, -1.2684e-01,  4.5313e-01,  3.8623e-01,\n",
       "          -3.9034e-01,  2.8355e-01, -1.7333e-01, -1.3079e-01,  2.0189e-01,\n",
       "          -1.9000e-01, -3.5636e-01, -1.3035e-01,  2.7792e-01,  3.0409e-01,\n",
       "          -4.7003e-01,  2.1124e-01,  1.1281e-01, -2.9967e-01,  1.4831e-01,\n",
       "          -3.4019e-02,  3.3557e-01, -2.1114e-01, -3.0035e-01, -8.9933e-02,\n",
       "           8.6479e-02, -2.2811e-02, -4.8861e-01, -3.2216e+00, -2.4982e-02,\n",
       "          -6.3939e-02, -5.1447e-01, -1.6890e-01,  7.4221e-02,  2.9663e-01,\n",
       "           1.6613e-01, -1.2431e-01,  2.8756e-02, -9.9541e-02, -1.3508e-01,\n",
       "           7.9337e-02,  8.7765e-02, -1.2674e-01,  3.3537e-01,  3.1987e-01,\n",
       "           1.7819e-01,  1.8216e-01, -1.4600e-01, -3.9105e-01,  2.4325e-01,\n",
       "           5.2269e-02,  2.0050e-01,  9.5163e-02,  1.1368e-01, -2.2827e-01,\n",
       "          -3.2528e-01,  1.8122e-01,  1.5704e-01,  2.6394e-01,  2.7343e-02,\n",
       "          -1.5739e-01, -1.3867e-01, -5.5207e-01, -2.0495e-01,  4.0356e-01,\n",
       "           9.8436e-02,  1.9781e-01,  1.9220e-03,  3.0344e-01,  5.7605e-01,\n",
       "           1.0536e-01,  2.2337e-01,  6.0458e-01, -2.7238e-01, -1.5849e-01,\n",
       "           6.4855e-02,  2.5539e-01, -2.1114e-01, -3.2648e-01, -3.7474e-01,\n",
       "           1.1793e+00, -1.4002e-01,  2.2007e-01, -2.3923e-01,  1.6199e-01,\n",
       "           3.7894e-01,  2.8934e-01,  2.7448e-01,  7.3810e-01,  1.1515e-01,\n",
       "           5.0009e-01, -2.4491e-01, -2.4136e-01, -4.6779e-01, -1.6250e-01,\n",
       "          -3.2947e-01, -1.8315e-01, -1.6006e-01, -1.2900e-01,  3.2779e-01,\n",
       "          -8.2391e-02, -1.0219e+00,  1.0298e-01,  2.9924e-01, -1.1568e-01,\n",
       "           2.8603e-01, -6.7669e-02,  1.1315e-01, -1.5578e-01, -2.1257e-02,\n",
       "          -4.7449e-01,  3.6927e-01, -6.3131e-01, -8.7204e-02,  1.7383e-03,\n",
       "           1.6450e-01, -5.6183e-01,  2.7700e-01, -1.3555e-01,  1.3583e-01,\n",
       "           7.5383e-02,  1.5388e-01,  2.1631e-02,  3.1015e-01,  6.1121e-01,\n",
       "          -7.8002e-01, -6.5965e-03,  8.6751e-03,  4.1615e-01, -2.1602e-01,\n",
       "           7.3176e-02, -2.9208e-02, -1.3517e-01,  7.0971e-03,  1.6383e-01,\n",
       "           5.8467e-02,  1.8445e-01,  2.2732e-01, -4.4870e-01,  2.7284e-01,\n",
       "          -6.0226e-01,  2.4972e-01,  5.8545e-01, -1.6380e-01, -1.0490e-01,\n",
       "           3.1198e-02,  4.0175e-01, -8.4861e-02, -1.0646e-02,  1.0256e-01,\n",
       "           1.5599e-01,  2.2362e-01, -1.6507e-01, -8.4575e-02,  2.4700e-01,\n",
       "          -1.2955e-01,  2.8271e-02, -4.2586e-01,  1.2471e-01, -1.4896e-01,\n",
       "          -2.9588e-01, -3.5443e-01,  1.1909e-01, -3.5945e-01,  1.7157e-01,\n",
       "           2.1128e-01,  4.4340e-01, -1.6489e-01,  2.0384e-01,  1.1149e-01,\n",
       "          -5.3033e-01,  3.7389e-01, -6.5592e-02,  4.9242e-01, -2.8353e-01,\n",
       "           4.1948e-03, -4.1123e-01,  6.2292e-01, -2.6607e-01, -3.7064e-01,\n",
       "          -2.2859e-02, -3.3245e-01,  3.5469e-01,  2.3666e-01,  4.6952e-02,\n",
       "           1.9551e-01, -1.3695e-01,  3.1102e-02,  4.4322e-02, -4.0545e-02,\n",
       "          -1.2675e+00,  3.6437e-01,  2.6670e-01, -1.6689e-01,  1.5107e-01,\n",
       "           1.7597e-01, -2.6724e-01,  3.1097e-01, -1.1824e-01,  2.7149e-01,\n",
       "          -2.1601e-01,  2.0671e-01,  8.2679e-02,  1.7936e-01,  1.6381e-02,\n",
       "           1.8340e-01,  1.7142e-01, -1.4476e-01, -4.1068e-01,  1.7398e-01,\n",
       "           2.0152e-01,  2.1295e-01,  1.4423e-01, -2.5194e-01,  1.6118e-01,\n",
       "          -2.2028e-03, -1.1022e-01,  4.5820e-01, -2.3185e-01,  3.5054e-01,\n",
       "          -1.5087e-01, -2.5811e-01, -2.8319e-01, -2.0880e-01,  9.4740e-03,\n",
       "           1.9268e-01,  1.4146e-01, -1.6657e-02,  4.5325e-01,  5.8823e-01,\n",
       "          -4.8882e-01,  3.4484e-01,  3.2807e-01,  2.4279e-01,  6.0510e-01,\n",
       "           1.5452e-01, -1.3420e-01,  2.1509e-01,  3.3842e-01, -4.0501e-01,\n",
       "          -8.7907e-02, -1.5368e-01, -2.1627e-01, -2.6665e-01,  4.3663e-02,\n",
       "          -2.5602e-02,  2.2670e-01,  1.9619e-02, -5.5871e-01,  1.1866e-01,\n",
       "           2.3395e-01,  1.2157e-01, -2.2187e-01,  2.7887e-01, -3.5418e-01,\n",
       "          -3.4356e-01,  2.1340e-01, -1.7516e-03,  1.0457e-01,  2.6813e-01,\n",
       "           2.5815e-01,  1.9001e-01, -3.3961e-01, -1.1073e-01, -3.8349e-01,\n",
       "           3.6393e-01,  2.5092e-02, -1.1760e-01, -7.1359e-03, -3.2471e-01,\n",
       "           9.6865e-02, -6.0026e-01, -5.7869e-01,  3.4590e-02, -4.9404e-01,\n",
       "           1.1825e-01, -4.1452e-02, -3.8671e-01,  1.0978e-01, -8.9292e-02,\n",
       "          -4.3752e-01, -3.6846e-01, -2.5284e-02, -3.9823e-02,  1.1366e-01,\n",
       "          -2.6603e-02, -1.2942e-01, -1.9001e-01, -7.9078e-02,  2.3907e-01,\n",
       "          -1.3004e-02,  5.6796e-01,  4.4208e-01,  3.1618e-01,  3.8074e-01,\n",
       "          -2.1049e-01,  3.4827e-01,  1.9870e-01, -2.8842e-01,  1.5254e-01,\n",
       "          -1.1985e-02,  2.4546e-01, -2.4966e-01, -1.6583e-01, -1.8941e-01,\n",
       "          -2.8729e-01,  1.6084e-01, -6.0683e-02,  2.3069e+00,  3.6332e-01,\n",
       "           2.4019e-01, -9.9201e-02,  6.6969e-01,  4.5205e-02,  6.4014e-02,\n",
       "          -7.9186e-02, -1.0121e-01,  7.3828e-01, -4.8338e-01,  4.9655e-01,\n",
       "          -1.0578e-01,  8.5999e-02,  5.9752e-01,  2.8380e-01,  1.2101e-01,\n",
       "          -5.3357e-01, -5.0865e-01,  8.3900e-02, -1.9191e-01,  7.3702e-02,\n",
       "           3.9591e-01, -1.5486e-01, -1.3222e-01,  2.2837e-01, -1.0984e-01,\n",
       "          -1.4616e-01, -6.3665e-03,  1.2139e-01, -2.5851e-01, -4.5124e-02,\n",
       "          -4.9588e-02,  4.6800e-01, -4.3872e-01,  2.2653e-01,  1.2225e-01,\n",
       "          -1.7893e-01, -2.4571e-02, -1.1322e-01, -1.9929e-01, -4.7675e-01,\n",
       "           3.1303e-01,  3.1520e-01, -1.0566e-01,  2.8984e-01,  8.4080e-02,\n",
       "          -3.5095e-01,  7.7930e-01,  1.9296e-02, -1.4543e-02, -1.7510e-01,\n",
       "          -2.6563e-01,  3.6364e-02,  2.0532e-01, -6.5798e-01, -5.6122e-02,\n",
       "           1.9521e-01, -5.6774e-01,  2.7332e-01,  7.7988e-02,  3.8616e-02,\n",
       "           2.8860e-01, -3.8067e-01,  4.8770e-02,  1.5440e-01, -3.9236e-02,\n",
       "           1.1072e-01,  5.5314e-02, -1.4373e-01, -2.2641e-01,  4.5677e-01,\n",
       "           1.9997e-01,  1.7525e-01,  3.1980e-01,  7.9585e-02, -6.4871e-03,\n",
       "          -3.0335e-01, -8.4807e-02, -3.0967e+00,  2.8133e-01, -9.7743e-02,\n",
       "          -2.0572e-01,  2.4046e-02, -2.0264e-02, -7.0296e-02,  6.4823e-02,\n",
       "           4.4389e-01, -1.8097e-01,  1.3247e-01,  2.7153e-01,  2.1729e-01,\n",
       "           1.8037e-01,  1.5509e-02,  8.5492e-02,  3.0109e-01, -1.7206e-01,\n",
       "          -1.0574e-01, -3.7326e-01, -2.4262e-01,  2.0177e-01, -1.6688e-01,\n",
       "          -1.8233e-01,  3.2405e-01,  1.9648e-01, -1.3748e-03, -2.3884e-01,\n",
       "           1.1304e-01,  2.9896e-01, -1.8018e-01,  3.1293e-01, -7.4231e-02,\n",
       "          -1.6397e-01,  1.3027e-01, -1.3298e-01, -2.2320e-01, -1.0787e-01,\n",
       "           5.9562e-01,  9.5868e-02,  3.3457e-01,  2.5230e-01, -2.6597e-02,\n",
       "           3.5772e-01, -7.6735e-02,  9.4247e-03,  3.9783e-01, -8.0367e-02,\n",
       "           4.8821e-01, -1.0987e-01,  3.5565e-01, -8.0767e-02,  1.1907e-01,\n",
       "          -2.0281e-01,  1.7809e-01,  1.2748e-02, -1.4723e-01,  3.7327e-01,\n",
       "           5.7181e-02, -4.1482e-01, -6.8019e-02,  3.6797e-02, -4.5104e-01,\n",
       "          -5.6338e-04,  3.2517e-01, -2.7335e-01,  2.8527e-01, -1.2771e-01,\n",
       "          -3.5444e-01, -3.8924e-01, -1.8143e-01, -3.0395e-01,  2.5749e-01,\n",
       "           3.6258e-01, -4.4045e-02, -2.6569e-01,  4.2224e-01,  2.4740e-01,\n",
       "           1.8381e-01,  2.4064e-01,  2.2842e-02, -3.2629e-01, -1.8139e-01,\n",
       "           1.2034e-01,  2.6191e-01, -7.6484e+00,  1.6388e-02,  2.4933e-02,\n",
       "          -1.2391e-01, -2.8656e-01, -5.5056e-01, -2.0173e-02,  1.9307e-02,\n",
       "           9.4937e-02, -1.8371e-02,  9.9381e-02, -5.5370e-02, -2.0952e-01,\n",
       "          -4.4857e-01,  1.6160e-02,  6.1820e-01]]),\n",
       " 'context10_bertoutput': tensor([[-2.3092e-02, -1.3597e-01,  3.4046e-03, -3.5777e-01, -4.1825e-01,\n",
       "          -4.2363e-01,  3.6923e-01,  9.4887e-01,  7.1783e-02, -5.9026e-01,\n",
       "          -3.4120e-01, -3.6678e-03,  2.5397e-01,  6.5556e-01,  8.8484e-01,\n",
       "           4.7203e-02, -3.9016e-02,  7.2851e-01,  1.2081e-01, -4.1482e-01,\n",
       "          -5.8246e-02, -5.5605e-01,  5.1224e-01, -6.9882e-02,  3.4676e-02,\n",
       "          -1.3210e-01, -2.3827e-01, -4.8810e-01,  1.5438e-01, -1.1056e-01,\n",
       "          -3.3037e-01,  2.1431e-01, -2.2458e-01, -3.2683e-01,  5.3890e-01,\n",
       "          -2.9840e-01, -1.3067e-01, -1.0413e-01,  2.7004e-01,  3.6662e-01,\n",
       "          -1.4042e-01, -3.9942e-01,  4.0772e-01,  8.7034e-02, -6.7755e-02,\n",
       "          -2.1040e-01, -3.2857e+00,  4.6577e-01, -4.5662e-01, -3.1499e-01,\n",
       "           5.2401e-01, -1.8480e-01,  1.5333e-03,  2.1946e-01,  3.2971e-01,\n",
       "           4.3109e-01, -7.0901e-01,  2.0209e-01,  1.8206e-02,  2.7270e-01,\n",
       "           1.3219e-01,  6.2045e-02, -2.7027e-01,  3.8482e-02, -6.5651e-02,\n",
       "           2.0319e-01,  2.5020e-01,  7.8394e-01, -3.5086e-01,  6.4066e-01,\n",
       "          -2.2498e-01,  4.0821e-02,  4.0565e-01, -8.4892e-02,  1.6186e-01,\n",
       "          -9.3295e-02, -6.5719e-03,  4.6483e-01, -9.7002e-02,  1.5559e-01,\n",
       "           3.5537e-01,  6.5398e-01,  3.2500e-01,  9.1707e-02,  1.9554e-01,\n",
       "           4.7409e-01, -2.6971e-01, -5.5367e-01,  3.5753e-01,  3.5618e-01,\n",
       "          -4.5330e-01, -1.4165e-01, -9.8268e-02,  3.0432e-01,  9.1990e-01,\n",
       "          -2.1776e-01,  1.1703e-01, -1.5261e-01, -2.0777e-01,  4.8332e-01,\n",
       "           1.8580e-01,  3.8120e-01, -9.4791e-02, -1.0328e+00, -2.1293e-01,\n",
       "           3.4517e-01, -1.1542e-01, -3.7004e-01,  3.5420e-01, -1.8452e+00,\n",
       "           2.0164e-01,  4.6778e-01, -6.8777e-01, -4.5555e-01,  1.0322e-01,\n",
       "           4.1433e-01,  8.8570e-01, -3.2346e-01,  1.7172e-01,  8.9490e-03,\n",
       "           4.2010e-02,  2.1440e-01,  3.4830e-01, -1.0827e-01, -4.2468e-02,\n",
       "           4.5881e-01,  4.9511e-01, -2.7150e-01,  5.3682e-01,  2.0999e-01,\n",
       "           9.6417e-02,  5.7210e-01, -1.2037e-01, -5.8584e-01,  1.1323e-01,\n",
       "          -1.7922e-01, -1.0166e-01, -2.0315e-01, -3.1847e-01, -7.0446e-02,\n",
       "          -6.9243e-01, -4.7593e-01, -2.8969e+00,  7.1114e-02,  7.1291e-01,\n",
       "           5.9082e-01, -2.5535e-01,  4.3218e-01,  4.2396e-02, -1.5244e-01,\n",
       "           2.5333e-02, -1.5337e-01, -1.8714e-01, -1.4969e-01, -4.9395e-01,\n",
       "           1.8222e-01,  4.1239e-02, -2.1108e-02,  1.7527e-01,  6.3835e-01,\n",
       "           5.1979e-01, -3.9007e-02,  8.7035e-02,  1.3242e-01, -5.2589e-01,\n",
       "           3.2677e-01,  4.5532e-01,  4.2611e-01, -2.6945e-01, -4.6191e-01,\n",
       "          -9.5641e-02,  5.1259e-02,  2.4059e-01,  2.5903e-01,  2.3873e-01,\n",
       "           5.7546e-02,  4.1473e-01,  3.7754e-01,  2.0035e-01, -2.5461e-01,\n",
       "          -2.9988e-01,  1.9344e-01,  9.1317e-02, -8.7854e-02,  4.5922e-01,\n",
       "          -1.3347e-01,  7.5172e-01, -5.1520e-02, -8.1912e-02,  7.2732e-01,\n",
       "          -4.8749e-01, -2.8577e-01,  5.8526e-01,  2.5918e-01,  5.9692e-01,\n",
       "          -2.9232e-01,  3.5583e-01, -5.5131e-01,  2.0701e-01,  2.5316e-01,\n",
       "          -1.0927e-02, -1.9162e-01,  2.3204e-01,  4.8916e-02, -6.8505e-01,\n",
       "           3.6313e+00,  2.3176e-01, -3.8260e-01, -7.6188e-02, -6.6218e-02,\n",
       "          -3.9467e-01,  1.7934e-01, -2.2256e-01,  3.1449e-01, -5.1674e-01,\n",
       "          -1.0325e-01,  5.1961e-01,  2.6111e-02, -2.2693e-01, -1.2869e-01,\n",
       "           2.1229e-01,  5.8251e-01, -9.4206e-02,  1.9805e-01, -4.6382e-01,\n",
       "          -5.0539e-02, -3.1222e-01,  2.2603e-01,  1.6440e-01, -1.3598e+00,\n",
       "          -1.4698e-01, -7.8162e-02, -5.9753e-01,  4.8517e-01,  3.3683e-02,\n",
       "           2.8215e-01, -5.6801e-02, -5.4535e-01,  1.8983e-03,  7.3752e-02,\n",
       "           2.3298e-01,  3.1029e-01, -4.9206e-01,  9.6543e-02, -4.3510e-01,\n",
       "           4.8995e-01,  1.2384e-01, -1.6153e-01,  5.7471e-01, -1.5203e-01,\n",
       "           5.4322e-01,  6.0809e-02,  1.1898e-02, -1.4205e-01,  2.4295e-01,\n",
       "           1.1165e-01,  4.0052e-02, -1.9930e-01, -9.1817e-01, -1.9386e-01,\n",
       "          -4.2046e-01, -3.4329e-01,  2.7094e-01,  2.0686e-01, -7.3025e-01,\n",
       "          -5.5086e-01,  8.5993e-02, -4.8852e-01,  4.5493e-01, -2.9926e-01,\n",
       "           1.8168e-01, -2.3892e-01, -4.8964e-01, -2.4162e+00,  2.3067e-01,\n",
       "          -9.3859e-03,  3.1240e-01, -1.5600e-02,  1.8655e-01, -1.8702e-01,\n",
       "           2.3068e-01,  3.0822e-01, -4.2018e-01,  4.7439e-01,  1.0847e-01,\n",
       "          -2.9038e-01,  2.3243e-01, -2.4876e-01,  4.2165e-01,  1.0121e-01,\n",
       "          -3.5902e-01, -3.8328e-01, -3.2306e-01, -2.0826e-02,  6.0833e-01,\n",
       "          -5.9521e-01,  4.7111e-01,  2.0489e-01, -4.5655e-01,  9.7614e-02,\n",
       "          -1.5129e-01, -8.3620e-02, -2.6388e-01,  1.4210e-01, -3.7685e-01,\n",
       "           1.3311e-01, -2.9594e-01, -4.8280e-01, -3.6128e+00,  5.5924e-02,\n",
       "          -4.2241e-01, -3.9993e-01,  6.7103e-02,  8.7088e-02,  5.5115e-01,\n",
       "          -1.1479e-01, -2.2284e-02, -4.3448e-02, -3.4523e-02,  4.3229e-01,\n",
       "           2.0566e-01,  1.0308e-01,  5.7042e-02,  4.4517e-01,  9.8090e-02,\n",
       "          -1.9727e-01,  4.4032e-01,  1.9282e-01, -6.0813e-01,  1.0966e-01,\n",
       "           2.9774e-01, -3.8111e-02,  5.7502e-01,  1.9010e-01, -3.9894e-01,\n",
       "          -4.6418e-02,  8.2774e-02,  2.7633e-01,  4.8851e-01, -1.7087e-01,\n",
       "          -1.6380e-01, -5.5848e-01, -3.8843e-01, -4.8198e-01,  2.2840e-01,\n",
       "          -2.7158e-02,  1.6216e-01, -3.8744e-01,  3.4923e-01,  2.3675e-01,\n",
       "          -1.9011e-01,  1.4704e-01,  8.1997e-01, -1.5958e-01,  6.7036e-03,\n",
       "           2.2971e-01,  1.6921e-01,  6.6751e-01, -2.5977e-01, -1.4076e-01,\n",
       "           1.5676e+00, -8.7310e-02,  1.1724e-01, -5.2227e-01,  2.9982e-01,\n",
       "           3.0858e-01,  1.0303e-01,  5.2458e-01,  8.1105e-01, -2.3742e-01,\n",
       "           4.9977e-01, -3.7564e-01,  8.1719e-03, -4.0857e-01,  1.8307e-01,\n",
       "          -7.7169e-01, -1.5133e-01,  1.2262e-02, -4.5210e-02,  2.5765e-01,\n",
       "          -1.0897e-01, -9.7811e-01, -6.7458e-02, -4.8647e-03, -2.8388e-01,\n",
       "           3.5174e-01, -3.2540e-01,  4.2207e-01, -1.6255e-01, -1.9933e-02,\n",
       "          -3.4716e-01,  3.6803e-01, -2.5517e-01,  5.1422e-02, -6.9367e-02,\n",
       "           3.3652e-01, -1.2440e+00,  1.4637e-01, -4.4747e-01,  1.4532e-01,\n",
       "           2.0925e-01,  3.3948e-01, -4.6878e-01,  1.9983e-01,  7.0495e-01,\n",
       "          -4.8788e-01,  2.1501e-01, -7.9594e-02,  5.9661e-01,  3.7494e-02,\n",
       "          -1.5095e-01, -2.6935e-02, -4.8322e-01, -2.1931e-01, -2.1006e-01,\n",
       "           1.7550e-01, -7.0641e-02,  1.1788e-01, -3.3827e-01, -1.3322e-01,\n",
       "          -4.2410e-01,  8.1228e-02,  7.3239e-01, -2.9365e-01, -1.4340e-01,\n",
       "           5.8504e-01,  4.0657e-01, -1.4739e-01,  6.2902e-02, -1.4327e-01,\n",
       "          -2.3971e-02,  1.8942e-01, -1.1900e-01,  2.4217e-01,  1.9223e-01,\n",
       "          -2.0039e-01, -4.1785e-01, -6.6010e-01,  7.7071e-02, -3.4648e-01,\n",
       "          -4.2477e-01, -9.7810e-02, -1.2016e-01, -6.5440e-01, -1.1780e-01,\n",
       "          -8.6869e-02,  3.7413e-01, -2.4605e-02,  5.4708e-01,  6.2071e-02,\n",
       "          -4.1331e-01,  6.4051e-01, -3.2695e-01,  9.6285e-01, -1.1879e-02,\n",
       "          -6.9657e-02, -6.7437e-01,  5.2776e-01, -3.9729e-04, -5.8458e-02,\n",
       "          -1.3662e-02, -5.7895e-01,  2.2628e-01,  2.7373e-01, -8.0370e-02,\n",
       "          -7.7653e-02, -2.8908e-03, -3.0697e-01,  4.8624e-01, -6.9466e-03,\n",
       "          -1.5969e+00,  5.2845e-01,  3.7753e-01,  1.7402e-01,  2.3207e-01,\n",
       "          -2.0641e-01, -4.3809e-01,  2.5302e-01, -1.5384e-01,  3.4299e-01,\n",
       "          -2.5823e-01,  6.7616e-03,  2.3453e-01,  5.8713e-02,  2.7717e-01,\n",
       "           6.7991e-02,  3.7696e-01, -1.4629e-01, -3.9118e-01, -1.6458e-01,\n",
       "           1.4170e-01,  5.3211e-01,  4.3465e-01,  5.2661e-01,  5.8079e-02,\n",
       "          -1.3239e-01, -6.2540e-02,  7.4077e-01, -4.0663e-01,  3.2298e-01,\n",
       "          -8.0653e-02, -9.1004e-01, -6.7738e-01, -4.1807e-01,  4.1736e-01,\n",
       "          -1.4588e-01,  3.2898e-01,  6.0831e-02,  3.5018e-01,  7.5554e-01,\n",
       "          -6.0278e-01,  4.1554e-01,  4.1286e-01,  3.2277e-01,  4.7350e-01,\n",
       "           5.1654e-01, -4.7224e-01,  4.2749e-01,  2.6021e-01, -3.2028e-01,\n",
       "           1.9500e-01, -4.4312e-02, -3.9978e-01, -1.4575e-01, -2.4825e-01,\n",
       "          -1.6696e-01,  1.2737e-01, -1.4766e-01, -4.8413e-01, -1.1071e-01,\n",
       "           3.1510e-01, -2.8309e-01, -6.5131e-01, -2.2838e-02, -1.6416e-01,\n",
       "          -7.1256e-01,  1.6825e-01, -2.8094e-01, -4.0958e-01,  5.8170e-01,\n",
       "           2.8978e-01,  4.2528e-01, -6.3513e-01,  2.7081e-01, -3.0022e-01,\n",
       "          -1.0732e-01, -3.1559e-02,  2.8867e-01, -1.0120e-02, -6.0162e-01,\n",
       "           3.6373e-01, -8.7727e-01, -5.5999e-01,  4.8851e-01, -1.7101e-01,\n",
       "           1.0146e-01, -1.5204e-01, -1.2849e-01,  1.4524e-02,  1.6296e-01,\n",
       "          -4.9785e-01, -4.0567e-01,  4.3953e-01,  4.3782e-02,  2.4931e-01,\n",
       "          -2.0996e-01, -1.9896e-01, -1.9883e-02, -7.8131e-02, -4.7756e-02,\n",
       "          -2.1234e-01,  6.0242e-01,  7.7988e-01,  4.1276e-01,  5.9897e-01,\n",
       "           3.5863e-01,  2.0687e-01,  1.9985e-01, -4.4919e-01,  2.2131e-01,\n",
       "          -1.6257e-01, -3.1842e-01, -2.7618e-01, -1.3105e-03,  1.8582e-01,\n",
       "          -3.4655e-01,  3.6261e-01, -3.6692e-01,  1.6714e+00,  2.7036e-01,\n",
       "           1.4324e-01,  1.0589e-02,  7.1253e-01, -8.9762e-02, -3.8428e-01,\n",
       "           2.4679e-02, -2.3297e-01,  6.8304e-01, -6.1263e-01,  5.4880e-01,\n",
       "          -3.2059e-01,  4.6843e-01,  7.1737e-01,  3.0752e-01, -1.4568e-02,\n",
       "          -6.3428e-01, -6.8686e-01, -1.3989e-01, -4.2695e-01,  5.1152e-02,\n",
       "           7.3611e-01,  1.2804e-01, -1.2690e-01,  3.1924e-01, -1.6734e-01,\n",
       "          -1.9846e-01, -8.3947e-02,  2.9997e-01, -8.6039e-02,  2.5181e-01,\n",
       "          -2.6307e-01,  6.4091e-01, -4.7453e-01, -4.4936e-03,  1.9776e-02,\n",
       "          -5.1988e-01, -3.3669e-01,  6.8971e-02, -2.3334e-01, -4.4693e-01,\n",
       "           6.8879e-01,  2.4232e-02, -5.9754e-01,  4.4246e-01, -9.1409e-02,\n",
       "          -3.2728e-01,  5.5150e-01,  1.2177e-01, -2.6739e-01,  8.4866e-02,\n",
       "          -5.5673e-01,  3.5892e-01,  1.6669e-01, -4.5612e-01, -2.4528e-01,\n",
       "          -1.4720e-01, -6.0705e-01,  5.6646e-01, -3.2969e-01,  2.0412e-01,\n",
       "           4.0789e-02, -8.1877e-02,  1.3285e-01,  2.5172e-01, -5.7939e-01,\n",
       "           4.7060e-01, -2.1615e-02, -6.6325e-01, -3.4193e-01,  6.2570e-01,\n",
       "           5.1471e-01, -1.3226e-01,  2.5684e-01,  1.5184e-01,  3.1220e-01,\n",
       "          -2.3102e-01, -1.9420e-01, -2.3857e+00,  9.1149e-02,  3.8924e-01,\n",
       "           4.8132e-02, -1.3843e-01,  1.3386e-02, -3.4561e-02,  1.8898e-01,\n",
       "           9.9518e-02, -6.6179e-02, -2.9034e-01,  2.6153e-01,  4.1935e-01,\n",
       "          -1.2426e-01,  1.2950e-01,  6.0088e-02,  5.9308e-01,  8.7302e-02,\n",
       "          -4.0877e-01,  5.7436e-02,  2.0392e-01, -2.1129e-01, -2.0217e-01,\n",
       "          -1.4215e-01, -5.9740e-01,  2.6377e-01, -8.4934e-03, -3.9743e-01,\n",
       "           2.2163e-01,  1.8391e-01, -1.4308e-01,  8.6771e-01, -2.8074e-01,\n",
       "          -9.7784e-02,  1.3776e-01,  1.1039e-01, -6.4248e-01, -1.4389e-01,\n",
       "           1.6247e-01,  2.9946e-01,  1.4375e-01,  3.7116e-01, -1.9235e-01,\n",
       "           2.7040e-01, -1.1774e-01,  1.1000e-01,  3.8678e-01, -1.2386e-01,\n",
       "           5.2655e-01, -3.8671e-01,  4.2585e-01,  1.3290e-01,  7.2259e-02,\n",
       "          -2.7048e-01,  2.2513e-01,  5.5156e-02,  1.3205e-02,  5.4756e-01,\n",
       "           3.8503e-01, -3.0447e-01, -4.5180e-01,  2.5413e-02, -9.1471e-02,\n",
       "           6.8503e-02,  4.3370e-01, -1.7745e-01, -1.4525e-01, -7.7973e-02,\n",
       "          -4.8680e-02, -4.2925e-01, -2.6237e-01, -2.0085e-01,  4.3305e-01,\n",
       "           6.3493e-01,  5.2369e-02, -1.6200e-01,  4.8602e-01,  3.9133e-01,\n",
       "           4.3676e-01,  2.2218e-01,  4.5172e-02, -2.3554e-01,  6.2265e-02,\n",
       "           3.7275e-01,  3.8331e-01, -6.4625e+00, -1.5696e-01, -3.7364e-01,\n",
       "          -5.4419e-01, -5.3518e-01, -6.1879e-01,  1.6170e-01, -3.5769e-01,\n",
       "           2.6073e-01, -4.0778e-01,  2.5604e-01, -6.1130e-02, -1.0974e-01,\n",
       "          -2.1369e-01,  1.6315e-01,  4.1583e-01]]),\n",
       " 'context11_bertoutput': tensor([[ 1.9014e-01, -4.7539e-02,  1.3785e-02, -2.7425e-01, -1.7895e-01,\n",
       "          -2.8410e-01,  3.3601e-01,  7.5347e-01, -9.6679e-02, -4.0055e-01,\n",
       "          -1.4105e-01, -1.7743e-01,  1.2729e-01,  3.9654e-01,  4.8401e-01,\n",
       "           3.6874e-01, -1.6052e-01,  6.7144e-01,  1.0633e-01, -6.9981e-02,\n",
       "          -2.1453e-01, -9.3064e-01,  1.8913e-01, -1.5424e-01,  2.2163e-01,\n",
       "          -9.4855e-02,  1.3014e-01, -2.1604e-01, -2.9331e-03, -1.0368e-01,\n",
       "          -3.6390e-01,  3.9210e-01,  1.0229e-01, -2.6148e-01,  3.6370e-01,\n",
       "          -3.6992e-01, -1.6416e-01, -2.7414e-03,  4.9948e-01,  3.7140e-01,\n",
       "           7.6011e-02, -2.4465e-01,  2.2101e-01,  4.3303e-02, -1.3151e-01,\n",
       "           3.0063e-02, -3.1796e+00,  3.4645e-01, -3.0981e-01, -3.4657e-01,\n",
       "           4.0352e-01, -6.9051e-02,  1.1505e-01, -1.5316e-01, -1.4761e-02,\n",
       "           4.1850e-01, -4.9794e-01,  2.7041e-01,  1.6476e-01,  8.6699e-02,\n",
       "           3.0124e-01,  2.0706e-01, -1.5191e-01,  1.6853e-01, -4.4921e-01,\n",
       "           2.8681e-02,  2.3846e-03,  6.2357e-01, -2.2931e-01,  7.1443e-01,\n",
       "          -6.4800e-04,  6.7733e-02,  2.4299e-01,  1.0364e-01,  1.7378e-01,\n",
       "          -1.1626e-01, -8.4500e-02,  6.1239e-01, -1.7385e-01,  3.8515e-01,\n",
       "           1.6924e-01,  2.7362e-01,  4.4961e-01,  7.4051e-02,  2.6497e-01,\n",
       "           4.6883e-01, -3.4269e-01, -3.4128e-01,  2.9193e-01,  3.1081e-01,\n",
       "           6.6285e-02,  1.4685e-01,  9.9816e-03,  7.2825e-02,  6.1722e-01,\n",
       "          -1.2769e-01,  1.5882e-01, -2.8921e-02,  1.1052e-01,  2.3703e-01,\n",
       "          -1.6836e-01,  3.0055e-01, -7.5994e-03, -7.1142e-01, -2.0341e-01,\n",
       "           6.2933e-02, -1.0060e-01, -3.2090e-01,  2.7566e-01, -2.5263e+00,\n",
       "           3.8540e-01,  4.4802e-01, -2.8834e-01, -6.1013e-01,  3.6481e-01,\n",
       "           2.5737e-01,  7.2272e-01, -1.3207e-01,  5.2345e-02,  6.5053e-02,\n",
       "          -2.0425e-01,  7.7375e-01,  2.3054e-01, -4.1609e-01,  6.0892e-01,\n",
       "           3.0096e-01,  2.8480e-01, -3.4917e-01,  3.3734e-01, -5.4842e-03,\n",
       "           2.4675e-01,  4.2451e-01, -1.4353e-01, -3.8538e-01,  2.5172e-02,\n",
       "          -3.5353e-02, -7.4354e-02, -1.7664e-01, -5.9882e-02, -3.7175e-02,\n",
       "          -6.6900e-01, -5.6730e-01, -3.0199e+00,  1.9939e-01,  2.1670e-01,\n",
       "           1.4529e-01, -4.6540e-02,  1.7395e-01, -1.2696e-01,  7.2293e-02,\n",
       "          -1.6102e-01, -2.5721e-02, -3.7977e-01,  8.6404e-02, -3.8759e-01,\n",
       "           4.5657e-01, -1.3948e-01, -1.0441e-01,  5.3317e-02,  8.0254e-01,\n",
       "           4.5787e-01,  2.6603e-01, -1.6308e-01,  4.0206e-02,  1.9756e-02,\n",
       "           5.9250e-02,  2.2223e-01,  1.0896e-01, -2.1422e-01, -4.9561e-01,\n",
       "           9.6572e-02, -1.4937e-01,  4.0153e-01, -9.3801e-02,  1.5701e-01,\n",
       "           1.7119e-02,  1.2677e-01, -1.0765e-01,  1.3160e-01, -2.2007e-01,\n",
       "          -1.9569e-01, -1.6576e-01,  6.0318e-02,  1.5952e-02,  1.2896e-01,\n",
       "          -1.9898e-01,  4.6294e-01, -1.8823e-01, -5.6883e-02,  3.0679e-01,\n",
       "          -4.7700e-01, -2.7078e-01,  5.3988e-01,  1.3567e-01,  4.7203e-01,\n",
       "           6.6893e-02,  2.8859e-02,  2.3025e-02,  4.1791e-01, -2.6479e-02,\n",
       "           8.5470e-02, -7.0883e-02,  2.0327e-01,  6.0417e-02, -3.9678e-01,\n",
       "           3.8188e+00, -3.9569e-02, -1.5289e-01,  3.5552e-02, -2.6327e-02,\n",
       "          -1.0063e-01,  1.3391e-02, -1.3459e-01,  1.8742e-01, -1.8554e-01,\n",
       "           5.7327e-02,  4.3753e-01, -2.6876e-02, -3.3332e-01,  4.6854e-02,\n",
       "           4.5140e-02,  2.4874e-01, -1.6000e-01,  2.3470e-02, -2.4974e-01,\n",
       "          -5.5073e-02,  2.0668e-01,  2.9402e-01,  2.8976e-01, -1.1744e+00,\n",
       "          -8.0358e-02, -2.1907e-01, -4.6641e-01,  3.0783e-01,  1.3619e-01,\n",
       "           1.2309e-01,  1.6474e-02, -8.2603e-02,  6.7855e-02,  1.5281e-01,\n",
       "           7.9157e-02,  2.5435e-01, -7.3475e-02, -2.8519e-03, -4.8640e-01,\n",
       "           1.2566e-01,  4.1494e-01, -4.6715e-02,  2.8539e-01, -1.8547e-01,\n",
       "           7.2026e-01,  5.8173e-02,  3.0325e-01, -1.1636e-01,  3.7811e-01,\n",
       "           2.6317e-02, -5.2808e-03, -9.5688e-02, -6.1708e-01, -1.7150e-01,\n",
       "          -3.1349e-01,  6.0887e-02, -5.2205e-02,  8.3346e-02, -3.2046e-01,\n",
       "          -2.2224e-01,  1.1170e-01, -4.6432e-01,  4.2958e-01, -8.1161e-02,\n",
       "           2.0050e-01, -1.6950e-01, -3.5856e-01, -3.2073e+00,  2.6226e-01,\n",
       "           1.3622e-01,  1.8876e-01,  1.4039e-01, -2.3836e-02, -1.1118e-01,\n",
       "          -7.5334e-02,  9.3513e-02, -2.2688e-01,  5.7477e-01,  2.8400e-01,\n",
       "          -5.1191e-01,  2.4174e-01, -3.1834e-01,  2.1395e-01,  3.6227e-01,\n",
       "          -1.2340e-02, -2.4781e-01, -1.6422e-01,  1.5478e-01,  3.0438e-01,\n",
       "          -2.4463e-01,  4.6564e-01, -4.2125e-02, -3.5767e-01,  1.3113e-01,\n",
       "          -2.9523e-01,  2.0734e-01, -3.1968e-01,  1.1887e-01, -3.5183e-01,\n",
       "           1.2883e-01, -2.2921e-01, -5.1459e-01, -3.7369e+00, -6.7924e-02,\n",
       "          -1.8977e-01, -3.4909e-01, -1.5081e-02,  1.2972e-01,  4.8003e-01,\n",
       "          -9.4636e-02,  1.8691e-01, -1.5343e-02,  7.6882e-02,  1.6276e-02,\n",
       "           3.4349e-01, -2.1813e-01,  2.2612e-01,  2.5971e-01,  2.6846e-02,\n",
       "          -1.7755e-01,  2.6964e-01,  4.2877e-02, -3.8287e-01,  3.5183e-01,\n",
       "           1.6959e-01,  6.2310e-02,  3.9083e-01,  7.3399e-02, -4.3301e-01,\n",
       "          -2.5343e-01,  1.0491e-01,  2.9780e-01,  4.9906e-01, -8.7879e-02,\n",
       "          -5.5081e-02, -9.1233e-02, -4.7901e-01, -1.9050e-01,  1.3518e-01,\n",
       "           1.1576e-01,  1.8629e-01, -1.0985e-01,  3.5885e-01,  3.5987e-01,\n",
       "          -2.6604e-01, -1.0402e-01,  5.7602e-01, -9.2520e-02, -5.9696e-02,\n",
       "           1.2006e-01,  2.3711e-01,  1.5174e-01, -1.4305e-01, -1.7223e-02,\n",
       "           1.3139e+00,  4.1896e-02,  3.2023e-01, -2.4983e-01,  3.0730e-02,\n",
       "           3.3008e-01,  2.2747e-01,  2.9309e-01,  7.8954e-01, -2.4016e-01,\n",
       "           4.3299e-01, -2.0550e-01, -5.0029e-02, -2.7797e-01,  1.0679e-01,\n",
       "          -4.5631e-01, -3.2802e-01, -1.0716e-01, -1.2746e-01,  1.5987e-01,\n",
       "          -4.0528e-02, -1.0807e+00, -2.7262e-03,  1.1223e-01, -2.7961e-01,\n",
       "           2.0443e-01, -1.4274e-01,  2.9305e-01, -9.8769e-02, -1.8132e-01,\n",
       "          -2.7735e-01,  4.8777e-01, -4.9216e-01, -1.2256e-01, -5.1250e-02,\n",
       "          -5.5936e-02, -7.6131e-01,  3.2496e-01, -8.8191e-02,  2.9611e-01,\n",
       "           6.5775e-02,  1.7404e-01, -1.1325e-01,  3.9774e-01,  5.4290e-01,\n",
       "          -6.5300e-01,  2.8928e-01,  1.1933e-01,  6.4772e-01,  1.4185e-01,\n",
       "           3.7946e-02, -1.3294e-02, -2.5424e-01,  2.2278e-02, -7.9847e-03,\n",
       "           2.4849e-01,  1.4314e-01,  2.5403e-02, -3.1661e-01,  9.1173e-02,\n",
       "          -5.5055e-01,  2.0097e-01,  5.6657e-01, -1.7972e-01, -6.5946e-02,\n",
       "           2.4009e-01,  3.1339e-01, -1.9241e-01,  1.1746e-02, -1.5010e-01,\n",
       "          -3.5521e-03,  1.5678e-01, -2.0484e-01, -5.0087e-02,  3.0306e-02,\n",
       "          -2.2197e-01, -8.7905e-02, -2.7798e-01, -4.5825e-02, -3.0818e-01,\n",
       "          -4.3192e-01, -4.2038e-01,  6.4857e-02, -3.7355e-01,  1.4338e-01,\n",
       "           3.1502e-02,  2.9674e-01, -1.5515e-01,  3.6030e-01, -1.6562e-01,\n",
       "          -1.7299e-01,  5.8191e-01, -5.3406e-01,  8.1930e-01, -5.9386e-02,\n",
       "          -1.6124e-02, -4.1461e-01,  3.7166e-01, -2.2980e-01, -1.5810e-01,\n",
       "           1.2804e-01, -1.6432e-01,  2.6387e-01,  2.7098e-01,  3.2714e-02,\n",
       "          -4.7723e-02, -1.8372e-01,  8.6000e-02,  1.5598e-01, -2.6125e-01,\n",
       "          -1.4969e+00,  5.2193e-01,  1.2996e-01, -1.0520e-01,  6.9684e-02,\n",
       "          -3.1168e-01, -2.7338e-01, -2.2576e-02, -3.4906e-01,  3.3035e-01,\n",
       "          -4.2820e-01,  1.0106e-01, -5.9582e-02, -6.6132e-02,  1.5114e-01,\n",
       "          -5.4287e-02,  2.3768e-01, -1.1275e-01, -3.2575e-01,  5.5870e-02,\n",
       "           2.4884e-01,  4.7014e-01,  3.8939e-01,  2.1366e-03,  4.1424e-01,\n",
       "          -2.1290e-03, -1.3591e-01,  6.6957e-01, -1.2426e-01,  4.7773e-02,\n",
       "          -9.6770e-02, -4.7307e-01, -2.6858e-01, -1.9634e-01,  4.1902e-01,\n",
       "           9.7128e-02,  1.9987e-02,  3.3852e-02,  2.8744e-01,  5.6616e-01,\n",
       "          -7.8083e-01,  3.4681e-01,  2.5891e-01,  4.0286e-01,  4.4446e-01,\n",
       "           5.4093e-02, -1.2883e-01,  1.5342e-01,  2.5310e-01, -2.0266e-01,\n",
       "           1.5462e-01, -2.7533e-01, -3.8596e-01, -6.0470e-02, -2.0872e-01,\n",
       "           3.7004e-02,  2.6507e-01,  4.3361e-02, -2.8811e-01,  9.7036e-02,\n",
       "           3.1501e-02, -4.8403e-02, -3.8900e-01,  2.3439e-01, -3.6248e-01,\n",
       "          -6.8314e-01,  1.5831e-03, -2.8779e-01,  1.1020e-02,  1.4739e-01,\n",
       "           2.8661e-01,  4.0796e-01, -5.6603e-01,  5.1238e-02, -2.8335e-01,\n",
       "           1.1705e-01,  1.5164e-02,  1.0867e-01,  7.5923e-03, -4.4284e-01,\n",
       "           4.1955e-01, -6.0114e-01, -4.8792e-01,  2.7281e-01, -3.0665e-01,\n",
       "          -8.4269e-03, -3.2335e-03, -3.4269e-02,  2.4013e-02, -2.6543e-02,\n",
       "          -1.6451e-02, -2.5029e-01,  2.5844e-02,  1.2837e-01,  6.3417e-02,\n",
       "           2.0845e-01, -1.5311e-01, -9.2479e-02, -2.9550e-01,  2.1320e-01,\n",
       "          -2.3991e-01,  4.1302e-01,  2.8859e-01,  5.2943e-01,  4.0050e-01,\n",
       "           1.5746e-01,  4.2885e-01,  1.7156e-01, -4.4130e-01,  2.5578e-01,\n",
       "           1.4756e-01,  1.1872e-01, -1.5096e-01, -8.5677e-02,  6.8421e-02,\n",
       "          -1.8366e-01,  2.7319e-01, -3.2753e-01,  1.9696e+00, -6.1008e-02,\n",
       "           2.8196e-02, -1.5686e-02,  6.1527e-01, -5.0404e-02,  1.1167e-01,\n",
       "          -2.5023e-02, -2.7066e-01,  4.6685e-01, -5.7870e-01,  5.9031e-01,\n",
       "          -2.9638e-01,  2.3471e-02,  4.4495e-01,  3.2464e-01,  1.0392e-01,\n",
       "          -4.7687e-01, -4.8580e-01, -1.0219e-01, -2.9589e-01, -1.0898e-01,\n",
       "           7.1440e-01, -2.9464e-02, -1.1718e-01,  2.0300e-01, -2.5120e-01,\n",
       "          -1.5713e-01,  1.8659e-01,  2.3696e-01,  6.5153e-02,  9.7188e-02,\n",
       "          -3.3968e-02,  4.3207e-01, -2.4011e-01, -9.0280e-03, -1.3435e-01,\n",
       "          -3.8057e-01, -3.8703e-01, -3.5994e-02, -9.1799e-02, -1.8714e-01,\n",
       "           4.0221e-01, -1.2164e-02, -2.1778e-01,  2.9162e-01, -4.0625e-02,\n",
       "          -3.0896e-01,  3.6134e-01, -5.2342e-03, -2.0575e-01, -4.6813e-02,\n",
       "          -1.3990e-01,  5.2351e-02,  2.3682e-01, -3.8673e-01, -1.6389e-01,\n",
       "          -4.1978e-01, -3.3448e-01,  5.6183e-01, -8.8617e-02,  1.2536e-01,\n",
       "          -9.0795e-02, -2.7297e-01,  7.0394e-02, -3.1280e-02, -1.3411e-01,\n",
       "           3.8804e-01,  2.6629e-01, -2.9622e-01, -1.7606e-01,  4.6008e-01,\n",
       "           3.1091e-01, -1.9589e-01,  7.5030e-02,  1.3761e-01,  4.2787e-02,\n",
       "          -2.6163e-01, -1.2631e-01, -2.7900e+00,  2.7023e-01,  1.1164e-01,\n",
       "          -9.5173e-02,  4.6793e-02,  1.0943e-01, -3.2008e-01,  2.4026e-02,\n",
       "           5.1400e-01, -4.8028e-02, -1.0334e-01,  5.1791e-01,  3.4476e-01,\n",
       "          -1.6126e-01,  1.1370e-01, -2.6608e-01,  3.6773e-01,  7.1049e-02,\n",
       "          -1.7340e-01,  1.3277e-01,  6.2655e-02,  1.2883e-01, -2.1411e-01,\n",
       "          -2.8408e-01, -1.5082e-01,  2.4463e-01,  1.5706e-01, -3.2323e-01,\n",
       "           3.3634e-01,  7.0573e-02, -3.3998e-01,  2.4832e-01, -1.2964e-01,\n",
       "          -1.6542e-01,  1.2621e-01, -2.9118e-02, -3.4131e-01, -1.4465e-01,\n",
       "           1.0280e-01,  2.5745e-01,  3.1245e-01,  3.1732e-01,  4.3022e-02,\n",
       "           2.2090e-01,  1.3777e-02,  2.0441e-01,  2.5289e-01, -7.7302e-03,\n",
       "           1.9379e-01, -2.9322e-01,  9.0197e-02, -1.3516e-01, -1.9358e-02,\n",
       "          -1.3425e-01,  2.8946e-01,  1.8120e-01, -3.0196e-02,  2.9746e-01,\n",
       "           2.0866e-01, -5.0434e-01, -5.4531e-01,  1.4098e-01, -9.2813e-02,\n",
       "          -8.5681e-02,  1.2559e-01, -1.1959e-01,  5.0061e-02, -7.7680e-02,\n",
       "           5.5218e-02, -2.4309e-01, -1.0905e-01, -7.5125e-02,  2.1553e-01,\n",
       "           5.0771e-01, -1.4505e-01, -1.0690e-01,  6.0204e-01,  3.0551e-01,\n",
       "           4.4676e-01,  1.6190e-01, -1.0923e-02, -3.9943e-01, -7.8054e-02,\n",
       "           1.9805e-01,  2.6393e-01, -7.1691e+00, -1.3859e-04,  6.3272e-03,\n",
       "          -5.1679e-01, -4.9074e-01, -8.0136e-01,  1.4891e-01, -1.6093e-01,\n",
       "           1.7430e-02, -3.7113e-02,  3.1080e-01, -1.2833e-01, -2.8121e-01,\n",
       "          -2.6854e-02,  2.1766e-01,  3.8542e-01]]),\n",
       " 'context12_bertoutput': tensor([[-2.9105e-02,  6.1969e-02,  1.6052e-01, -1.6352e-01, -4.5303e-01,\n",
       "          -3.3811e-01,  5.2497e-01,  4.8097e-01, -4.2518e-02, -2.6721e-01,\n",
       "          -2.4749e-01, -1.1140e-01,  3.8235e-02,  2.7941e-01,  6.1930e-01,\n",
       "           1.9386e-01, -3.8110e-01,  5.0101e-01,  1.0662e-01, -3.2838e-02,\n",
       "          -2.1432e-01, -5.4116e-01,  2.7173e-02, -1.7106e-01,  3.6119e-01,\n",
       "          -1.2474e-01,  1.2181e-02, -2.6128e-01,  2.0812e-01,  2.2275e-01,\n",
       "          -2.7621e-01,  1.9596e-01,  1.2262e-01, -2.0185e-01,  2.8912e-01,\n",
       "          -1.7773e-01, -9.2657e-03, -1.3136e-01,  1.4235e-01,  2.5380e-01,\n",
       "          -7.7294e-02, -1.0057e-01,  1.2751e-01,  2.9573e-02, -2.6944e-02,\n",
       "           2.7021e-02, -2.9039e+00,  1.9847e-01, -3.6507e-01, -4.2160e-01,\n",
       "           4.4433e-01, -2.7789e-01, -1.8364e-02,  1.3250e-01, -8.3070e-02,\n",
       "           3.2617e-01, -2.4543e-01,  3.7449e-01,  3.0261e-01,  3.6129e-01,\n",
       "           2.6361e-01,  1.4739e-01, -1.7323e-01,  2.1300e-01, -1.3773e-01,\n",
       "           6.7289e-02, -4.3988e-02,  5.1695e-01, -3.1038e-01,  6.2175e-01,\n",
       "          -1.5577e-01, -2.4761e-03,  3.2105e-01, -3.6329e-02, -2.0867e-02,\n",
       "           6.3725e-02,  1.3240e-01,  2.9524e-01, -2.3818e-01,  1.8491e-01,\n",
       "           1.5873e-01,  1.6464e-01,  4.0953e-01,  4.0717e-03,  1.0918e-01,\n",
       "           3.0744e-01, -1.4789e-01, -4.7872e-01,  2.9326e-01,  3.2041e-01,\n",
       "          -8.6598e-02, -3.9549e-02,  1.7671e-01,  2.4180e-01,  6.6208e-01,\n",
       "           2.2343e-02, -4.1434e-03, -1.1023e-01,  1.5139e-02, -7.8400e-03,\n",
       "          -2.7472e-01,  2.0081e-01, -4.8317e-02, -5.4188e-01,  3.9014e-02,\n",
       "          -1.4358e-02, -1.2302e-01, -3.0505e-02,  2.6024e-01, -2.8084e+00,\n",
       "           2.7433e-01,  4.1070e-01, -3.3356e-01, -2.9454e-01,  2.2293e-01,\n",
       "           9.5757e-02,  6.3512e-01, -7.9764e-03,  2.9698e-01, -1.2652e-02,\n",
       "          -1.6438e-01,  4.9312e-01,  5.5594e-02, -4.1376e-01,  1.7897e-01,\n",
       "           4.0757e-01,  2.8754e-01, -3.4544e-01,  5.5844e-01,  2.1783e-01,\n",
       "           6.6496e-02,  5.3659e-01, -2.0102e-01, -3.2077e-01,  1.8396e-02,\n",
       "          -1.8766e-01, -4.3063e-02, -8.0352e-02, -1.3364e-01, -2.3518e-01,\n",
       "          -5.4804e-01, -3.2503e-01, -3.1905e+00,  5.2166e-01,  2.8263e-01,\n",
       "           1.6423e-01, -1.9988e-01,  2.1346e-01, -2.6190e-01,  1.7479e-01,\n",
       "          -4.2352e-01,  7.5068e-02, -1.5100e-01,  1.7220e-01, -2.9667e-01,\n",
       "           1.1408e-01, -1.9685e-02, -4.3943e-01,  1.7348e-01,  6.6909e-01,\n",
       "           2.8302e-01,  1.4540e-01,  5.9887e-02, -7.9954e-02,  9.3509e-02,\n",
       "          -2.9190e-01,  1.7296e-01, -8.9745e-03, -3.3564e-02, -3.7060e-01,\n",
       "          -9.8872e-02, -2.2793e-02,  2.1517e-01,  1.2475e-01,  3.0558e-01,\n",
       "          -1.3405e-01,  2.5105e-01, -3.0648e-01,  3.8606e-02, -1.0888e-01,\n",
       "          -2.2385e-01,  2.4118e-02,  1.6283e-01,  1.4010e-01,  9.1636e-02,\n",
       "           1.0049e-01,  5.4020e-01, -1.5201e-01,  4.9101e-02,  2.4453e-01,\n",
       "          -5.6394e-02, -2.3291e-01,  6.4871e-01,  1.8745e-01,  4.3987e-01,\n",
       "           1.1249e-01, -1.8163e-01, -1.6074e-01,  5.3056e-01, -1.3057e-01,\n",
       "           1.4926e-01, -1.6601e-01,  2.6590e-01,  1.2567e-01, -2.7671e-01,\n",
       "           4.0333e+00, -8.0485e-02, -1.6746e-01,  5.6167e-02,  2.2854e-01,\n",
       "          -1.6431e-01,  6.2963e-03, -5.3734e-02,  4.0669e-02, -2.8453e-02,\n",
       "          -2.1938e-01,  3.3882e-01,  5.3817e-02, -2.6971e-01,  2.8707e-01,\n",
       "           1.1572e-01,  3.1229e-01, -3.0098e-01,  5.4579e-02, -1.7022e-01,\n",
       "           2.6371e-02,  1.0510e-01,  7.2733e-02,  3.1033e-01, -1.1578e+00,\n",
       "          -1.7010e-01,  9.3146e-02, -6.2159e-01,  2.9939e-01,  1.4053e-01,\n",
       "          -1.0301e-01,  1.1924e-03,  1.3935e-02,  5.8565e-02, -2.7972e-02,\n",
       "           1.6819e-01,  1.4419e-01, -3.5899e-03, -1.1495e-02, -2.6582e-01,\n",
       "           3.8269e-01,  2.3919e-01, -2.6130e-01,  3.0286e-01, -1.6109e-01,\n",
       "           9.7397e-01, -1.8824e-01, -1.7398e-02, -3.5351e-01,  3.1887e-01,\n",
       "           4.4351e-02, -3.0049e-02, -1.5460e-01, -8.2469e-01, -2.1356e-01,\n",
       "          -1.4160e-01,  6.0249e-03,  5.3452e-02,  1.0633e-01, -2.9872e-01,\n",
       "          -3.3358e-01, -8.1265e-02, -5.1924e-01,  3.2631e-02, -2.4534e-01,\n",
       "           2.9201e-02, -1.2136e-01, -2.2177e-01, -3.8656e+00,  1.8054e-01,\n",
       "           2.2207e-01,  2.7889e-01,  2.5640e-01, -2.5508e-02, -1.6550e-01,\n",
       "           7.9175e-02,  2.1489e-01, -1.8787e-01,  3.2164e-01,  3.8786e-01,\n",
       "          -3.4182e-01,  2.3874e-01, -1.0165e-01,  9.6513e-02,  3.2542e-01,\n",
       "          -6.1753e-02, -3.9958e-01, -2.2498e-01,  2.0487e-01,  1.6847e-01,\n",
       "          -5.4220e-01,  1.4975e-01,  1.2614e-01, -4.2612e-01,  7.5730e-02,\n",
       "          -3.3294e-01,  2.9606e-01, -5.6331e-01, -2.0273e-02, -2.0913e-01,\n",
       "           1.3841e-01, -1.4772e-01, -2.3006e-01, -2.9654e+00,  2.6156e-01,\n",
       "          -3.7119e-02, -3.3074e-01, -4.3553e-02, -7.1715e-02,  1.3891e-01,\n",
       "           1.8813e-01,  1.6784e-01, -1.2749e-01, -4.6526e-03, -1.3621e-02,\n",
       "           1.4353e-01, -1.2802e-01,  1.4849e-01,  2.7965e-01,  5.9410e-02,\n",
       "           6.0909e-03,  8.5502e-02, -1.0222e-01, -1.6446e-01,  1.7244e-01,\n",
       "           1.2359e-01,  1.1618e-01,  2.3432e-01,  1.9793e-01, -2.9328e-01,\n",
       "          -5.3397e-02, -2.6157e-02,  2.3533e-01,  2.5972e-01,  3.0754e-04,\n",
       "          -4.6138e-02, -7.6146e-02, -5.0413e-01, -1.7050e-01,  2.6007e-01,\n",
       "           2.6421e-01,  1.1475e-01, -7.8721e-02,  1.5220e-01,  5.4563e-01,\n",
       "          -1.8621e-01,  1.3846e-01,  6.4086e-01, -7.4668e-02, -1.1325e-01,\n",
       "           1.6698e-01,  1.6902e-01, -3.3962e-02, -2.4108e-01, -8.5084e-02,\n",
       "           1.2842e+00,  8.0810e-02,  2.4298e-01, -3.5808e-01,  1.8141e-01,\n",
       "           2.4891e-01,  2.0223e-01,  3.2743e-01,  8.7926e-01,  2.5292e-02,\n",
       "           4.2021e-01, -2.1893e-01,  8.7019e-02, -3.9591e-01,  1.0899e-01,\n",
       "          -1.4495e-01, -1.9586e-01, -3.9363e-04, -2.5280e-01,  2.5158e-01,\n",
       "           4.7495e-02, -1.0388e+00, -1.5359e-01,  1.0463e-01, -3.3550e-01,\n",
       "           2.4643e-01, -8.5490e-02,  1.9723e-01, -1.9485e-01, -1.1488e-01,\n",
       "          -4.8113e-01,  3.9627e-01, -5.8504e-01, -2.5635e-01, -2.0431e-02,\n",
       "           4.0735e-02, -5.2838e-01,  2.6484e-01, -2.1376e-01,  3.2679e-01,\n",
       "           3.0459e-02,  2.4318e-01,  5.4943e-02,  2.0183e-01,  5.1986e-01,\n",
       "          -9.4227e-01, -2.7598e-02,  1.5819e-01,  5.0953e-01, -4.1930e-02,\n",
       "          -1.5415e-01, -1.9428e-02, -2.6357e-01, -5.9813e-02,  4.4331e-02,\n",
       "           3.0439e-01, -1.3954e-01,  1.4105e-01, -3.1817e-01,  2.4637e-01,\n",
       "          -4.2825e-01,  1.6850e-01,  7.2809e-01, -1.0602e-01, -6.4025e-02,\n",
       "           2.0712e-01,  3.2243e-01, -1.1512e-02,  1.0415e-01,  5.0740e-02,\n",
       "           1.5331e-01,  3.0237e-01, -2.1435e-01, -1.0577e-01,  2.1065e-01,\n",
       "          -3.2977e-02, -1.1001e-01, -1.8063e-01,  9.1283e-02, -3.1631e-01,\n",
       "          -4.4896e-01, -5.9368e-01, -1.7627e-01, -5.0789e-01,  2.7093e-01,\n",
       "           1.1415e-01,  2.6143e-01, -2.2785e-01,  1.8083e-01,  1.0876e-01,\n",
       "          -1.8252e-01,  5.9147e-01, -1.9796e-01,  6.0317e-01,  4.1060e-02,\n",
       "          -1.2041e-01, -6.3778e-01,  3.3607e-01, -2.3561e-01, -1.9764e-01,\n",
       "          -2.8606e-02, -3.8575e-01,  3.2588e-01,  1.5803e-01, -6.9503e-02,\n",
       "           5.7834e-02, -3.4754e-02, -1.2252e-01,  1.9409e-01, -8.1172e-02,\n",
       "          -1.3948e+00,  3.7447e-01,  1.7979e-01, -4.8553e-02,  1.6230e-01,\n",
       "          -2.8538e-01, -2.8511e-01,  2.1691e-01, -4.1666e-01,  1.2737e-01,\n",
       "          -3.1117e-01,  1.7374e-01, -6.6375e-02, -1.0497e-01, -2.5964e-02,\n",
       "          -8.1455e-02,  4.0026e-01, -1.3615e-01, -3.9922e-01,  2.6251e-01,\n",
       "           8.6559e-02,  3.9990e-01,  2.8918e-01, -1.5954e-01,  3.5924e-02,\n",
       "           4.0603e-02, -1.4385e-02,  5.4437e-01, -1.3570e-01,  1.8394e-01,\n",
       "          -6.2425e-02, -4.2939e-01, -2.8102e-01, -1.5577e-01,  3.0536e-01,\n",
       "           2.3357e-01,  1.1711e-01, -2.5537e-02,  3.1950e-01,  5.3790e-01,\n",
       "          -4.9867e-01,  2.7720e-01,  3.0981e-01,  1.9687e-01,  6.4572e-01,\n",
       "           1.3009e-01, -1.7390e-01,  9.7495e-02,  3.2833e-01, -1.5580e-01,\n",
       "           1.3328e-01, -2.1198e-01, -2.0086e-01, -2.3754e-01, -1.1646e-01,\n",
       "          -1.2012e-01,  8.1529e-02, -1.1044e-01, -2.8678e-01, -9.0613e-02,\n",
       "           2.1354e-01,  4.3878e-02, -1.8553e-01,  1.5607e-01, -2.3413e-01,\n",
       "          -6.2010e-01,  2.8194e-01, -5.2740e-02,  2.6253e-02,  2.1230e-01,\n",
       "           2.6715e-01,  3.2214e-01, -4.8475e-01,  4.8894e-02, -2.5748e-01,\n",
       "           3.4720e-01,  1.3144e-01,  3.5231e-01,  1.6766e-01, -4.3599e-01,\n",
       "           3.7250e-01, -6.0361e-01, -2.8757e-01,  4.7068e-01, -3.9484e-01,\n",
       "           1.8919e-02, -1.3816e-02, -2.2664e-01,  2.1935e-01, -5.6735e-02,\n",
       "          -1.5006e-01, -2.1989e-01,  1.9500e-01, -1.3318e-01,  1.5082e-01,\n",
       "           9.1719e-02, -3.5270e-02,  5.5341e-02, -2.2410e-01,  5.6243e-02,\n",
       "           2.3965e-02,  3.1294e-01,  2.9790e-01,  4.4835e-01,  4.9796e-01,\n",
       "           9.8293e-02,  4.6778e-01,  2.5352e-01, -4.4308e-01,  1.9598e-01,\n",
       "          -6.2553e-02,  1.6912e-01, -1.8653e-01, -3.0956e-01, -3.1756e-02,\n",
       "          -2.0577e-01,  9.6550e-02, -2.6252e-01,  2.2880e+00,  3.1354e-02,\n",
       "           1.7693e-01, -4.3834e-02,  5.5089e-01, -3.6012e-02,  9.1365e-02,\n",
       "          -1.1172e-01, -3.6039e-01,  4.1569e-01, -4.8271e-01,  4.7282e-01,\n",
       "          -1.4745e-01,  1.7777e-01,  4.4560e-01,  3.0095e-01,  1.0041e-01,\n",
       "          -6.2192e-01, -1.7611e-01, -4.7418e-02, -4.3463e-01, -5.8219e-02,\n",
       "           4.0502e-01,  1.5955e-01,  4.6833e-02,  6.7037e-02, -2.0861e-01,\n",
       "          -2.8777e-01,  2.1522e-01, -1.8610e-02, -4.5775e-02,  1.0141e-01,\n",
       "           8.4814e-02,  6.8163e-01, -2.4142e-01,  9.1984e-02,  3.8627e-02,\n",
       "          -1.6804e-01, -2.7947e-01, -5.6224e-02, -4.5075e-02, -2.7341e-01,\n",
       "           4.3133e-01,  9.6440e-02,  1.4312e-01,  4.2487e-01, -7.1051e-02,\n",
       "          -4.3504e-01,  7.1442e-01,  1.7782e-01, -2.8811e-01, -2.6041e-01,\n",
       "          -3.8166e-01,  1.4054e-01,  2.1393e-01, -3.6979e-01, -1.2626e-01,\n",
       "          -2.4689e-01, -1.6685e-01,  3.9699e-01, -4.6231e-02,  1.2659e-01,\n",
       "           1.6601e-01, -2.0809e-01,  2.5517e-02, -6.9552e-03, -7.6854e-02,\n",
       "           2.0581e-01, -1.0834e-01, -3.1302e-01, -2.5782e-01,  5.9485e-01,\n",
       "           9.0106e-02, -1.8710e-02,  2.1195e-01,  1.9452e-01,  1.8334e-02,\n",
       "          -7.8133e-02, -9.3023e-02, -3.1224e+00,  2.2045e-01, -1.1487e-02,\n",
       "          -1.5063e-01, -1.3204e-01,  1.3920e-01, -1.7412e-01,  7.1801e-02,\n",
       "           1.5203e-01, -1.6607e-01, -1.9289e-01,  4.4835e-01,  2.2635e-01,\n",
       "          -1.3886e-02,  5.2959e-02, -6.5661e-02,  4.7326e-01,  1.1308e-01,\n",
       "          -2.6009e-01, -3.7455e-02, -1.8934e-01,  1.9775e-01, -1.9133e-01,\n",
       "          -1.5419e-01, -1.2379e-02,  3.0934e-01,  1.1225e-01, -5.3399e-01,\n",
       "           3.6764e-01,  3.3684e-01, -3.9929e-01,  1.6540e-01, -1.3194e-01,\n",
       "          -2.1264e-01,  1.5777e-01,  7.1063e-02, -1.6430e-01, -2.5640e-01,\n",
       "           1.4303e-01,  1.6422e-01,  2.7221e-01,  2.1274e-01,  1.9460e-01,\n",
       "           2.1057e-01, -7.3725e-02, -2.0028e-01,  4.1634e-01, -7.4674e-02,\n",
       "           3.3754e-01, -1.9203e-01,  3.9075e-01, -1.5973e-01, -8.0333e-03,\n",
       "          -1.7763e-01,  2.5361e-01,  2.0001e-01,  2.3161e-01,  3.9619e-01,\n",
       "           2.0965e-01, -4.7377e-01, -2.4418e-01,  1.0112e-01, -3.2981e-01,\n",
       "          -9.6240e-02,  1.3398e-01, -2.6449e-01, -1.2009e-01, -1.2639e-01,\n",
       "          -2.2699e-03, -2.1648e-01, -2.1107e-01, -3.0263e-02,  2.3945e-01,\n",
       "           3.3184e-01,  6.2360e-02,  3.5478e-03,  6.1303e-01,  7.7142e-02,\n",
       "           1.7814e-01,  1.0829e-01,  8.0601e-02, -3.1651e-01, -3.6116e-01,\n",
       "           1.3769e-01,  2.1275e-01, -8.0471e+00, -1.8290e-01, -8.1404e-03,\n",
       "          -2.0689e-01, -4.7635e-02, -4.9163e-01,  7.9446e-02,  8.6452e-02,\n",
       "          -1.6096e-03,  4.3527e-02,  2.4955e-01,  8.8355e-02, -1.5554e-01,\n",
       "          -3.7431e-01,  2.4314e-01,  3.8960e-01]]),\n",
       " 'context13_bertoutput': tensor([[ 1.3784e-01, -2.6492e-01,  1.8319e-01, -2.5983e-01, -2.1674e-01,\n",
       "          -5.5183e-01,  3.7540e-01,  8.8783e-01, -1.3462e-01, -5.2019e-01,\n",
       "          -1.9054e-01, -3.7306e-01,  1.6449e-01,  5.2434e-01,  6.7413e-01,\n",
       "          -6.5035e-02, -2.8418e-01,  9.6899e-01,  4.1181e-02, -1.6680e-01,\n",
       "          -1.4914e-01, -9.9177e-01,  1.8641e-01, -1.3105e-01,  2.5024e-01,\n",
       "          -8.4971e-02,  2.8482e-02, -2.7908e-01,  8.5988e-02, -3.6647e-01,\n",
       "          -4.1052e-01,  2.2067e-01, -5.6142e-02,  4.7959e-03,  5.5957e-01,\n",
       "          -2.5646e-01, -1.3923e-01, -1.2944e-01,  4.3133e-01,  6.1351e-01,\n",
       "          -1.5654e-01, -4.2181e-01,  4.9343e-01, -2.4737e-02, -3.7474e-03,\n",
       "          -2.6867e-01, -3.2545e+00,  5.0904e-01, -3.7007e-01, -9.4374e-02,\n",
       "           3.8195e-01,  9.1325e-02,  9.3978e-02, -1.8701e-01,  3.7144e-01,\n",
       "           4.1339e-01, -5.7577e-01,  3.4847e-01, -1.7379e-01,  3.7665e-01,\n",
       "           3.7584e-02,  1.7912e-01, -1.7842e-01,  2.4037e-02, -2.4451e-01,\n",
       "           9.5717e-02,  7.0061e-02,  6.5878e-01, -2.8850e-01,  7.0595e-01,\n",
       "          -2.4532e-01, -1.2274e-01,  1.4116e-01, -9.4233e-02,  3.4207e-01,\n",
       "           2.0886e-02, -1.6085e-01,  4.8403e-01, -1.9315e-01,  3.8309e-02,\n",
       "           9.2743e-02,  4.1747e-01,  2.9217e-01,  2.1743e-01,  3.4785e-01,\n",
       "           4.8150e-01, -5.3999e-01, -6.4293e-01,  2.9684e-01,  2.4691e-01,\n",
       "          -2.3117e-01,  1.2151e-02, -1.1027e-02,  3.1233e-01,  9.1522e-01,\n",
       "          -1.0670e-01,  2.1344e-01, -1.4789e-01, -1.1346e-02,  3.0186e-01,\n",
       "           1.9185e-02,  3.0822e-01,  9.3430e-02, -8.7181e-01, -5.0406e-01,\n",
       "           3.0565e-01, -9.4138e-02, -2.6632e-01,  1.8060e-01, -2.3335e+00,\n",
       "           3.5248e-01,  4.8923e-01, -5.6596e-01, -6.6824e-01,  1.7035e-01,\n",
       "          -3.2016e-02,  9.1481e-01, -1.2495e-01,  1.3694e-01,  6.0367e-02,\n",
       "          -5.8627e-02,  5.0560e-01,  2.7344e-01, -3.2564e-01,  3.0397e-01,\n",
       "           4.5438e-01,  3.4495e-01, -2.3247e-01,  5.0703e-01,  8.1798e-02,\n",
       "           2.1113e-01,  3.4829e-01, -4.8830e-01, -4.4851e-01,  3.2111e-02,\n",
       "          -6.5253e-02, -2.2241e-01, -1.6908e-01,  1.6322e-02, -6.3267e-02,\n",
       "          -4.8506e-01, -4.6880e-01, -3.0399e+00,  2.7215e-01,  6.9932e-01,\n",
       "           4.6448e-01, -2.7221e-01,  1.0211e-01, -3.1099e-02,  2.2909e-02,\n",
       "          -6.6771e-02, -1.5024e-01, -7.2061e-02,  7.6557e-02, -4.5277e-01,\n",
       "           4.2858e-01, -2.3708e-03, -2.4071e-01,  1.4473e-01,  7.0101e-01,\n",
       "           4.9940e-01, -4.7468e-02,  2.6294e-01,  2.9544e-02, -3.7846e-01,\n",
       "           4.3247e-01,  2.5423e-01,  2.2985e-01, -2.8295e-01, -5.0234e-01,\n",
       "          -8.8500e-02,  1.0623e-01,  5.1710e-01,  1.0778e-01,  2.1348e-02,\n",
       "          -2.5983e-02,  1.8597e-01,  1.3365e-01,  2.0166e-01, -1.9302e-01,\n",
       "          -3.4089e-01,  7.8277e-02, -4.8833e-02, -9.6386e-02,  3.2231e-01,\n",
       "          -1.8820e-02,  6.3598e-01, -5.3558e-03,  1.9770e-01,  2.6882e-01,\n",
       "          -3.7796e-01, -1.6645e-01,  4.3790e-01,  1.4772e-01,  3.9008e-01,\n",
       "          -3.8917e-01,  8.7620e-02, -3.5663e-01,  3.5748e-01, -1.4705e-03,\n",
       "          -1.7080e-01, -1.9901e-01,  4.1599e-01,  2.8798e-01, -4.5978e-01,\n",
       "           3.9851e+00, -1.1461e-01, -2.3823e-01, -2.1988e-02,  1.6843e-01,\n",
       "          -2.5386e-01,  1.1084e-01, -1.5433e-01,  3.0274e-01, -3.6388e-01,\n",
       "          -1.2236e-01,  3.4042e-01, -1.6826e-01, -1.6501e-01,  2.8697e-01,\n",
       "           5.5160e-02,  3.2505e-01, -8.8730e-02,  3.3546e-02, -2.5609e-01,\n",
       "           2.1670e-02,  1.0202e-01,  4.6481e-01,  2.4495e-01, -1.1052e+00,\n",
       "          -2.1207e-01, -7.8328e-02, -5.6436e-01,  2.8369e-01,  4.2819e-02,\n",
       "           2.9998e-01,  5.9042e-02, -3.3529e-01,  1.1770e-01, -1.9763e-02,\n",
       "           2.3391e-03,  9.5815e-02, -1.3770e-01,  6.4232e-02, -4.6659e-01,\n",
       "           5.0718e-01,  5.8351e-02, -2.6236e-01,  6.2720e-01, -2.0978e-01,\n",
       "           7.1694e-01,  5.3724e-02,  1.1125e-01, -1.2719e-01,  4.0482e-01,\n",
       "           1.3238e-02, -1.2030e-01, -1.9215e-01, -8.8088e-01, -2.1221e-02,\n",
       "          -2.7477e-01, -6.3932e-02,  2.0299e-01, -6.9504e-02, -6.5492e-01,\n",
       "          -4.6828e-01,  1.2539e-01, -4.9875e-01,  4.0335e-01, -3.0907e-02,\n",
       "           5.6160e-02, -3.2371e-01, -3.4547e-01, -2.9765e+00,  2.9323e-01,\n",
       "           2.6500e-02,  9.9002e-02,  3.4071e-01,  5.6664e-02, -2.4694e-01,\n",
       "           9.7633e-02,  3.5789e-02, -2.0499e-01,  7.2550e-01,  5.7209e-01,\n",
       "          -5.5429e-01, -8.3938e-02, -1.4256e-01,  8.6478e-02,  2.4706e-01,\n",
       "          -3.4961e-01, -4.4909e-01, -4.0053e-01, -3.9679e-02,  4.3326e-01,\n",
       "          -3.3275e-01,  4.6944e-01,  2.0832e-01, -3.2351e-01, -9.9713e-02,\n",
       "          -1.9048e-01,  2.4043e-02, -4.7400e-01, -2.5804e-02, -2.6674e-01,\n",
       "           1.9878e-01, -1.0515e-01, -6.7725e-01, -3.0553e+00,  9.4242e-02,\n",
       "          -3.0024e-01, -5.8820e-01, -1.2635e-01, -2.2360e-01,  6.4713e-01,\n",
       "          -8.3982e-02,  2.4534e-01, -9.9401e-02,  1.7521e-01,  1.5551e-01,\n",
       "           3.3364e-01,  7.0210e-03,  6.7135e-02,  7.1772e-01,  3.4406e-01,\n",
       "          -2.3016e-03,  3.8515e-01, -5.1490e-03, -3.5588e-01,  3.3882e-01,\n",
       "           1.3734e-02,  3.0492e-01,  5.0562e-01,  9.6661e-02, -2.7667e-01,\n",
       "          -2.9188e-01,  2.0449e-01,  3.3877e-01,  4.7139e-01, -9.1953e-02,\n",
       "          -2.4038e-01, -3.3068e-01, -4.4442e-01, -2.3912e-01,  2.6607e-01,\n",
       "           1.8474e-01,  2.1113e-01, -1.9249e-01,  1.1124e-01,  2.1061e-01,\n",
       "          -3.2504e-01,  2.9459e-01,  8.6829e-01, -5.9334e-03,  2.1012e-01,\n",
       "          -4.9383e-02,  4.9332e-01,  4.6198e-01, -2.1479e-01,  1.5273e-01,\n",
       "           1.6420e+00, -6.4412e-02,  3.7728e-01, -3.0320e-01,  2.2644e-01,\n",
       "           2.3958e-01, -1.5158e-01,  3.6560e-01,  8.8223e-01,  2.4067e-01,\n",
       "           5.4441e-01, -4.9398e-01,  5.9101e-02, -4.6030e-01, -1.8275e-01,\n",
       "          -7.1554e-01, -2.7340e-01, -1.3565e-01, -2.9491e-01,  1.8290e-01,\n",
       "          -1.4617e-02, -8.1772e-01,  2.1402e-01,  9.7799e-02, -3.1865e-01,\n",
       "           2.8984e-01, -1.6298e-01,  2.3651e-01, -4.0328e-01, -1.9037e-01,\n",
       "          -3.6633e-01,  4.2686e-01, -4.3901e-01, -1.0020e-01,  2.8281e-02,\n",
       "           3.3772e-01, -8.9629e-01,  1.6168e-01, -2.0986e-01,  3.3880e-01,\n",
       "           9.3211e-02,  5.4760e-01, -4.2547e-01,  3.7550e-01,  5.8847e-01,\n",
       "          -7.2555e-01,  1.4391e-01, -9.1932e-02,  6.0098e-01,  3.5566e-01,\n",
       "          -1.0002e-01, -9.9518e-02, -4.3203e-01, -1.2367e-01, -4.1716e-02,\n",
       "           1.2341e-01,  2.0583e-01,  1.0580e-01, -2.1723e-01,  1.8207e-01,\n",
       "          -5.6563e-01, -1.0391e-01,  6.1930e-01, -2.6151e-01,  4.2896e-03,\n",
       "           3.8001e-01,  6.0407e-01, -2.3255e-01,  2.4049e-02, -3.2241e-01,\n",
       "           6.0548e-02,  1.4366e-01, -1.7452e-01,  1.0881e-01, -1.4616e-01,\n",
       "          -2.7652e-01, -3.2186e-01, -3.6821e-01,  1.0225e-01, -2.8358e-01,\n",
       "          -3.8883e-01,  3.5980e-02, -2.8375e-02, -6.2766e-01,  3.2903e-02,\n",
       "          -1.5360e-02,  2.2595e-01, -1.2840e-01,  3.6183e-01, -4.7734e-02,\n",
       "          -1.9706e-01,  5.6734e-01, -6.9197e-01,  7.3575e-01, -9.6473e-02,\n",
       "           1.5332e-01, -5.5432e-01,  4.4450e-01, -9.0497e-02, -7.7535e-02,\n",
       "          -1.4923e-01, -5.9662e-01,  1.7914e-01,  3.4708e-01, -7.5623e-02,\n",
       "           4.8169e-03,  1.3307e-01, -1.9553e-01,  1.1171e-01, -1.9358e-01,\n",
       "          -1.6771e+00,  6.4209e-01,  2.0174e-01, -6.7873e-02,  2.4517e-01,\n",
       "          -1.7447e-01, -1.7567e-01,  8.0565e-02, -2.8674e-01,  5.3339e-01,\n",
       "          -3.5101e-01, -1.0984e-01,  1.4138e-01,  3.0409e-01,  3.7446e-01,\n",
       "          -9.9335e-02,  2.8970e-01,  1.0017e-01, -4.3762e-01, -2.7504e-01,\n",
       "           7.0630e-02,  4.3806e-01,  2.9441e-01,  3.0717e-01,  2.9493e-01,\n",
       "          -1.0034e-01, -9.9803e-02,  4.3427e-01, -1.6006e-01,  2.5666e-01,\n",
       "          -1.2018e-01, -7.7064e-01, -5.7040e-01, -3.5579e-01,  5.4138e-01,\n",
       "           2.7960e-01,  2.7171e-01,  7.6355e-02,  3.6280e-01,  6.9840e-01,\n",
       "          -7.1213e-01,  2.9654e-01,  9.9723e-02,  3.9866e-01,  5.9799e-01,\n",
       "           2.5156e-01, -2.6492e-01,  1.7709e-01,  7.5017e-02, -1.9617e-01,\n",
       "           2.9315e-01, -1.9222e-01, -1.3627e-01, -2.8708e-01, -4.2513e-01,\n",
       "          -1.9462e-02,  3.1521e-01, -1.2602e-01, -3.6933e-01,  6.4163e-02,\n",
       "           4.2376e-01, -4.5979e-01, -4.4102e-01,  3.4572e-02, -2.8351e-01,\n",
       "          -7.0695e-01,  1.1616e-01, -2.8901e-01, -2.1506e-01,  1.7074e-01,\n",
       "           1.1846e-01,  5.2327e-01, -4.7101e-01,  4.7976e-02, -3.8222e-01,\n",
       "          -7.0978e-02, -1.7366e-01, -2.0827e-01,  1.4232e-01, -4.3776e-01,\n",
       "           3.9497e-01, -1.0320e+00, -6.9963e-01,  2.4916e-01, -4.6184e-01,\n",
       "          -1.1341e-01, -1.7295e-02, -7.9514e-02, -4.6575e-02,  1.5747e-01,\n",
       "          -3.5186e-01, -2.3205e-01,  3.4984e-01,  3.0075e-02,  2.0312e-01,\n",
       "          -6.2197e-02, -4.0509e-02, -7.7577e-02, -9.0601e-03, -4.0559e-02,\n",
       "          -8.1380e-02,  8.2419e-01,  4.7296e-01,  4.7348e-01,  6.3659e-01,\n",
       "           8.9252e-02,  3.7424e-01, -2.0020e-01, -3.6187e-01,  5.1915e-01,\n",
       "          -2.1148e-01, -2.3420e-01, -2.7110e-01,  1.9717e-01,  2.9862e-02,\n",
       "          -4.4411e-01,  6.3602e-02, -4.0196e-01,  1.9793e+00,  2.5618e-01,\n",
       "           2.1705e-02, -2.2130e-01,  5.3562e-01, -1.6476e-01, -2.3649e-01,\n",
       "           3.1986e-01, -3.0038e-01,  8.7516e-01, -5.9486e-01,  6.6098e-01,\n",
       "          -5.0579e-01,  1.7888e-01,  6.8514e-01,  2.7684e-01,  1.1499e-01,\n",
       "          -5.9204e-01, -5.2796e-01, -8.8267e-02, -2.1791e-01,  2.6629e-01,\n",
       "           6.6100e-01,  1.3035e-01, -9.5202e-02,  1.5122e-01, -2.6102e-01,\n",
       "          -3.0958e-01, -1.2306e-01,  4.6015e-01, -1.8751e-01,  4.8789e-02,\n",
       "          -6.0359e-02,  7.1706e-01, -3.1187e-01,  6.9138e-02, -2.8671e-01,\n",
       "          -3.8433e-01, -2.7171e-01,  4.6911e-03, -1.1582e-01, -4.0052e-01,\n",
       "           5.9771e-01, -1.5903e-01, -4.5321e-01,  4.0827e-01, -6.0782e-02,\n",
       "          -2.5723e-01,  4.6010e-01,  3.0945e-02, -1.7718e-01,  4.6612e-02,\n",
       "          -1.6891e-01,  2.2564e-01,  1.8588e-01, -4.5494e-01, -2.6618e-01,\n",
       "           8.1313e-02, -5.7090e-01,  6.5041e-01, -3.1641e-01,  1.0343e-01,\n",
       "           1.1425e-01, -3.3724e-01,  4.3665e-01,  1.1571e-01, -4.1855e-01,\n",
       "           4.7718e-01,  2.8877e-01, -5.5435e-01, -2.0677e-01,  6.7296e-01,\n",
       "           4.3938e-01, -1.9834e-01,  2.3882e-01, -7.7969e-02,  4.6355e-01,\n",
       "          -3.0095e-01, -4.0252e-02, -2.6944e+00,  5.6357e-01,  3.1629e-01,\n",
       "          -3.3503e-02, -1.8281e-01,  2.2799e-01, -3.4089e-01,  1.9165e-01,\n",
       "           3.6220e-01, -1.6162e-02, -3.3679e-02,  3.7383e-01,  6.5704e-01,\n",
       "          -1.8986e-01,  7.1879e-03, -6.3339e-02,  6.0900e-01, -6.1757e-02,\n",
       "          -5.0359e-01, -4.3438e-02,  1.7615e-01,  8.7315e-02, -5.0377e-01,\n",
       "          -4.3906e-01, -1.0971e-01,  1.7617e-01, -4.3699e-03, -4.2152e-01,\n",
       "           3.1431e-02,  2.8336e-01, -2.2103e-01,  6.8880e-01,  3.8591e-02,\n",
       "          -1.6754e-01,  2.7721e-01,  1.7584e-01, -1.0889e-01,  1.1183e-02,\n",
       "           2.0110e-01,  3.0221e-01,  2.8506e-01,  4.1526e-01, -2.5113e-01,\n",
       "           2.3960e-01, -2.1620e-01,  3.8855e-01,  2.5611e-01,  2.9568e-02,\n",
       "           9.3076e-02, -1.7157e-01,  3.0983e-01,  5.2394e-02,  1.1090e-01,\n",
       "          -1.4130e-01,  1.7498e-01,  1.1235e-01,  1.1979e-01,  2.6664e-01,\n",
       "           2.1076e-01, -6.0439e-01, -3.2630e-01,  8.7464e-02, -1.8299e-01,\n",
       "           2.9865e-02,  1.5700e-01, -2.0946e-01,  1.2758e-01, -3.6888e-02,\n",
       "           6.0658e-02, -4.1821e-01, -1.5407e-01, -1.7020e-01,  1.5613e-01,\n",
       "           5.5667e-01, -3.4222e-02, -1.8477e-01,  4.3717e-01,  2.7519e-01,\n",
       "           6.9223e-01,  4.1663e-01,  5.6011e-03, -3.6042e-01, -1.3456e-01,\n",
       "           1.9110e-01,  3.5247e-01, -7.1904e+00,  3.6803e-02, -3.3162e-01,\n",
       "          -6.3553e-01, -7.6882e-01, -6.8001e-01, -9.5435e-02, -3.3401e-01,\n",
       "          -9.4077e-02, -2.2933e-01,  2.6634e-01,  1.3317e-01, -3.1980e-01,\n",
       "          -2.0104e-01,  6.1876e-02,  4.8190e-01]]),\n",
       " 'anchor_addfeat': {'num_entireuppercasewords': 0.0,\n",
       "  'num_urls': 3.4812880765883375e-05,\n",
       "  'num_exclamationmarks': 0.0005056890012642225,\n",
       "  'num_strongsubj': 0.0,\n",
       "  'num_weaksubj': 0.0,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 4.492089110213406e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 2.7942912629497935e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '1'},\n",
       " 'context8_addfeat': {'num_entireuppercasewords': 0.0,\n",
       "  'num_urls': 0.0,\n",
       "  'num_exclamationmarks': 0.0001685630004214075,\n",
       "  'num_strongsubj': 6.429627724554748e-05,\n",
       "  'num_weaksubj': 5.1939957409234925e-05,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 1.1230222775533515e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 1.3971456314748967e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '0'},\n",
       " 'context9_addfeat': {'num_entireuppercasewords': 0.00014996250937265683,\n",
       "  'num_urls': 0.0,\n",
       "  'num_exclamationmarks': 0.000337126000842815,\n",
       "  'num_strongsubj': 0.0,\n",
       "  'num_weaksubj': 0.0,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 1.2834540314895447e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 2.0957184472123452e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '0'},\n",
       " 'context10_addfeat': {'num_entireuppercasewords': 0.00014996250937265683,\n",
       "  'num_urls': 3.4812880765883375e-05,\n",
       "  'num_exclamationmarks': 0.0001685630004214075,\n",
       "  'num_strongsubj': 0.00012859255449109496,\n",
       "  'num_weaksubj': 0.00010387991481846985,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 5.454679633830565e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 4.890009710162139e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '0'},\n",
       " 'context11_addfeat': {'num_entireuppercasewords': 9.997500624843789e-05,\n",
       "  'num_urls': 3.4812880765883375e-05,\n",
       "  'num_exclamationmarks': 0.0005056890012642225,\n",
       "  'num_strongsubj': 0.0,\n",
       "  'num_weaksubj': 0.0,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 3.0482033247876685e-05,\n",
       "  'num_elong': 0.00211864406779661,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 4.1914368944246904e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '1'},\n",
       " 'context12_addfeat': {'num_entireuppercasewords': 9.997500624843789e-05,\n",
       "  'num_urls': 0.0,\n",
       "  'num_exclamationmarks': 0.00025284450063211124,\n",
       "  'num_strongsubj': 0.0,\n",
       "  'num_weaksubj': 0.0,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 1.4438857854257377e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 1.3971456314748967e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '1'},\n",
       " 'context13_addfeat': {'num_entireuppercasewords': 9.997500624843789e-05,\n",
       "  'num_urls': 3.4812880765883375e-05,\n",
       "  'num_exclamationmarks': 8.428150021070375e-05,\n",
       "  'num_strongsubj': 0.00012859255449109496,\n",
       "  'num_weaksubj': 5.1939957409234925e-05,\n",
       "  'num_emoji': 0.0,\n",
       "  'num_tokens': 3.689930340532441e-05,\n",
       "  'num_elong': 0.0,\n",
       "  'num_hashtags': 0.0,\n",
       "  'num_uppercasewords': 2.0957184472123452e-05,\n",
       "  'loc_hashtag': '0',\n",
       "  'loc_mention': '1'},\n",
       " 'anchor_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 3.4813e-05,\n",
       "          5.0569e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4921e-05, 0.0000e+00,\n",
       "          0.0000e+00, 2.7943e-05]]),\n",
       " 'context10_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.4996e-04, 3.4813e-05,\n",
       "          1.6856e-04, 1.2859e-04, 1.0388e-04, 0.0000e+00, 5.4547e-05, 0.0000e+00,\n",
       "          0.0000e+00, 4.8900e-05]]),\n",
       " 'context11_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 9.9975e-05, 3.4813e-05,\n",
       "          5.0569e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0482e-05, 2.1186e-03,\n",
       "          0.0000e+00, 4.1914e-05]]),\n",
       " 'context12_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 9.9975e-05, 0.0000e+00,\n",
       "          2.5284e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4439e-05, 0.0000e+00,\n",
       "          0.0000e+00, 1.3971e-05]]),\n",
       " 'context13_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 9.9975e-05, 3.4813e-05,\n",
       "          8.4281e-05, 1.2859e-04, 5.1940e-05, 0.0000e+00, 3.6899e-05, 0.0000e+00,\n",
       "          0.0000e+00, 2.0957e-05]]),\n",
       " 'context8_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.6856e-04, 6.4296e-05, 5.1940e-05, 0.0000e+00, 1.1230e-05, 0.0000e+00,\n",
       "          0.0000e+00, 1.3971e-05]]),\n",
       " 'context9_addfeattensor': tensor([[1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.4996e-04, 0.0000e+00,\n",
       "          3.3713e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2835e-05, 0.0000e+00,\n",
       "          0.0000e+00, 2.0957e-05]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_polished(instances):\n",
    "    \"\"\"\n",
    "    split the integer feature values into some bins and convert it into string\n",
    "    :param instances: instances with integer values\n",
    "    :return: instances with string values\n",
    "    \"\"\"\n",
    "    # get the statistics of the features whose value is integer\n",
    "    bin_size = 6\n",
    "    feat_dicts = [value for instance in instances for key, value in instance.items() if key.endswith(\"addfeat\")]\n",
    "    keys = [key for key, value in feat_dicts[0].items() if isinstance(value, int)]\n",
    "    feat_numbers = {key: [feat_dict[key] for feat_dict in feat_dicts] for key in keys}\n",
    "    cutted_dict = defaultdict(dict)\n",
    "    for key, value in feat_numbers.items():\n",
    "        stats = Counter(value)\n",
    "        # only get the most common value\n",
    "        most_common = dict(stats.most_common(int(len(stats) / bin_size) + 1))\n",
    "        for small_key in most_common.keys():\n",
    "            cutted_dict[key].update({small_key: str(small_key)})\n",
    "        cutted_dict[key].update({'other': 'other'})\n",
    "\n",
    "    # update the feature value\n",
    "    for instance in instances:\n",
    "        keys = [key for key in instance.keys() if key.endswith(\"addfeat\")]\n",
    "        for key in keys:\n",
    "            feat_values = instance[key]\n",
    "            for feat_name in list(feat_values.keys()):\n",
    "                feat_value = feat_values[feat_name]\n",
    "                if not isinstance(feat_value, int):\n",
    "                    continue\n",
    "                if feat_value in cutted_dict.keys():\n",
    "                    instance[key][feat_name] = cutted_dict[feat_name][feat_value]\n",
    "                else:\n",
    "                    instance[key][feat_name] = cutted_dict[feat_name]['other']\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_instances, dev_instances, test_instances = preprocess.split_instances(instances)\n",
    "train_loader, dev_loader, test_loader = preprocess.get_data_loader(train_instances, dev_instances, test_instances, batch_size=batch_size)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parameter setting\n",
    "bert_feat_dim = 768\n",
    "lstm_dim = 1500\n",
    "output_dim = 2\n",
    "hidden_dim = 512\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "class BertLstmTest(nn.Module):\n",
    "\n",
    "    def __init__(self, additional_feat_dim=0):\n",
    "        super(BertLstmTest, self).__init__()\n",
    "        self.bert_feat_dim = bert_feat_dim\n",
    "        self.additional_feat_dim = additional_feat_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lstm = nn.LSTM(self.bert_feat_dim+self.additional_feat_dim+7, self.lstm_dim, batch_first=True, bidirectional=True)\n",
    "#         self.fc1 = nn.Linear(self.lstm_dim*2, hidden_dim)\n",
    "        self.fc1 = nn.Linear(self.lstm_dim*2*7, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)       \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, feat_combined):\n",
    "        \n",
    "        # get the bert output\n",
    "        context1_feat = feat_combined[:, :self.bert_feat_dim * 1].unsqueeze(1)\n",
    "        context2_feat = feat_combined[:, self.bert_feat_dim * 1:self.bert_feat_dim * 2].unsqueeze(1)\n",
    "        context3_feat = feat_combined[:, self.bert_feat_dim * 2:self.bert_feat_dim * 3].unsqueeze(1)\n",
    "        anchor_feat = feat_combined[:, self.bert_feat_dim * 3:self.bert_feat_dim * 4].unsqueeze(1)\n",
    "        context4_feat = feat_combined[:, self.bert_feat_dim * 4:self.bert_feat_dim * 5].unsqueeze(1)\n",
    "        context5_feat = feat_combined[:, self.bert_feat_dim * 5:self.bert_feat_dim * 6].unsqueeze(1)\n",
    "        context6_feat = feat_combined[:, self.bert_feat_dim * 6:self.bert_feat_dim * 7].unsqueeze(1)\n",
    "\n",
    "        # get the additional features\n",
    "        addfeat_dim = int((feat_combined.shape[1] - self.bert_feat_dim * 7) / 7)\n",
    "        context1_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 0:self.bert_feat_dim * 7 + addfeat_dim * 1].unsqueeze(1)\n",
    "        context2_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 1:self.bert_feat_dim * 7 + addfeat_dim * 2].unsqueeze(1)\n",
    "        context3_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 2:self.bert_feat_dim * 7 + addfeat_dim * 3].unsqueeze(1)\n",
    "        anchor_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 3:self.bert_feat_dim * 7 + addfeat_dim * 4].unsqueeze(1)\n",
    "        context4_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 4:self.bert_feat_dim * 7 + addfeat_dim * 5].unsqueeze(1)\n",
    "        context5_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 5:self.bert_feat_dim * 7 + addfeat_dim * 6].unsqueeze(1)\n",
    "        context6_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 6:self.bert_feat_dim * 7 + addfeat_dim * 7].unsqueeze(1)\n",
    "\n",
    "        # get the position encoding\n",
    "        position_dim = 7\n",
    "        context1_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 0: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 1].unsqueeze(1)\n",
    "        context2_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 1: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 2].unsqueeze(1)\n",
    "        context3_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 2: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 3].unsqueeze(1)\n",
    "        anchor_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 3: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 4].unsqueeze(1)\n",
    "        context4_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 4: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 5].unsqueeze(1)\n",
    "        context5_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 5: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 6].unsqueeze(1)\n",
    "        context6_position = feat_combined[:, (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 6: (self.bert_feat_dim + addfeat_dim) * 7 + position_dim * 7].unsqueeze(1)\n",
    "\n",
    "        # prepare for the input of LSTM\n",
    "        lstm_input = torch.cat((\n",
    "            torch.cat((context1_feat, context1_addfeat, context1_position), dim=2),\n",
    "            torch.cat((context2_feat, context2_addfeat, context2_position), dim=2),\n",
    "            torch.cat((context3_feat, context3_addfeat, context3_position), dim=2),\n",
    "            torch.cat((anchor_feat, anchor_addfeat, anchor_position), dim=2),\n",
    "#             anchor_feat,\n",
    "            torch.cat((context4_feat, context4_addfeat, context4_position), dim=2),\n",
    "            torch.cat((context5_feat, context5_addfeat, context5_position), dim=2),\n",
    "            torch.cat((context6_feat, context6_addfeat, context6_position), dim=2),\n",
    "        ), dim=1)\n",
    "        \n",
    "        # pass the LSTM\n",
    "        lstm_output, _ = self.lstm(lstm_input)\n",
    "        \n",
    "# #         only take the last hidden state\n",
    "#         lstm_output = lstm_output[:, -1, :]\n",
    "\n",
    "        lstm_output = torch.reshape(lstm_output, (-1, 7*self.lstm_dim*2))\n",
    "        \n",
    "        # global average pooling on the BiLSTM output\n",
    "#         lstm_output = nn.AvgPool1d(7, 7)(lstm_output.permute(0,2,1)).permute(0,2,1).squeeze(1)\n",
    "                \n",
    "        # pass the fully-connected layer(s)\n",
    "        out = self.fc1(lstm_output)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# define the label mapping\n",
    "label_to_idx = {'Yes': 1, 'No': 0}\n",
    "idx_to_label = {1: 'Yes', 0: 'No'}\n",
    "    \n",
    "# define some global parameters\n",
    "learning_rate = 1e-05\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "additional_feat_dim = instances[0]['anchor_addfeattensor'].shape[1]\n",
    "classifier = BertLstmTest(additional_feat_dim=additional_feat_dim)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# define the optimizer, loss function, and some parameters\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch: 1 || Epoch Time: 0m 8s\n",
      "Epoch: 1 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 1 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 2 || Epoch Time: 0m 8s\n",
      "Epoch: 2 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 2 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 3 || Epoch Time: 0m 8s\n",
      "Epoch: 3 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 3 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 4 || Epoch Time: 0m 8s\n",
      "Epoch: 4 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 4 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 5 || Epoch Time: 0m 8s\n",
      "Epoch: 5 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 5 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 6 || Epoch Time: 0m 8s\n",
      "Epoch: 6 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 6 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 7 || Epoch Time: 0m 8s\n",
      "Epoch: 7 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 7 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 8 || Epoch Time: 0m 8s\n",
      "Epoch: 8 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 8 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 9 || Epoch Time: 0m 8s\n",
      "Epoch: 9 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 9 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 10 || Epoch Time: 0m 8s\n",
      "Epoch: 10 || Train loss: 0.08, Train Acc: 0.68\n",
      "Epoch: 10 || Dev loss: 0.08, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 11 || Epoch Time: 0m 8s\n",
      "Epoch: 11 || Train loss: 0.07, Train Acc: 0.70\n",
      "Epoch: 11 || Dev loss: 0.08, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "The loss on development set does not decrease, stop training!\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.29      0.02      0.03       226\n",
      "         Yes       0.68      0.98      0.80       473\n",
      "\n",
      "    accuracy                           0.67       699\n",
      "   macro avg       0.48      0.50      0.42       699\n",
      "weighted avg       0.55      0.67      0.55       699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 5\n",
    "best_valid_loss = float('inf')\n",
    "check_stopping = 0\n",
    "model_name = f'test_bert_lstm.pt'\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    classifier.train()\n",
    "    train_loss, train_acc = learning_helper.train(classifier, train_loader, optimizer, criterion, device, label_to_idx)\n",
    "    dev_loss, dev_acc = learning_helper.evaluate(classifier, dev_loader, criterion, device, label_to_idx)\n",
    "    classifier.train()\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Epoch: {i+1} || Epoch Time: {elapsed_mins}m {elapsed_secs}s\")\n",
    "    print(f\"Epoch: {i+1} || Train loss: {train_loss:.02f}, Train Acc: {train_acc:.02f}\")\n",
    "    print(f\"Epoch: {i+1} || Dev loss: {dev_loss:.02f}, Dev Acc: {dev_acc:.02f}\")\n",
    "\n",
    "    # check if we need to save the model\n",
    "    if dev_loss < best_valid_loss:\n",
    "        check_stopping = 0\n",
    "        best_valid_loss = dev_loss\n",
    "#         torch.save(classifier.state_dict(), model_name)\n",
    "        torch.save(classifier.state_dict(), model_name)\n",
    "    else:\n",
    "        check_stopping += 1\n",
    "        print(f\"The loss on development set does not decrease\")\n",
    "        if check_stopping == patience:\n",
    "            print(\"The loss on development set does not decrease, stop training!\")\n",
    "            break\n",
    "            \n",
    "classifier.eval()\n",
    "pred_labels = evaluator.test_model(classifier, test_loader, idx_to_label, device)\n",
    "gold_labels = [x['adjudicated_label'] for x in test_instances]\n",
    "print('-' * 60)\n",
    "print(classification_report(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'aa.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.43      0.25      0.32       226\n",
      "         Yes       0.70      0.84      0.76       473\n",
      "\n",
      "    accuracy                           0.65       699\n",
      "   macro avg       0.56      0.55      0.54       699\n",
      "weighted avg       0.61      0.65      0.62       699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "additional_feat_dim = instances[0]['anchor_addfeattensor'].shape[1]\n",
    "temp_classifier = BertLstmTest(additional_feat_dim=additional_feat_dim)\n",
    "temp_classifier = temp_classifier.to(device)\n",
    "temp_classifier.load_state_dict(torch.load(model_name))\n",
    "\n",
    "temp_classifier.eval()\n",
    "pred_labels = evaluator.test_model(temp_classifier, test_loader, idx_to_label, device)\n",
    "gold_labels = [x['adjudicated_label'] for x in test_instances]\n",
    "print('-' * 60)\n",
    "print(classification_report(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MPQA lexicon\n",
    "mpqa_path = os.path.join('data', 'reference', 'MPQA_Lexicon')\n",
    "mpqa_lexicon = preprocess.load_mpqa(mpqa_path)\n",
    "\n",
    "# extract additional features\n",
    "print(\"Extracting additional features using SpaCy ...\")\n",
    "start_time = time.time()\n",
    "instances = preprocess.add_additional_features(instances, mpqa_lexicon)\n",
    "end_time = time.time()\n",
    "elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "print(f\"Time spent for SpaCy preprocessing: {elapsed_mins}m {elapsed_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances, dev_instances, test_instances = preprocess.split_instances(instances)\n",
    "train_loader, dev_loader, test_loader = preprocess.get_data_loader(train_instances, dev_instances, test_instances, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parameter setting\n",
    "bert_feat_dim = 768\n",
    "lstm_dim = 512\n",
    "output_dim = 2\n",
    "hidden_dim = 512\n",
    "dropout_rate = 0.5\n",
    "\n",
    "\n",
    "class Test(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.bert_feat_dim = bert_feat_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lstm = nn.LSTM(self.bert_feat_dim, self.lstm_dim, batch_first=True, bidirectional=True)\n",
    "#         self.fc_combined = nn.Linear(self.bert_feat_dim*7+self.lstm_dim*2, hidden_dim)\n",
    "#         self.fc1 = nn.Linear(self.lstm_dim*2, hidden_dim)\n",
    "#         self.fc1 = nn.Linear(self.lstm_dim*2+self.bert_feat_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        # test\n",
    "        self.fc_lstm = nn.Linear(self.lstm_dim*2, self.hidden_dim)\n",
    "        self.fc_combined = nn.Linear(self.lstm_dim*2+self.bert_feat_dim, self.hidden_dim)\n",
    "        self.fc_anchor = nn.Linear(self.bert_feat_dim, self.hidden_dim)        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bn = nn.BatchNorm1d(self.lstm_dim*2, affine=True)\n",
    "\n",
    "    def forward(self, feat_combined):\n",
    "        \n",
    "        # get the bert output\n",
    "        context1_feat = feat_combined[:, :self.bert_feat_dim * 1].unsqueeze(1)\n",
    "        context2_feat = feat_combined[:, self.bert_feat_dim * 1:self.bert_feat_dim * 2].unsqueeze(1)\n",
    "        context3_feat = feat_combined[:, self.bert_feat_dim * 2:self.bert_feat_dim * 3].unsqueeze(1)\n",
    "        anchor_feat = feat_combined[:, self.bert_feat_dim * 3:self.bert_feat_dim * 4].unsqueeze(1)\n",
    "        context4_feat = feat_combined[:, self.bert_feat_dim * 4:self.bert_feat_dim * 5].unsqueeze(1)\n",
    "        context5_feat = feat_combined[:, self.bert_feat_dim * 5:self.bert_feat_dim * 6].unsqueeze(1)\n",
    "        context6_feat = feat_combined[:, self.bert_feat_dim * 6:self.bert_feat_dim * 7].unsqueeze(1)\n",
    "\n",
    "        # get the additional features\n",
    "        addfeat_dim = int((feat_combined.shape[0] - self.bert_feat_dim * 7) / 7)\n",
    "        context1_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 0:self.bert_feat_dim * 7 + addfeat_dim * 1].unsqueeze(1)\n",
    "        context2_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 1:self.bert_feat_dim * 7 + addfeat_dim * 2].unsqueeze(1)\n",
    "        context3_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 2:self.bert_feat_dim * 7 + addfeat_dim * 3].unsqueeze(1)\n",
    "        anchor_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 3:self.bert_feat_dim * 7 + addfeat_dim * 4].unsqueeze(1)\n",
    "        context4_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 4:self.bert_feat_dim * 7 + addfeat_dim * 5].unsqueeze(1)\n",
    "        context5_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 5:self.bert_feat_dim * 7 + addfeat_dim * 6].unsqueeze(1)\n",
    "        context6_addfeat = feat_combined[:, self.bert_feat_dim * 7 + addfeat_dim * 6:self.bert_feat_dim * 7 + addfeat_dim * 7].unsqueeze(1)\n",
    "\n",
    "        # prepare for the input of LSTM\n",
    "        lstm_input = torch.cat((\n",
    "            context1_feat,\n",
    "            context2_feat,\n",
    "            context3_feat,\n",
    "#             anchor_feat,\n",
    "            context4_feat,\n",
    "            context5_feat,\n",
    "            context6_feat\n",
    "        ), dim=1)\n",
    "        \n",
    "        # pass the LSTM\n",
    "        lstm_output, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        # only take the last hidden state\n",
    "#         out = out[:, -1, :]\n",
    "\n",
    "        # global maxpooling on the BiLSTM output\n",
    "        lstm_output = nn.AvgPool1d(6, 6)(lstm_output.permute(0,2,1)).permute(0,2,1).squeeze(1)\n",
    "        \n",
    "        combined_out = torch.cat((lstm_output, anchor_feat.squeeze(1)), dim=1)\n",
    "        \n",
    "        lstm_output = self.fc_lstm(lstm_output)\n",
    "        combined_out = self.fc_combined(combined_out)\n",
    "        anchor_feat = self.fc_anchor(anchor_feat).squeeze(1)\n",
    "        \n",
    "        all_combined = torch.cat((lstm_output, combined_out, anchor_feat), dim=1)\n",
    "        \n",
    "#         print(out.shape)\n",
    "        \n",
    "        # batch normalization\n",
    "#         out = self.bn(out)\n",
    "        \n",
    "        # pass the fully-connected layer(s)\n",
    "        out = self.fc1(all_combined)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# define the label mapping\n",
    "label_to_idx = {'Yes': 1, 'No': 0}\n",
    "idx_to_label = {1: 'Yes', 0: 'No'}\n",
    "    \n",
    "# define some global parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 16\n",
    "patience = 10\n",
    "learning_rate = 1e-04\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "additional_feat_dim = train_instances[0]['anchor_addfeattensor'].shape[1]\n",
    "classifier = Test(additional_feat_dim=additional_feat_dim)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "# define the optimizer, loos function, and some parameters\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch: 1 || Epoch Time: 0m 1s\n",
      "Epoch: 1 || Train loss: 0.04, Train Acc: 0.64\n",
      "Epoch: 1 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 2 || Epoch Time: 0m 2s\n",
      "Epoch: 2 || Train loss: 0.04, Train Acc: 0.65\n",
      "Epoch: 2 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "------------------------------------------------------------\n",
      "Epoch: 3 || Epoch Time: 0m 1s\n",
      "Epoch: 3 || Train loss: 0.04, Train Acc: 0.67\n",
      "Epoch: 3 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 4 || Epoch Time: 0m 1s\n",
      "Epoch: 4 || Train loss: 0.04, Train Acc: 0.69\n",
      "Epoch: 4 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 5 || Epoch Time: 0m 1s\n",
      "Epoch: 5 || Train loss: 0.04, Train Acc: 0.69\n",
      "Epoch: 5 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 6 || Epoch Time: 0m 1s\n",
      "Epoch: 6 || Train loss: 0.04, Train Acc: 0.70\n",
      "Epoch: 6 || Dev loss: 0.04, Dev Acc: 0.68\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 7 || Epoch Time: 0m 1s\n",
      "Epoch: 7 || Train loss: 0.04, Train Acc: 0.71\n",
      "Epoch: 7 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 8 || Epoch Time: 0m 1s\n",
      "Epoch: 8 || Train loss: 0.04, Train Acc: 0.72\n",
      "Epoch: 8 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 9 || Epoch Time: 0m 1s\n",
      "Epoch: 9 || Train loss: 0.03, Train Acc: 0.74\n",
      "Epoch: 9 || Dev loss: 0.04, Dev Acc: 0.67\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 10 || Epoch Time: 0m 1s\n",
      "Epoch: 10 || Train loss: 0.03, Train Acc: 0.75\n",
      "Epoch: 10 || Dev loss: 0.04, Dev Acc: 0.66\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 11 || Epoch Time: 0m 1s\n",
      "Epoch: 11 || Train loss: 0.03, Train Acc: 0.74\n",
      "Epoch: 11 || Dev loss: 0.04, Dev Acc: 0.66\n",
      "The loss on development set does not decrease\n",
      "------------------------------------------------------------\n",
      "Epoch: 12 || Epoch Time: 0m 1s\n",
      "Epoch: 12 || Train loss: 0.03, Train Acc: 0.77\n",
      "Epoch: 12 || Dev loss: 0.04, Dev Acc: 0.65\n",
      "The loss on development set does not decrease\n",
      "The loss on development set does not decrease, stop training!\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.35      0.07      0.11       226\n",
      "         Yes       0.68      0.94      0.79       473\n",
      "\n",
      "    accuracy                           0.66       699\n",
      "   macro avg       0.51      0.50      0.45       699\n",
      "weighted avg       0.57      0.66      0.57       699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 10\n",
    "best_valid_loss = float('inf')\n",
    "check_stopping = 0\n",
    "model_name = f'retrained_{mode}_classifier.pt'\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    classifier.train()\n",
    "    train_loss, train_acc = learning_helper.train(classifier, train_loader, optimizer, criterion, device, label_to_idx)\n",
    "    dev_loss, dev_acc = learning_helper.evaluate(classifier, dev_loader, criterion, device, label_to_idx)\n",
    "    classifier.train()\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_mins, elapsed_secs = learning_helper.epoch_time(start_time, end_time)\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Epoch: {i+1} || Epoch Time: {elapsed_mins}m {elapsed_secs}s\")\n",
    "    print(f\"Epoch: {i+1} || Train loss: {train_loss:.02f}, Train Acc: {train_acc:.02f}\")\n",
    "    print(f\"Epoch: {i+1} || Dev loss: {dev_loss:.02f}, Dev Acc: {dev_acc:.02f}\")\n",
    "\n",
    "    # check if we need to save the model\n",
    "    if dev_loss < best_valid_loss:\n",
    "        check_stopping = 0\n",
    "        best_valid_loss = dev_loss\n",
    "        torch.save(classifier.state_dict(), model_name)\n",
    "    else:\n",
    "        check_stopping += 1\n",
    "        print(f\"The loss on development set does not decrease\")\n",
    "        if check_stopping == patience:\n",
    "            print(\"The loss on development set does not decrease, stop training!\")\n",
    "            break\n",
    "            \n",
    "classifier.eval()\n",
    "pred_labels = evaluator.test_model(classifier, test_loader, idx_to_label, device)\n",
    "gold_labels = [x['adjudicated_label'] for x in test_instances]\n",
    "print('-' * 60)\n",
    "print(classification_report(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Test:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([2048, 16058]) from checkpoint, the shape in current model is torch.Size([2048, 5376]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-51f09920e5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgold_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjudicated_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_instances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/jupyter/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1045\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Test:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([2048, 16058]) from checkpoint, the shape in current model is torch.Size([2048, 5376])."
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "classifier = Test()\n",
    "classifier = classifier.to(device)\n",
    "classifier.load_state_dict(torch.load(model_name))\n",
    "pred_labels = evaluator.test_model(classifier, test_loader, idx_to_label, device)\n",
    "gold_labels = [x['adjudicated_label'] for x in test_instances]\n",
    "print('-' * 60)\n",
    "print(classification_report(gold_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add more info to the annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "annotation_filepath = 'data/annotations/annotation_context.json'\n",
    "original_batch_filepath = 'batch_6540.csv'\n",
    "data_dir = 'data'\n",
    "\n",
    "lookup_dict = defaultdict(dict)\n",
    "with open(original_batch_filepath, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        lookup_dict[row['instance_id']] = row\n",
    "        \n",
    "def add_info(lookup_dict, annotation_filepath):\n",
    "    instances = []\n",
    "    with open(annotation_filepath, 'r') as jsonfile:\n",
    "        lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "        for line in lines:\n",
    "            instance = {}\n",
    "            temp_instance = json.loads(line)\n",
    "            # remove \"Input.\" in the keys\n",
    "            for key, value in temp_instance.items():\n",
    "                if key.startswith(\"Input.\"):\n",
    "                    if not key.endswith(\"url\"):\n",
    "                        newkey = key.split(\".\")[-1]\n",
    "                        instance[newkey] = value\n",
    "                else:\n",
    "                    instance[key] = value\n",
    "\n",
    "            # add image filepath, json filepath, screenshot url, tweet_text, and timestamp\n",
    "            original_dict = lookup_dict[temp_instance['Input.instance_id']]\n",
    "            for key, value in original_dict.items():\n",
    "                if key.endswith(\"url\"):\n",
    "                    tweet_id = value.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "                    # add json filepath\n",
    "                    jsonname = f\"anchor_{tweet_id}.json\" if 'anchor' in key else f\"{tweet_id}.json\"\n",
    "                    jsonpath = os.path.join(data_dir, 'json_files', '_'.join(value.split(\"/\")[-1].split(\"_\")[:2]), jsonname)\n",
    "                    jsonkey = key.split(\"_\")[0] + \"_jsonpath\"\n",
    "                    instance[jsonkey] = jsonpath\n",
    "\n",
    "                    # add tweet text\n",
    "                    with open(jsonpath, 'r') as tweetfile:\n",
    "                        tweet = json.loads(tweetfile.read())\n",
    "                    textkey = key.split(\"_\")[0] + \"_tweettext\"\n",
    "                    instance[textkey] = tweet['full_text']\n",
    "\n",
    "                    # add image filepath if image exists\n",
    "                    instance[key] = value\n",
    "                    imagename = f\"anchor_{tweet_id}.jpg\" if 'anchor' in key else f\"{tweet_id}.jpg\"\n",
    "                    imagepath = os.path.join(data_dir, 'image_files', '_'.join(value.split(\"/\")[-1].split(\"_\")[:2]), imagename)\n",
    "                    hasimg = os.path.isfile(imagepath)\n",
    "                    if hasimg:\n",
    "                        imagekey = key.split(\"_\")[0] + \"_imagepath\"\n",
    "                        instance[imagekey] = imagepath\n",
    "\n",
    "                if key.endswith(\"timestamp\"):\n",
    "                    instance[key] = original_dict[key]\n",
    "            instances.append(instance)\n",
    "    return instances\n",
    "\n",
    "instances = add_info(lookup_dict, annotation_filepath)\n",
    "\n",
    "new_annot_filename = 'new_annot.json'\n",
    "with open(new_annot_filename, 'w') as newjson:\n",
    "    for instance in instances:\n",
    "        newjson.write(json.dumps(instance))\n",
    "        newjson.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy the json file and image file to the loctmp2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "original_folder = '/media/zhaomin/Zhaomin_SSD/project_repo/emnlp2021/saved_tweets_original'\n",
    "data_dir = 'data'\n",
    "\n",
    "original_batch_filepath = 'batch_6540.csv'\n",
    "lookup_dict = defaultdict(dict)\n",
    "with open(original_batch_filepath, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        lookup_dict[row['instance_id']] = row\n",
    "        \n",
    "annotation_filepath = 'data/annotations/annotation_context.json'\n",
    "with open(annotation_filepath, 'r') as jsonfile:\n",
    "    lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "    for line in lines:\n",
    "        instance = json.loads(line)\n",
    "        instance_id = instance['Input.instance_id']\n",
    "        \n",
    "        # create folder if it does not exist\n",
    "        if not os.path.isdir(os.path.join(data_dir, 'json_files', instance_id)):\n",
    "            os.mkdir(os.path.join(data_dir, 'json_files', instance_id))\n",
    "        if not os.path.isdir(os.path.join(data_dir, 'image_files', instance_id)):\n",
    "            os.mkdir(os.path.join(data_dir, 'image_files', instance_id))\n",
    "            \n",
    "        # find event path\n",
    "        original_dict = lookup_dict[instance_id]\n",
    "        event_name = re.split('(\\d+)', instance_id.split(\"_\")[0])[0]\n",
    "        for original_event in os.listdir(original_folder):\n",
    "            \n",
    "            # make sure the event and year are matched\n",
    "            if original_event.split(\"_\")[0] == event_name:\n",
    "                if original_event.split(\"_\")[1].split(\"-\")[0] == re.split('(\\d+)', instance_id.split(\"_\")[0])[1]:\n",
    "                    original_event_path = os.path.join(original_folder, original_event, f\"final_tweet_folder_{original_event}\", instance_id.split(\"_\")[-1])\n",
    "                    break\n",
    "        \n",
    "        for key, value in original_dict.items():\n",
    "            if key.endswith(\"url\"):\n",
    "                \n",
    "                if 'anchor' in key:\n",
    "                    real_instance_id = value.split(\"/\")[-1].split(\"_\")[1]\n",
    "                    src_jsonfilename = f\"anchor_{real_instance_id}.json\"\n",
    "                    src_imagefilename = f\"anchor_{real_instance_id}.jpg\"\n",
    "                    src_jsonfilepath = os.path.join(original_event_path, src_jsonfilename)\n",
    "                    src_imagefilepath = os.path.join(original_event_path, src_imagefilename)\n",
    "                    dst_jsonfilepath = os.path.join(data_dir, 'json_files', instance_id, src_jsonfilename)\n",
    "                    dst_imagefilepath = os.path.join(data_dir, 'image_files', instance_id, src_imagefilename)\n",
    "                    copyfile(src_jsonfilepath, dst_jsonfilepath)\n",
    "                    copyfile(src_imagefilepath, dst_imagefilepath)\n",
    "                \n",
    "                else:\n",
    "                    tweet_id = value.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "                    \n",
    "                    # copy json file\n",
    "                    src_jsonfilename = f\"{tweet_id}.json\"\n",
    "                    src_jsonfilepath = os.path.join(original_event_path, src_jsonfilename)\n",
    "                    dst_jsonfilepath = os.path.join(data_dir, 'json_files', instance_id, src_jsonfilename)\n",
    "                    copyfile(src_jsonfilepath, dst_jsonfilepath)\n",
    "                    \n",
    "                    # copy image file if it exists\n",
    "                    src_imagefilename = f\"{tweet_id}.jpg\"\n",
    "                    if src_imagefilename in os.listdir(original_event_path):\n",
    "                        src_imagefilepath = os.path.join(original_event_path, src_imagefilename)\n",
    "                        dst_imagefilepath = os.path.join(data_dir, 'image_files', instance_id, src_imagefilename)\n",
    "                        copyfile(src_imagefilepath, dst_imagefilepath)\n",
    "                    \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save split for replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = instances\n",
    "y = [x['adjudicated_label'] for x in instances]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train)\n",
    "\n",
    "split = {'train': [x['instance_id'] for x in X_train],\n",
    "         'dev': [x['instance_id'] for x in X_dev],\n",
    "         'test': [x['instance_id'] for x in X_test]}\n",
    "\n",
    "with open(\"saved_split\", 'w') as splitfile:\n",
    "    splitfile.write(json.dumps(split))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stuff",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
