{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/annotations/new_annot.json'\n",
    "instances = preprocess.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor_location': 'Atlanta',\n",
       " 'instance_id': 'thanksgiving2019_1199840018380001281',\n",
       " 'event': 'thanksgiving2019',\n",
       " 'Answer.Q1_A2ZJS73XSSMRTD': 'Yes',\n",
       " 'Answer.Q2_A2ZJS73XSSMRTD': '4',\n",
       " 'adjudicated_label': 'Yes',\n",
       " 'anchor_timestamp': 'Wed Nov 27 23:58:46 +0000 2019',\n",
       " 'anchor_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/anchor_1199840018380001281.json',\n",
       " 'anchor_tweettext': 'Just a sampling of our Thanksgiving Eve feast. Special shout-out to @FOX5ATLCallaway and @Elepo for organizing it!!!!!! @FOX5Atlanta https://t.co/w6k2p0c46u',\n",
       " 'anchor_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_anchor_1199840018380001281.png',\n",
       " 'anchor_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/anchor_1199840018380001281.jpg',\n",
       " 'context8_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199503490923536389.json',\n",
       " 'context8_tweettext': '@aungeliquefox5 Yes! And so generous !',\n",
       " 'context8_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199503490923536389.png',\n",
       " 'context8_timestamp': 'Wed Nov 27 01:41:32 +0000 2019',\n",
       " 'context9_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199732704411082752.json',\n",
       " 'context9_tweettext': '@AlexaLiackoFOX5 YOU NAME IT!!!!',\n",
       " 'context9_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199732704411082752.png',\n",
       " 'context9_timestamp': 'Wed Nov 27 16:52:21 +0000 2019',\n",
       " 'context10_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199744669372030978.json',\n",
       " 'context10_tweettext': \"YES! I am SO happy for Celine. She has one of the greatest voices in music. And she's been through so much. No one deserves this more! https://t.co/3v6CjjiSDX\",\n",
       " 'context10_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199744669372030978.png',\n",
       " 'context10_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1199744669372030978.jpg',\n",
       " 'context10_timestamp': 'Wed Nov 27 17:39:54 +0000 2019',\n",
       " 'context11_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199840318230794241.json',\n",
       " 'context11_tweettext': 'Mood when I saw @FOX5ATLCallaway slicing our @FOX5Atlanta Thanksgiving Eve Turkey!!! YAAASSSSSS!!! https://t.co/gBosgWALqr',\n",
       " 'context11_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199840318230794241.png',\n",
       " 'context11_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1199840318230794241.jpg',\n",
       " 'context11_timestamp': 'Wed Nov 27 23:59:58 +0000 2019',\n",
       " 'context12_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1199877306862522373.json',\n",
       " 'context12_tweettext': '@CityStonecrest @FOX5ATLCallaway @Elepo @FOX5Atlanta GONE!!! LOL',\n",
       " 'context12_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1199877306862522373.png',\n",
       " 'context12_timestamp': 'Thu Nov 28 02:26:57 +0000 2019',\n",
       " 'context13_jsonpath': 'data/json_files/thanksgiving2019_1199840018380001281/1200164708428525572.json',\n",
       " 'context13_tweettext': 'I have SO much to be thankful for! Take time to spend this holiday with the people you love. @FOX5Atlanta https://t.co/dGVNcaOIo3',\n",
       " 'context13_url': 'http://www.cse.unt.edu/~blanco/screenshot/thanksgiving2019_1199840018380001281_1200164708428525572.png',\n",
       " 'context13_imagepath': 'data/image_files/thanksgiving2019_1199840018380001281/1200164708428525572.jpg',\n",
       " 'context13_timestamp': 'Thu Nov 28 21:28:59 +0000 2019'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = preprocess.add_bert_output(instances, anchor_only=False)\n",
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def add_vgg_output(instances, anchor_only):\n",
    "    \"\"\"\n",
    "    add the image representation (from VGG16) into the dictionary\n",
    "    :param instances: a list of instances need to be updated\n",
    "    :return: a new new list of instances that are already updated\n",
    "    \"\"\"\n",
    "    # Use the GPU, if available, to get the image representation\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # get the pretrained VGG16 (discard the last classification layer)\n",
    "    vgg16_model = models.vgg16(pretrained=True)\n",
    "    for p in vgg16_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    vgg16_model = vgg16_model.to(device)\n",
    "    vgg16_model.eval()\n",
    "\n",
    "    for instance in instances:\n",
    "        \n",
    "        # get the path of image file\n",
    "        filepath = instance['anchor_imagepath']\n",
    "\n",
    "        # preprocess the image, convert it into RGB format if it is not RGB\n",
    "        input_image = Image.open(filepath).convert('RGB')\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # get the image representation\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = vgg16_model(input_tensor)\n",
    "            # move the tensor to the CPU to free GPU memory\n",
    "            output = output.to('cpu')\n",
    "\n",
    "        # add image representation into the dictionary\n",
    "        instance['anchor_vggoutput'] = output\n",
    "\n",
    "        if not anchor_only:\n",
    "            \n",
    "            for i in range(8, 14):\n",
    "            \n",
    "                # get the path of image file if the image exists\n",
    "                imagekey = f\"context{i}_imagepath\"\n",
    "                if imagekey in instance.keys():\n",
    "                    \n",
    "                    filepath = instance[imagekey]\n",
    "\n",
    "                    # preprocess the image, convert it into RGB format if it is not RGB\n",
    "                    input_image = Image.open(filepath).convert('RGB')\n",
    "                    preprocess = transforms.Compose([\n",
    "                        transforms.Resize(256),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "                    input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "                    # get the image representation\n",
    "                    with torch.no_grad():\n",
    "                        \n",
    "                        output = vgg16_model(input_tensor)\n",
    "                        # move the tensor to the CPU to free GPU memory\n",
    "                        output = output.to('cpu')\n",
    "\n",
    "                    # add image representation into the dictionary\n",
    "                    instance[f\"context{i}_vggoutput\"] = output\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = add_vgg_output(instances, anchor_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = preprocess.add_vgg_output(instances, anchor_only=False)\n",
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add more info to the annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "annotation_filepath = 'data/annotations/annotation_context.json'\n",
    "original_batch_filepath = 'batch_6540.csv'\n",
    "data_dir = 'data'\n",
    "\n",
    "lookup_dict = defaultdict(dict)\n",
    "with open(original_batch_filepath, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        lookup_dict[row['instance_id']] = row\n",
    "        \n",
    "def add_info(lookup_dict, annotation_filepath):\n",
    "    instances = []\n",
    "    with open(annotation_filepath, 'r') as jsonfile:\n",
    "        lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "        for line in lines:\n",
    "            instance = {}\n",
    "            temp_instance = json.loads(line)\n",
    "            # remove \"Input.\" in the keys\n",
    "            for key, value in temp_instance.items():\n",
    "                if key.startswith(\"Input.\"):\n",
    "                    if not key.endswith(\"url\"):\n",
    "                        newkey = key.split(\".\")[-1]\n",
    "                        instance[newkey] = value\n",
    "                else:\n",
    "                    instance[key] = value\n",
    "\n",
    "            # add image filepath, json filepath, screenshot url, tweet_text, and timestamp\n",
    "            original_dict = lookup_dict[temp_instance['Input.instance_id']]\n",
    "            for key, value in original_dict.items():\n",
    "                if key.endswith(\"url\"):\n",
    "                    tweet_id = value.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "                    # add json filepath\n",
    "                    jsonname = f\"anchor_{tweet_id}.json\" if 'anchor' in key else f\"{tweet_id}.json\"\n",
    "                    jsonpath = os.path.join(data_dir, 'json_files', '_'.join(value.split(\"/\")[-1].split(\"_\")[:2]), jsonname)\n",
    "                    jsonkey = key.split(\"_\")[0] + \"_jsonpath\"\n",
    "                    instance[jsonkey] = jsonpath\n",
    "\n",
    "                    # add tweet text\n",
    "                    with open(jsonpath, 'r') as tweetfile:\n",
    "                        tweet = json.loads(tweetfile.read())\n",
    "                    textkey = key.split(\"_\")[0] + \"_tweettext\"\n",
    "                    instance[textkey] = tweet['full_text']\n",
    "\n",
    "                    # add image filepath if image exists\n",
    "                    instance[key] = value\n",
    "                    imagename = f\"anchor_{tweet_id}.jpg\" if 'anchor' in key else f\"{tweet_id}.jpg\"\n",
    "                    imagepath = os.path.join(data_dir, 'image_files', '_'.join(value.split(\"/\")[-1].split(\"_\")[:2]), imagename)\n",
    "                    hasimg = os.path.isfile(imagepath)\n",
    "                    if hasimg:\n",
    "                        imagekey = key.split(\"_\")[0] + \"_imagepath\"\n",
    "                        instance[imagekey] = imagepath\n",
    "\n",
    "                if key.endswith(\"timestamp\"):\n",
    "                    instance[key] = original_dict[key]\n",
    "            instances.append(instance)\n",
    "    return instances\n",
    "\n",
    "instances = add_info(lookup_dict, annotation_filepath)\n",
    "\n",
    "new_annot_filename = 'new_annot.json'\n",
    "with open(new_annot_filename, 'w') as newjson:\n",
    "    for instance in instances:\n",
    "        newjson.write(json.dumps(instance))\n",
    "        newjson.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy the json file and image file to the loctmp2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "original_folder = '/media/zhaomin/Zhaomin_SSD/project_repo/emnlp2021/saved_tweets_original'\n",
    "data_dir = 'data'\n",
    "\n",
    "original_batch_filepath = 'batch_6540.csv'\n",
    "lookup_dict = defaultdict(dict)\n",
    "with open(original_batch_filepath, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        lookup_dict[row['instance_id']] = row\n",
    "        \n",
    "annotation_filepath = 'data/annotations/annotation_context.json'\n",
    "with open(annotation_filepath, 'r') as jsonfile:\n",
    "    lines = jsonfile.read().split(\"\\n\")[:-1]\n",
    "    for line in lines:\n",
    "        instance = json.loads(line)\n",
    "        instance_id = instance['Input.instance_id']\n",
    "        \n",
    "        # create folder if it does not exist\n",
    "        if not os.path.isdir(os.path.join(data_dir, 'json_files', instance_id)):\n",
    "            os.mkdir(os.path.join(data_dir, 'json_files', instance_id))\n",
    "        if not os.path.isdir(os.path.join(data_dir, 'image_files', instance_id)):\n",
    "            os.mkdir(os.path.join(data_dir, 'image_files', instance_id))\n",
    "            \n",
    "        # find event path\n",
    "        original_dict = lookup_dict[instance_id]\n",
    "        event_name = re.split('(\\d+)', instance_id.split(\"_\")[0])[0]\n",
    "        for original_event in os.listdir(original_folder):\n",
    "            \n",
    "            # make sure the event and year are matched\n",
    "            if original_event.split(\"_\")[0] == event_name:\n",
    "                if original_event.split(\"_\")[1].split(\"-\")[0] == re.split('(\\d+)', instance_id.split(\"_\")[0])[1]:\n",
    "                    original_event_path = os.path.join(original_folder, original_event, f\"final_tweet_folder_{original_event}\", instance_id.split(\"_\")[-1])\n",
    "                    break\n",
    "        \n",
    "        for key, value in original_dict.items():\n",
    "            if key.endswith(\"url\"):\n",
    "                \n",
    "                if 'anchor' in key:\n",
    "                    real_instance_id = value.split(\"/\")[-1].split(\"_\")[1]\n",
    "                    src_jsonfilename = f\"anchor_{real_instance_id}.json\"\n",
    "                    src_imagefilename = f\"anchor_{real_instance_id}.jpg\"\n",
    "                    src_jsonfilepath = os.path.join(original_event_path, src_jsonfilename)\n",
    "                    src_imagefilepath = os.path.join(original_event_path, src_imagefilename)\n",
    "                    dst_jsonfilepath = os.path.join(data_dir, 'json_files', instance_id, src_jsonfilename)\n",
    "                    dst_imagefilepath = os.path.join(data_dir, 'image_files', instance_id, src_imagefilename)\n",
    "                    copyfile(src_jsonfilepath, dst_jsonfilepath)\n",
    "                    copyfile(src_imagefilepath, dst_imagefilepath)\n",
    "                \n",
    "                else:\n",
    "                    tweet_id = value.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "                    \n",
    "                    # copy json file\n",
    "                    src_jsonfilename = f\"{tweet_id}.json\"\n",
    "                    src_jsonfilepath = os.path.join(original_event_path, src_jsonfilename)\n",
    "                    dst_jsonfilepath = os.path.join(data_dir, 'json_files', instance_id, src_jsonfilename)\n",
    "                    copyfile(src_jsonfilepath, dst_jsonfilepath)\n",
    "                    \n",
    "                    # copy image file if it exists\n",
    "                    src_imagefilename = f\"{tweet_id}.jpg\"\n",
    "                    if src_imagefilename in os.listdir(original_event_path):\n",
    "                        src_imagefilepath = os.path.join(original_event_path, src_imagefilename)\n",
    "                        dst_imagefilepath = os.path.join(data_dir, 'image_files', instance_id, src_imagefilename)\n",
    "                        copyfile(src_imagefilepath, dst_imagefilepath)\n",
    "                    \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
