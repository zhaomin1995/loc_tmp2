{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2068c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d22a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def load_data(data_filepath, split_filepath):\n",
    "    \n",
    "    train_data, test_data = [], []\n",
    "\n",
    "    with open(split_filepath, 'r') as file:\n",
    "        splits = json.load(file)\n",
    "        train_ids = splits['train']\n",
    "        test_ids = splits['test']\n",
    "        \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            item = json.loads(line)\n",
    "            kept_annotations = [item[key] for key in item.keys() if key.startswith(\"Answer.Q1_\")]\n",
    "            if len(kept_annotations) == 0:\n",
    "                continue\n",
    "            texts = [\n",
    "                clean_text(item['context8_tweettext']),\n",
    "                clean_text(item['context9_tweettext']),\n",
    "                clean_text(item['context10_tweettext']),\n",
    "                clean_text(item['context11_tweettext']),\n",
    "                clean_text(item['context12_tweettext']),\n",
    "                clean_text(item['context13_tweettext']),\n",
    "            ]\n",
    "            instance = {'texts': texts, 'label': item['adjudicated_label'], 'location': item['anchor_location']}\n",
    "            if item['instance_id'] in train_ids:\n",
    "                train_data.append(instance)\n",
    "            if item['instance_id'] in test_ids:\n",
    "                test_data.append(instance)\n",
    "                \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "data_filepath = 'data/data.json'\n",
    "split_filepath = 'data/data_split'\n",
    "train_data, test_data = load_data(data_filepath, split_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee1969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45028ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f752f3dbd164a2d935bab75c7cedc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.63 GiB total capacity; 22.38 GiB already allocated; 75.56 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSeq2SeqLM, AutoTokenizer\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/flan-ul2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    492\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:2903\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2894\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2896\u001b[0m     (\n\u001b[1;32m   2897\u001b[0m         model,\n\u001b[1;32m   2898\u001b[0m         missing_keys,\n\u001b[1;32m   2899\u001b[0m         unexpected_keys,\n\u001b[1;32m   2900\u001b[0m         mismatched_keys,\n\u001b[1;32m   2901\u001b[0m         offload_index,\n\u001b[1;32m   2902\u001b[0m         error_msgs,\n\u001b[0;32m-> 2903\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2921\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   2922\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:3260\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3250\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3251\u001b[0m     state_dict,\n\u001b[1;32m   3252\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3256\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3257\u001b[0m )\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3260\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3267\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3268\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3276\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:725\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    722\u001b[0m             fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_name:\n\u001b[0;32m--> 725\u001b[0m             \u001b[43mset_module_quantized_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/transformers/utils/bitsandbytes.py:97\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     95\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_8bit:\n\u001b[0;32m---> 97\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInt8Params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_4bit:\n\u001b[1;32m     99\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParams4bit(new_value, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:333\u001b[0m, in \u001b[0;36mInt8Params.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    329\u001b[0m     device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m ):\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     new_param \u001b[38;5;241m=\u001b[39m Int8Params(\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    337\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m         has_fp16_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_fp16_weights,\n\u001b[1;32m    341\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:296\u001b[0m, in \u001b[0;36mInt8Params.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# we store the 8-bit rows-major weight\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# we convert this weight to the turning/ampere weight during the first inference pass\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     CB, CBt, SCB, SCBt, coo_tensorB \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdouble_quant(B)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m CBt\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 23.63 GiB total capacity; 22.38 GiB already allocated; 75.56 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = 'google/flan-ul2'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True).to('cuda:2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f8fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = \"\"\"### Prompt: Read the tweets below and determine its sentiment.\n",
    "### Tweets: Dallas is so bad.\n",
    "OPTIONS:\n",
    "1. Negative\n",
    "2. Positive\n",
    "### Answer: \"\"\"\n",
    "\n",
    "inputs = tokenizer(model_input, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7b979",
   "metadata": {},
   "source": [
    "### Test UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6393b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7636c5b068441e1a0aa4ec664bc1585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/ul2\", load_in_8bit=True, device_map='auto')                                                                                                   \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/ul2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458a787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of United States is Donald Trump. Question: Who is the president of India? Answer: The president of India is Ram Nath Kovind. Question: Who is the president of Pakistan? Answer: The president of Pakistan is Imran Khan. Question: Who is the president of Bangladesh? Answer: The president of Bangladesh is Abdullah Ahmed. Question: Who is the president of Bangladesh? Answer: The president of Bangladesh is Sheikh Hasina. Question: Who is the\n"
     ]
    }
   ],
   "source": [
    "input_string = (\n",
    "    \"[NLG] What is the president of United States?\\nAnswer: \"\n",
    ")                                          \n",
    "\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "\n",
    "outputs = model.generate(inputs, max_length=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea3f839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a1ee178",
   "metadata": {},
   "source": [
    "### Test various model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99716a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-xxl\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-xxl\",\n",
    "                                              load_in_8bit=True, \n",
    "                                              device_map=\"auto\",\n",
    "                                              trust_remote_code=True,\n",
    "                                              cache_dir='/mnt/DATA/hf_cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7cc9dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2. Negative'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = 'Dallas is so bad and I really wanna go back in the future'\n",
    "prompt = f\"\"\"Determine the sentiment of the given sentence.\n",
    "\n",
    "{test_sent}\n",
    "\n",
    "OPTIONS:\n",
    "1. Positive.\n",
    "2. Negative.\n",
    "ANSWER: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "output_tokens = model.generate(**input_ids, max_new_tokens=150, do_sample=False, use_cache=True)\n",
    "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58409b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f21fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab38be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2aa636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759e0777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 KB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /home/UNT/zx0034/envs/llm/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/UNT/zx0034/envs/llm/lib/python3.10/site-packages (from scikit-learn) (1.25.2)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687355e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "##########  flan_ul2_all_few-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.35      0.77      0.48       350\n",
      "         Yes       0.71      0.28      0.40       699\n",
      "\n",
      "    accuracy                           0.45      1049\n",
      "   macro avg       0.53      0.53      0.44      1049\n",
      "weighted avg       0.59      0.45      0.43      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_t5_target_zero-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.34      0.97      0.50       350\n",
      "         Yes       0.74      0.04      0.08       699\n",
      "\n",
      "    accuracy                           0.35      1049\n",
      "   macro avg       0.54      0.51      0.29      1049\n",
      "weighted avg       0.60      0.35      0.22      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_alpaca_early_target_zero-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.34      0.99      0.50       350\n",
      "         Yes       0.80      0.02      0.03       699\n",
      "\n",
      "    accuracy                           0.34      1049\n",
      "   macro avg       0.57      0.50      0.27      1049\n",
      "weighted avg       0.65      0.34      0.19      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_t5+_early_target_zero-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.34      0.87      0.48       350\n",
      "         Yes       0.68      0.14      0.23       699\n",
      "\n",
      "    accuracy                           0.38      1049\n",
      "   macro avg       0.51      0.50      0.36      1049\n",
      "weighted avg       0.56      0.38      0.32      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_t5+_target_later_few-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.33      0.89      0.48       350\n",
      "         Yes       0.65      0.10      0.17       699\n",
      "\n",
      "    accuracy                           0.37      1049\n",
      "   macro avg       0.49      0.50      0.33      1049\n",
      "weighted avg       0.55      0.37      0.28      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_ul2+_all_zero-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.34      0.59      0.43       350\n",
      "         Yes       0.68      0.43      0.53       699\n",
      "\n",
      "    accuracy                           0.49      1049\n",
      "   macro avg       0.51      0.51      0.48      1049\n",
      "weighted avg       0.57      0.49      0.50      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_t5+_target_few-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.34      0.93      0.50       350\n",
      "         Yes       0.74      0.09      0.17       699\n",
      "\n",
      "    accuracy                           0.37      1049\n",
      "   macro avg       0.54      0.51      0.33      1049\n",
      "weighted avg       0.61      0.37      0.28      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_ul2_early_target_zero-shot_response  ##########\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.35      0.79      0.49       350\n",
      "         Yes       0.72      0.27      0.40       699\n",
      "\n",
      "    accuracy                           0.45      1049\n",
      "   macro avg       0.54      0.53      0.44      1049\n",
      "weighted avg       0.60      0.45      0.43      1049\n",
      "\n",
      "############################################################\n",
      "##########  flan_ul2+_target_zero-shot_response  ##########\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1049, 1045]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response_filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2539\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2406\u001b[0m     {\n\u001b[1;32m   2407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2430\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2431\u001b[0m ):\n\u001b[1;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m \n\u001b[1;32m   2434\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2542\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/llm/lib/python3.10/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1049, 1045]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "response_folder = 'output/responses/'\n",
    "for response_filename in os.listdir(response_folder):\n",
    "    response_filepath = os.path.join(response_folder, response_filename)\n",
    "    with open(response_filepath, 'r') as file:\n",
    "        response = json.load(file)\n",
    "    labels = response['labels']\n",
    "    print(\"#\" * 60)\n",
    "    print(\"#\" * 10 + \"  \" + response_filename + \"  \" + \"#\" * 10)\n",
    "    print(classification_report(response['labels'], response['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3782819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "Yes\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "Yes\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for pred in response['predictions']:\n",
    "    if pred.startswith('1'):\n",
    "        predictions.append('Yes')\n",
    "    if pred.startswith('2'):\n",
    "        predictions.append('No')\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49fab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dd112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8450584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822a716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9a2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774275c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01618bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43c78301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9b076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5254e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4c589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
